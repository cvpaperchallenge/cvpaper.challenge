+slide
  .title Joint Event Detection and Description in Continuous Video Streams
  .info
    .authors Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko
    .conference arXiv:1802.10250
  .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

  .item1
    .text
      h1 概要
      p End-to-Endでイベント検出（行動の時系列セグメント化）とキャプショニングを実行するタスクを提供する。モデルには3D Convolutionや階層的LSTM（two-level hierarchical LSTM）を採用した。基本的には
        a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
        |をベースにして研究を行なっているが、{Controller, Captioner} LSTMの二段階により前の候補のセンテンスやビデオコンテキストを考慮しつつ状態を更新（Controller LSTM）し、候補領域の特徴を参照しつつキャプションを生成（Captioner LSTM）する。時系列候補領域とキャプションはmulti-task学習、End-to-Endで学習される。
  .item2
    .text
      p
        img(src=`${figpath}180302VideoCaption.png`,alt="180302VideoCaption")
  .item3
    .text
      h1 新規性・結果
      p 候補領域生成やキャプショニングの精度を検証した。また、データセットにはActivityNet CaptionsやTACoS Datasetを用いた。候補領域については従来法のDAPが30, multi-scale DAPが38 @AUC (IoU>0.8)に対して提案法であるJEDDi-Netは58.21を記録した。また、キャプショニングについては従来法が{17.95, 4.82, 17.29} (各BLEU1, METEOR, CIDEr)の問題に対して{19.97, 8.58, 19.88}を記録した。
  .item4
    .text
      h1 コメント/リンク集
      p 動画のタスクはカテゴリのみでなく言語やより表現力豊かな認識ができなければいけない時期になって来た？時系列表現にもまだまだ課題が多いので、これからさらに動画認識にチャレンジすべき。また、キャプショニングの問題は感性評価に対する知見や確固たる評価方法が確立されるとさらに面白くなるのではないか。
      ul
        li
          a(href="https://arxiv.org/pdf/1802.10250.pdf") PDF
        li
          a(href="https://www.bu.edu/cs/profiles/kate-saenko/") Kate Saenko
        li
          a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
        li
          a(href="https://github.com/kenshohara/3D-ResNets-PyTorch") 3D Convolution (3D-ResNets-PyTorch)
  .slide_index #{getSlideIndex()}
  .timestamp 2018.3.2 09:36:26
