<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>cvpaper.challenge</title>
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/white.css">
    <link rel="stylesheet" href="css/layout.css">
    <link rel="stylesheet" href="lib/css/zenburn.css">
  </head>
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
    
  </script>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <div class="paper-abstract">
            <div class="title">End-to-end Driving via Conditional Imitation Learning</div>
            <div class="info">Felipe Codevilla, Matthias Müller, Alexey Dosovitskiy, Antonio López, and Vladlen Koltun, arXiv 1710.02410</div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">自動運転を模倣学習により行う手法を提案。実空間での学習結果をヴァーチャルな空間での自動運転にて再現することができた。RGB画像、計測（e.g. スピード）や命令（e.g. turn right）などからステアリング、アクセル、ブレーキなどのコマンドを出力して自動車を操作する。</div>
            </div>
            <div class="item2"><img src="slides/figs/180205conditionalimitationlearning.png"><img src="slides/figs/180205framework_imitation.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>模倣学習による自動運転を実現した。</li>
                  <li>実空間とシミュレーションベースの転移を行うことにも成功。</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>リンク集</h1>
              <div class="text">
                <ul>
                  <li>自動運転の学習はシミュレーションベースで完結してしまう可能性がある。</li>
                  <li>メタ学習/模倣学習/強化学習などはCVに徐々に取り入れられてくるはずなので、2018年は学習しておいて損はない？</li>
                  <li><a href="https://arxiv.org/pdf/1710.02410.pdf" target="blank">[論文] End-to-end Driving via Conditional Imitation Learning</a></li>
                  <li><a href="https://www.youtube.com/watch?v=cFtnflNe5fM" target="blank">YouTube</a></li>
                  <li><a href="http://hirokatsukataoka.net/" target="blank">slide by Hirokatsu Kataoka</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#1]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Open3D: A Modern Library for 3D Data Processing</div>
            <div class="info">Qian-Yi Zhou et al., arXiv 1801.09847</div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">3Dデータを取り扱い、迅速な開発を可能にする<a href="http://www.open3d.org/" target="blank">Open3D</a>を提供する。Open3DはC++/Pythonをサポート、並列化にも対応しており、クラウドで開発することが可能。
                点群読み込み-ダウンサンプリング-法線の計算、シーン再構築、3次元可視化などの処理が含まれている。
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180205open3d.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">3次元画像処理のコミュニティにて有益なオープンソースを提供し、その<a href="https://github.com/IntelVCL/Open3D" target="blank">コード</a>も提供されている。</div>
            </div>
            <div class="item4">
              <h1>コメント・リンク集</h1>
              <div class="text">
                <ul>
                  <li><a href="http://www.open3d.org/" target="blank">Project page</a></li>
                  <li><a href="https://arxiv.org/pdf/1801.09847.pdf" target="blank">[論文] Open3D: A Modern Library for 3D Data Processing</a></li>
                  <li><a href="https://github.com/IntelVCL/Open3D" target="blank">Code</a></li>
                  <li><a href="http://hirokatsukataoka.net/" target="blank">slide by Hirokatsu Kataoka</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#2]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Hierarchical Variational Autoencoders for Music</div>
            <div class="info">A. Roberts et al., NIPS WS on Machine Learning for Creativity and Design, 2017.</div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                音楽を生成するためのHierarchical Variational Autoencoders (VAE) を提案．
                エンコーダとデコーダがLSTMで構成されているReccurent VAEがベース．<a href="https://goo.gl/twGuP2">結果サンプル</a>長い音楽（実験では32小節）を単純なLSTMデコーダで生成するのは難しいので，
                この研究では複数のLSTMを階層的に重ねて，段階的に長くしていくHierarchical VAEを提案．
                ループメロディの外挿や，メロディの生成，3ピース構成の音楽生成の実験で性能を検討．
                結果の音楽やコードは公開されている．
              </div>
            </div>
            <div class="item2"><img src="slides/figs/hierarchical_vae.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>階層的なLSTMによるデコーダをVAEによる音楽生成に導入</li>
                  <li><a href="https://goo.gl/twGuP2">結果サンプル</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>これも長期的な構成を考えて生成することはできていない</li>
                  <li>Future Workにはインタフェースを作るとあるし，1曲まるごと作るというよりは適当にサンプルを出して作曲家のアイデアを促進することを目指しているのかな．</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#3]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Generating the Future with Adversarial Transformers</div>
            <div class="info">C. Vondrick et al., CVPR, 2017.</div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                未来の動画を予測して生成する手法を提案．
                4フレーム x 64画素 x 64画素のクリップを入力として，その後の16フレームの動画を生成．
                完全に新しいフレームを生成するのは難しいので，入力フレームの変換により未来のフレームを生成するのがポイント．
                論文の主張としては，きれいな動画を作るにはLow-Levelな情報が重要だけど，未来予測のためにはHigh-Levelな理解も必要で，
                その両者を一つのネットワークで一気に学習するのは難しいとしている．
                だから，Low-Levelな情報は元のフレームを変換することで引っ張ってきて，ネットワークはHigh-Levelな特徴抽出に集中させるのが良いとのこと．
                このネットワークの学習はGANベース．
                生成動画の主観評価や可視化，Generatorの特徴を利用した物体認識タスクなどで性能を評価．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/generating_future.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>元のフレームからの変換により未来のフレームを生成する手法を提案</li>
                  <li>未来の動画生成において，敵対的学習により大規模な教師なしデータを利用した学習を実現</li>
                  <li>直接動画を生成したり，回帰誤差で学習したりする手法よりも良いことを主観評価実験で確認          </li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>入力が4フレームだけだけど，もっと増やすと性能は変わるのか気になる</li>
                  <li>主観評価で本物と比較すると提案手法が一番嫌われている率が高いのもちょっと気になる</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#4]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">DensePose: Dense Human Pose Estimation In The Wild</div>
            <div class="info">Rıza Alp Guler et al.</div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                身体の表面形状まで考慮したDenseな姿勢推定手法に関する研究。サーフェイスモデルを提供するSMPLタイプとアノテーションベースのMSCOCOタイプを提供。手法はMask RCNN（w/ ResNet-50, ROI-align, Regression）をベースに構築している。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180205densepose.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>DenseReg [Guler,CVPR17]は顔表面の推定に対して、本研究では身体全体の表面やデンスなポイントを回帰。</li>
                  <li>SMPLやDense-COCOのデータセットを構築</li>
                  <li>非拘束（in the wild）の環境にてDensePoseを学習。</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>リアルが完全に崩壊した。（Face2Faceの全身モデル版が実現可能になった？）</li>
                  <li>CG/UIの分野との親和性がより高くなった</li>
                  <li><a href="https://arxiv.org/abs/1802.00434" target="blank">DensePose: Dense Human Pose Estimation In The Wild</a></li>
                  <li><a href="https://arxiv.org/abs/1612.01202" target="blank">DenseReg</a></li>
                  <li><a href="http://smpl.is.tue.mpg.de" target="blank">SMPL</a></li>
                  <li><a href="http://hirokatsukataoka.net/" target="blank">slide by Hirokatsu Kataoka</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#5]</div>
          </div>
        </section>
      </div>
    </div>
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>
    <script>
      Reveal.initialize({
        history: true,
        center: false,
        width: '100%',
        height: '100%',
        transition: 'none',
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
  </body>
</html>