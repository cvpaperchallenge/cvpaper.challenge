<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>cvpaper.challenge</title>
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/white.css">
    <link rel="stylesheet" href="css/layout.css">
    <link rel="stylesheet" href="lib/css/zenburn.css">
  </head>
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
    
  </script>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <div class="paper-abstract">
            <div class="title">We Are Humor Beings: Understanding and Prediction Visual Humor</div>
            <div class="info">
              <div class="authors">Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Virginia Tech, TTI-Chicago, Facebook AI Research</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                <ul>
                  <li>視覚とユーモアの関係をモデル化(不調和説に基づく)</li>
                  <li>アニメ画像の面白さを推定</li>
                  <li>画像のオブジェクトと面白さの関連性を推定</li>
                  <li>データセットの作成</li>
                  <li>抽象的なシーンを使用したユーモアを引き起こすシーンの理解</li>
                </ul>
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180205HumorBeings.jpg"></div>
            <div class="item3">
              <h1>手法1</h1>
              <div class="text">
                <ul>
                  <li>面白さ推定</li>
                  <li>特徴抽出し，重み付き誤差平均(面白さによって誤差を変える)</li>
                  <li>Abstract Scenesのアニメ画像(VQAの画像とクラウドソーシングで依頼した面白い画像)に5段階評価したデータセット(AVH)</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>手法2</h1>
              <div class="text">
                <ul>
                  <li>面白い画像・面白くない画像の変換</li>
                  <li>オブジェクトを変更することで，面白い⇔面白くない画像に相互変換</li>
                  <li>AVHから面白いものを選び，クラウドソーシングで依頼した面白くない画像を使い/面白い画像ペアを作成</li>
                  <li>どのオブジェクトが面白さに影響しているか調査</li>
                  <li>結果：特に人や動物などのオブジェクトが面白さに影響</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#1]</div>
            <div class="slide_editor">Munetaka Minoguchi</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Vehicle Three-Dimensional Pose and Shape Estimation from Multiple Monocular Vision</div>
            <div class="info">
              <div class="authors">Wenhao Ding, Shuaijun Li, Guilin Zhang, Xiangyu Lei, Huihuan Qian, Yangsheng Xu</div>
              <div class="conference">arXiv:1802.03515</div>
            </div>
            <div class="slide_editor">Hirokatsu Kataoka</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>オーバーラップが少ない複数視点カメラから自動車の3次元姿勢や形状を復元する研究。CNNにより自動車のキーポイントや姿勢/3次元形状を出力する。これら情報をヒントに、カメラ視点を推定する。2D画像上でのキーポイント推定にはconv-de-convを4回繰り返すhourglassアーキテクチャを採用、3次元姿勢や形状の推定にはCross Projection Optimization (CPO)を採用し2D-3Dの投影誤差を最小化した。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180302Vehicle3DPose.png" alt="180302Vehicle3DPose"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>物体のキーポイント検出においてState-of-the-art。6DoF推定手法 (Pavlakos, ICRA17)では12キーポイントの平均誤差が37.88であったが、提案手法では10.48まで低減した。また、回転/並進誤差も3DVP (Xiang+, CVPR15)では11.18/N/Aであったが、2.87/4.73まで向上させた。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1802.03515.pdf">PDF</a></li>
                  <li><a href="https://arxiv.org/abs/1703.04670">6-DoF Object Pose from Semantic Keypoints</a></li>
                  <li><a href="http://cvgl.stanford.edu/papers/xiang_cvpr15_3dvp.pdf">Data-Driven 3D Voxel Patterns for Object Category Recognition</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#2]</div>
            <div class="timestamp">2018.3.2 10:15:30</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Joint Event Detection and Description in Continuous Video Streams</div>
            <div class="info">
              <div class="authors">Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko</div>
              <div class="conference">arXiv:1802.10250</div>
            </div>
            <div class="slide_editor">Hirokatsu Kataoka</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>End-to-Endでイベント検出（行動の時系列セグメント化）とキャプショニングを実行するタスクを提供する。モデルには3D Convolutionや階層的LSTM（two-level hierarchical LSTM）を採用した。基本的には<a href="https://cs.stanford.edu/people/ranjaykrishna/densevid/">Dense Captioning Events in Videos</a>をベースにして研究を行なっているが、{Controller, Captioner} LSTMの二段階により前の候補のセンテンスやビデオコンテキストを考慮しつつ状態を更新（Controller LSTM）し、候補領域の特徴を参照しつつキャプションを生成（Captioner LSTM）する。時系列候補領域とキャプションはmulti-task学習、End-to-Endで学習される。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180302VideoCaption.png" alt="180302VideoCaption"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>候補領域生成やキャプショニングの精度を検証した。また、データセットにはActivityNet CaptionsやTACoS Datasetを用いた。候補領域については従来法のDAPが30, multi-scale DAPが38 @AUC (IoU>0.8)に対して提案法であるJEDDi-Netは58.21を記録した。また、キャプショニングについては従来法が{17.95, 4.82, 17.29} (各BLEU1, METEOR, CIDEr)の問題に対して{19.97, 8.58, 19.88}を記録した。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント/リンク集</h1>
                <p>動画のタスクはカテゴリのみでなく言語やより表現力豊かな認識ができなければいけない時期になって来た？時系列表現にもまだまだ課題が多いので、これからさらに動画認識にチャレンジすべき。また、キャプショニングの問題は感性評価に対する知見や確固たる評価方法が確立されるとさらに面白くなるのではないか。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1802.10250.pdf">PDF</a></li>
                  <li><a href="https://www.bu.edu/cs/profiles/kate-saenko/">Kate Saenko</a></li>
                  <li><a href="https://cs.stanford.edu/people/ranjaykrishna/densevid/">Dense Captioning Events in Videos</a></li>
                  <li><a href="https://github.com/kenshohara/3D-ResNets-PyTorch">3D Convolution (3D-ResNets-PyTorch)</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#3]</div>
            <div class="timestamp">2018.3.2 09:36:26</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Neural Aesthetic Image Reviewer</div>
            <div class="info">
              <div class="authors">W. Wang, et al.</div>
              <div class="conference">arXiv:1802.10240</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>AIが写真の感性評価やコメント生成を行なってくれる。写真とそのコメントが対応づけられた大規模DBであるAVA-Reviews dataset（52,118画像、312,708コメント）を学習することで写真を入力して図の(1)Predictionや(2)Commentsのようなものが得られる。モデルはCNNにより感性評価（Low-/High-Aesthetic category）を、CNN+LSTM（RNN）によりコメント（e.g. Fastastic colors）を出力する。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180301NeuralAesthetic.png" alt="180301NeuralAesthetic"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>(i) 人間のような画像に対する感性評価（image aesthetics）をコンピュータに実装した。(ii)自然言語の出力により人間の高次な認知能力を実現。(iii) 画像-言語の組み合わせによるデータセットAVA-Reviews datasetを新規に構築した。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1802.10240.pdf">PDF</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#4]</div>
            <div class="timestamp">2018.3.2 08:39:04</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Interpreting CNN Knowledge via an Explanatory Graph</div>
            <div class="info">
              <div class="authors">Q. Zhang, et al.</div>
              <div class="conference">AAAI 2018</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  深層学習の解釈性に関する論文であり、畳み込み層の特徴マップの応答を外的に解析して対応する反応を可視化。畳み込みの各フィルタが異なる部位（e.g. 馬の耳や目）に反応するので、グラフにより解析して元画像の対象位置にアクセス。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180301interpretability.png" alt="180301interpretability.png"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>
                  Ground-truthなしに各部位に関する解釈性を与えたことが新規性である。図に示すように入力画像に対するパーツごとの解析をフィルタの反応やグラフの解析から可視化することができる。
                  さらに、異なる画像間においても一貫性のある反応を得ることができた。
                </p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>自由記述欄</h1>
                <p>
                  深層学習は教師なしによる解釈性を獲得しているが、まだ反応している部分の可視化や部分ごとの解析が進んでいるにすぎない。さらなる発展のためには、言語的な解釈や人間にわかりやすい加工（イラストとか？）が必要になるのではないだろうか。</p>
              </div>
            </div>
            <div class="slide_index">[#5]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">HemaApp: Noninvasive Blood Screening of Hemoglobin using Smartphone Cameras</div>
            <div class="info">
              <div class="authors">E. J. Wang et al.,</div>
              <div class="conference">Ubicomp 2016</div>
            </div>
            <div class="slide_editor">Kensho Hara</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  スマホカメラを使って非侵襲なヘモグロビン濃度の測定を実現．血中の酸素飽和度の測定などはこれまでにもあったがヘモグロビン濃度まで測定できているものはなかった．
                  照明条件とRGBの変化からヘモグロビン濃度を推定するためのアルゴリズムを提案．
                </p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180228_hemaapp.png" alt="180228_hemaapp.png"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>特別な装置を使うことなく簡単にスマホカメラでヘモグロビン濃度測定を実現した点．</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>自由記述欄</h1>
                <p>システムやユーザスタディの完成度や完全に実現できたときの有用性が評価された？</p>
              </div>
            </div>
            <div class="slide_index">[#6]</div>
            <div class="timestamp">2018.3.1 10:44:03</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">End-to-end Driving via Conditional Imitation Learning</div>
            <div class="info">
              <div class="authors">Felipe Codevilla, Matthias Müller, Alexey Dosovitskiy, Antonio López, and Vladlen Koltun,</div>
              <div class="conference">arXiv</div>
              <div class="paper_id">1710.02410</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">自動運転を模倣学習により行う手法を提案。実空間での学習結果をヴァーチャルな空間での自動運転にて再現することができた。RGB画像、計測（e.g. スピード）や命令（e.g. turn right）などからステアリング、アクセル、ブレーキなどのコマンドを出力して自動車を操作する。</div>
            </div>
            <div class="item2"><img src="slides/figs/180205conditionalimitationlearning.png"><img src="slides/figs/180205framework_imitation.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>模倣学習による自動運転を実現した。</li>
                  <li>実空間とシミュレーションベースの転移を行うことにも成功。</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>リンク集</h1>
              <div class="text">
                <ul>
                  <li>自動運転の学習はシミュレーションベースで完結してしまう可能性がある。</li>
                  <li>メタ学習/模倣学習/強化学習などはCVに徐々に取り入れられてくるはずなので、2018年は学習しておいて損はない？</li>
                  <li><a href="https://arxiv.org/pdf/1710.02410.pdf" target="blank">[論文] End-to-end Driving via Conditional Imitation Learning</a></li>
                  <li><a href="https://www.youtube.com/watch?v=cFtnflNe5fM" target="blank">YouTube</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#7]</div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">slide by Hirokatsu Kataoka</a></div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Open3D: A Modern Library for 3D Data Processing</div>
            <div class="info">
              <div class="authors">Qian-Yi Zhou et al.,</div>
              <div class="conference">arXiv</div>
              <div class="paper_id">1801.09847</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">3Dデータを取り扱い、迅速な開発を可能にする<a href="http://www.open3d.org/" target="blank">Open3D</a>を提供する。Open3DはC++/Pythonをサポート、並列化にも対応しており、クラウドで開発することが可能。
                点群読み込み-ダウンサンプリング-法線の計算、シーン再構築、3次元可視化などの処理が含まれている。
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180205open3d.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">3次元画像処理のコミュニティにて有益なオープンソースを提供し、その<a href="https://github.com/IntelVCL/Open3D" target="blank">コード</a>も提供されている。</div>
            </div>
            <div class="item4">
              <h1>コメント・リンク集</h1>
              <div class="text">
                <ul>
                  <li><a href="http://www.open3d.org/" target="blank">Project page</a></li>
                  <li><a href="https://arxiv.org/pdf/1801.09847.pdf" target="blank">[論文] Open3D: A Modern Library for 3D Data Processing</a></li>
                  <li><a href="https://github.com/IntelVCL/Open3D" target="blank">Code</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#8]</div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">slide by Hirokatsu Kataoka</a></div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Hierarchical Variational Autoencoders for Music</div>
            <div class="info">
              <div class="authors">A. Roberts et al.,</div>
              <div class="conference">NIPS WS on Machine Learning for Creativity and Design, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                音楽を生成するためのHierarchical Variational Autoencoders (VAE) を提案．
                エンコーダとデコーダがLSTMで構成されているReccurent VAEがベース．<a href="https://goo.gl/twGuP2">結果サンプル</a>長い音楽（実験では32小節）を単純なLSTMデコーダで生成するのは難しいので，
                この研究では複数のLSTMを階層的に重ねて，段階的に長くしていくHierarchical VAEを提案．
                ループメロディの外挿や，メロディの生成，3ピース構成の音楽生成の実験で性能を検討．
                結果の音楽やコードは公開されている．
              </div>
            </div>
            <div class="item2"><img src="slides/figs/hierarchical_vae.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>階層的なLSTMによるデコーダをVAEによる音楽生成に導入</li>
                  <li><a href="https://goo.gl/twGuP2">結果サンプル</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>これも長期的な構成を考えて生成することはできていない</li>
                  <li>Future Workにはインタフェースを作るとあるし，1曲まるごと作るというよりは適当にサンプルを出して作曲家のアイデアを促進することを目指しているのかな．</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#9]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Generating the Future with Adversarial Transformers</div>
            <div class="info">
              <div class="authors">C. Vondrick et al.,</div>
              <div class="conference">CVPR, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                未来の動画を予測して生成する手法を提案．
                4フレーム x 64画素 x 64画素のクリップを入力として，その後の16フレームの動画を生成．
                完全に新しいフレームを生成するのは難しいので，入力フレームの変換により未来のフレームを生成するのがポイント．
                論文の主張としては，きれいな動画を作るにはLow-Levelな情報が重要だけど，未来予測のためにはHigh-Levelな理解も必要で，
                その両者を一つのネットワークで一気に学習するのは難しいとしている．
                だから，Low-Levelな情報は元のフレームを変換することで引っ張ってきて，ネットワークはHigh-Levelな特徴抽出に集中させるのが良いとのこと．
                このネットワークの学習はGANベース．
                生成動画の主観評価や可視化，Generatorの特徴を利用した物体認識タスクなどで性能を評価．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/generating_future.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>元のフレームからの変換により未来のフレームを生成する手法を提案</li>
                  <li>未来の動画生成において，敵対的学習により大規模な教師なしデータを利用した学習を実現</li>
                  <li>直接動画を生成したり，回帰誤差で学習したりする手法よりも良いことを主観評価実験で確認          </li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>入力が4フレームだけだけど，もっと増やすと性能は変わるのか気になる</li>
                  <li>主観評価で本物と比較すると提案手法が一番嫌われている率が高いのもちょっと気になる</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#10]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">DensePose: Dense Human Pose Estimation In The Wild</div>
            <div class="info">
              <div class="authors">Rıza Alp Guler et al.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                身体の表面形状まで考慮したDenseな姿勢推定手法に関する研究。サーフェイスモデルを提供するSMPLタイプとアノテーションベースのMSCOCOタイプを提供。手法はMask RCNN（w/ ResNet-50, ROI-align, Regression）をベースに構築している。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180205densepose.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>DenseReg [Guler,CVPR17]は顔表面の推定に対して、本研究では身体全体の表面やデンスなポイントを回帰。</li>
                  <li>SMPLやDense-COCOのデータセットを構築</li>
                  <li>非拘束（in the wild）の環境にてDensePoseを学習。</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>リアルが完全に崩壊した。（Face2Faceの全身モデル版が実現可能になった？）</li>
                  <li>CG/UIの分野との親和性がより高くなった</li>
                  <li><a href="https://arxiv.org/abs/1802.00434" target="blank">DensePose: Dense Human Pose Estimation In The Wild</a></li>
                  <li><a href="https://arxiv.org/abs/1612.01202" target="blank">DenseReg</a></li>
                  <li><a href="http://smpl.is.tue.mpg.de" target="blank">SMPL</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#11]</div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">slide by Hirokatsu Kataoka</a></div>
          </div>
        </section>
      </div>
    </div>
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>
    <script>
      Reveal.initialize({
        history: true,
        center: false,
        width: '100%',
        height: '100%',
        transition: 'none',
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
  </body>
</html>