extends layouts/_layout

block slides
  +slide
    .title Learning Features by Watching Objects Move 
    .info
      .authors Ying Zhang, Tao Xiang, Timothy M. Hospedales, Huchuan Lu
      .conference 2017
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 教師モデルと生徒モデルを分けていた従来の蒸留に対してモデル同士の相互学習を提案。ハードラベルによる交差エントロピーと対象モデル以外のモデルの出力とのKL距離を最小化するように学習する。様々なモデル同士の相互学習実験や通常の蒸留との比較、相互学習を行った場合の解がより高い汎化性能を保有していることの検証実験も行っている。

        
    .item2
      .text
        p
          img(src=`${figpath}deep_mutual2.png`)
          img(src=`${figpath}deep_mutual3.png`)
          
    .item3
      .text
        h1 新規性・結果
        p 画像識別において通常の蒸留を行うよりも精度が良くなった。生徒モデルの中で相対的に小規模なモデルのみならず大規模なモデルも独立で学習を行うより精度が良かった。さらに相互学習を行うことで、wider minimaに収束しているという実験結果萌えたれた。特に出力される事後確率のエントロピーが大きくなるように学習されることがwider minimaへの収束を促していることがいわれている。
        
    .item4
      .text
        h1 自由記述欄
        p ハードラベルありき（ないと相互学習が正しい方向に向かわない）の手法であったが、教師なし手法に拡張できたら面白くなりそうだと感じる。
        ul
          li
            a(href="https://arxiv.org/abs/1706.00384") 論文
    .slide_index #{getSlideIndex()}
  
  +slide
    .title Learning Features by Watching Objects Move 
    .info
      .authors Deepak Pathak, Ross Girshick, Piotr Dollar, Trevor Darrell, and Bharath Hariharan
      .conference CVPR 2017
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 動き特徴を利用した前景（物体）領域情報は汎用的な表現学習に役立つという考えから、NLCなどのhand-craftな手法を組み合わせて擬似的な動体領域を作成し、それを教師として領域分割をCNNに解かせることで表現特徴を得る。物体検出、物体・行動認識、意味領域分割の問題設定において評価を行った。表現学習のデータとしてYFCCを用いている。
        
    .item2
      .text
        p
          img(src=`${figpath}watching_object.png`, alt="180302AffordanceNet")
          img(src=`${figpath}watching_object3.png`, alt="180302AffordanceNet")
          img(src=`${figpath}watching_object2.png`, alt="180302AffordanceNet")
          
    .item3
      .text
        h1 新規性・結果
        p Pascal VOCの物体検出において教師なし表現学習でSoTA。特にfine-tuningに利用するデータが少量の場合と多くの層のパラメータを固定してfine-tuinigした場合で大きな効果を発揮した。しかし、物体・行動認識、意味領域分割においては従来手法より劣っている。
        
    .item4
      .text
        h1 自由記述欄
        p 実験は丁寧に行われてる印象。表現学習の設定自体が物体検出を意識しているようにも感じられ（単一物体が写っている画像を優先的に取り出している？など）、物体検出でうまくいくのは当たり前な気もした。
        |しかし、意味領域分割で精度が出ない原因がよくわからなかった（物体部分はできているが背景の分割ができていない？）。
        ul
          li
            a(href="https://arxiv.org/abs/1612.06370") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Focal Loss for Dense Object Detection
    .info
      .authors Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar, Facebook AI Research (FAIR)
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 1-stage物体検出手法の精度向上を図る。 YOLOやSSDなどは，矩形領域における前景と背景の面積が不均衡であるため，2-stage物体検出手法に勝てないと推測。この問題を解決するためにクロスエントロピーを再構築したFocal Lossを提案。学習時のネガティブサンプルの影響を減らすことができる。
    .item2
      .text
        p
          img(src=`${figpath}180303FocalLoss.jpg`)
    .item3
      .text
        h1 手法
        p Focal Loss
        p クロスエントロピーに重みを追加
        ul
          li 正解の場合には重みを低減
          li 不正解の場合には従来通り
        p →正解になりやすい背景に引っ張られなくなる
    .item4
      .text
        h1 結果・リンク集
        p 既存の最先端2ステージ検出器(2017年現在)の全てにおいて精度を上回り，既存の1ステージ検出器の検出速度と同等
        ul
          li
            a(href="https://arxiv.org/pdf/1708.02002.pdf") 論文
          li
            a(href="https://github.com/facebookresearch/Detectron") ソースコード

    .slide_index #{getSlideIndex()}

  +slide
    .title Towards Diverse and Natural Image Descriptions via a Conditional GAN
    .info
      .authors Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 画像キャプショニングの性能向上を図る。従来のロバスト性が低いRNNに代わって，GANのフレームワークを採用することで，自然性と多様性を向上。図より，ジェネレータ(G)が文を生成し，ディスクリミネータ(E)が文や段落がどれだけうまく記述されているかを評価する。GとEを同時に学習させることにより，自然な文章を生成。
    .item2
      .text
        p
          img(src=`${figpath}180303TowardsDiverse_1.jpg`)
    .item3
      .text
        h1 結果・リンク集
        ul
          li 人間，G-MLE，G-GAN(本手法)の3つを比較して性能評価
          li ユーザー調査、定性的な例、および検索アプリケーションなどの評価により，より自然かつ多様、意味的に関連する記述を実現
        ul
          li
            a(href="https://arxiv.org/pdf/1703.06029.pdf") 論文
          li
            a(href="https://arxiv.org/abs/1411.4555") MLE
    .item4
      .text
        p
          img(src=`${figpath}180303TowardsDiverse_2.jpg`)

    .slide_index #{getSlideIndex()}

  +slide
    .title Learning to Segment Every Thing
    .info
      .authors Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ラベルが完全に手に入らない際にでも転移学習が可能なセグメンテーション手法（論文中ではPartially Supervised Training Paradigm, weight transfer functionを紹介）を提案する。条件として、bboxが手に入っている物体に対してセグメンテーション領域を学習可能。Mask R-CNNをベースとしているが、Weight Transfer Functionを追加、セグメントの重みを学習・推定して誤差計算と学習繰り返し。
    .item2
      .text
        p
          img(src=`${figpath}180303SegmentEverything.png`,alt="180303SegmentEverything")
    .item3
      .text
        h1 新規性・結果
        p Visual Genome Datasetから3,000の視覚的概念を獲得、MSCOCOから80のマスクアノテーションを獲得した。
    .item4
      .text
        h1 コメント・リンク集
        p 弱教師付き学習が現実的な精度で動作するようになってきた？アノテーションはお金や知識があっても非常に大変なタスクであり、いかに減らすかという方向に研究が進められている。（What's next?ー弱教師/教師なしの先とは？）
        ul
          li
            a(href="https://arxiv.org/pdf/1711.10370.pdf") 論文
          li
            a(href="http://ronghanghu.com/") 著者
          li
            a(href="http://kaiminghe.com/") Kaiming He
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.3 10:46:40

  +slide
    .title One-Shot Visual Imitation Learning via Meta-Learning
    .info
      .authors Chelsea Finn*, Tianhe Yu*, Tianhao Zhang, Pieter Abbeel, and Sergey Levine
      .conference NIPS 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ワンショットで人間の教示を模倣するロボットのための学習「One-shot Imitation Learning」を提案。人間が物体を把持するなど動作を教示するとロボットが特徴や動作を学習してタスクをこなす様子を学習。Model-Agnostic Meta-Learning（MAML; ICML2017）を応用したモデルを提案し、（VR空間、人間のデモによる）教示から動作を学習する。アーキテクチャはCNNをベースとしてRGB入力から特徴を抽出、中間層（全結合層直前）からロボットの動作やバイアス項を入力してロボットの行動（pre-/post-update）を出力する。
    .item2
      .text
        p
          img(src=`${figpath}180303OneshotImitation.png`,alt="180303OneshotImitation")
    .item3
      .text
        h1 新規性・結果
        p ロボットの把持タスクをシミュレーション/実空間にておこなった。シミュレーションでは提案法のMILが1ショットで85.81%、5ショットで88.75%（従来法LSTMでは各78.38%, 83.11%）。実空間では90%を実現（従来法LSTM/contextualでは25%）。詳細にはプロジェクトページやプレゼンのビデオを参照。
    .item4
      .text
        h1 コメント・リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1709.04905.pdf") 論文
          li
            a(href="https://sites.google.com/view/one-shot-imitation") Project
          li
            a(href="https://github.com/tianheyu927/mil") GitHub
          li
            a(href="https://vimeo.com/252186304") Presen
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.3 10:13:42

  +slide
    .title BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography
    .info
      .authors Michael J. Wilber, Chen Fang, Hailin Jin, Aaron Hertzmann, John Collomosse, Serge Belongie
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p CVにおけるデータセットでは，写真に対するラベル付けが一般的。写真だけでなく，イラストや風景画などに対して，以下の3属性のラベルを付加。
        ul
          li メディア：漫画、油絵、鉛筆スケッチ、水彩画などで作成した画像にラベル付け
          li 感情：視聴者に平静、幸せ/陽気、悲しい/悲観的、恐ろしい/恐れを感じさせるような画像にラベル付け
          li オブジェクト：自転車、車、猫、犬、花、人などの画像にラベル付け
    .item2
      .text
        p
          img(src=`${figpath}180302BehanceArtisticMedia.jpg`)
    .item3
      .text
        h1 手法
        p Behance Artistic Media Dataset
        ul
          li ラベル作成にhuman-in-the-loopを採用し，人間とコンピュータのハイブリッドを図る。
          li 全てのラベルについて学習し，ランク付けを行う。その後，高い順位の画像を人がラベル付け。これを4回繰り返す。
          li 基本的にはLSUNの方法に基づいている(リンク参照)
    .item4
      .text
        h1 結果・リンク集
        p 物体認識や物体検出，画像の類似度，属性推定など様々なタスクの機械学習実験を実施。定性的な評価にはなるが，明らかにVOCやImageNetなどの既存のデータセットよりも広い表現の画像で多くのタスクが処理可能となる。
        ul
          li
            a(href="https://arxiv.org/pdf/1704.08614.pdf") 論文
          li
            a(href="http://behance.net") Behence
          li
            a(href="https://arxiv.org/abs/1506.03365") LSUN
    .slide_index #{getSlideIndex()}

  +slide
    .title AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection
    .info
      .authors Thanh-Toan Do, Anh Nguyen, Ian Reid, Darwin G. Caldwell, Nikos G. Tsagarakis
      .conference ICRA 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 物体検出とアフォーダンス（というよりは機能？）のセグメントを同時に回帰するネットワーク、AffordanceNetに関する論文。ロボットへの把持位置/機能教示を行うことができる。基本的なモデルはMask R-CNNを適用していて、物体検出のためのbboxと物体に対する機能セグメントを正解として学習する。多タスクの誤差関数は物体カテゴリ、座標、機能セグメントの3つに関するものである。
    .item2
      .text
        p
          img(src=`${figpath}180302AffordanceNet.png`,alt="180302AffordanceNet")
    .item3
      .text
        h1 新規性・結果
        p 従来、物体検知と機能推定は別個に学習・認識されていたが、本研究では多タスク学習の枠組みで、単一モデルにてEnd-to-End学習した。IIT-AFF Datasetにて73.35（SoTAは69.62）、UMD Datasetにて79.9（SoTAは77.0）。モデルも公開されており、誰もがAffordanceNetを実装できるようにしている。
    .item4
      .text
        h1 コメント・リンク集
        p 任意のセグメンテーションラベルさえあれば、物体検知とあらゆる高次なセグメンテーションモデルが実現可能となった。
        ul
          li
            a(href="https://arxiv.org/pdf/1709.07326.pdf") 論文
          li
            a(href="https://github.com/nqanh/affordance-net") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 20:38:13


  +slide
    .title Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN
    .info
      .authors Jiyang Gao, Zijian (James) Guo, Zhen Li, Ram Nevatia
      .conference arXiv:1711.07607
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 画像識別における知識蒸留（Knowledge Distillation）の内容である。本論文ではある概念（e.g. 動物認識、人工物認識）ごとに教師となる識別器を事前学習しておき、それらの知識を単一の識別器に学習（これをKnowledge Concentrationと呼称）する。いわば複数の先生がある生徒に教えるという流れで学習する。
    .item2
      .text
        p
          img(src=`${figpath}180302KnowledgeConcentration.png`,alt="180302KnowledgeConcentration")
    .item3
      .text
        h1 新規性・結果
        p よくも悪くも、現状のCVはImageNetの1,000カテゴリに頼っているが、これを100倍の100,000カテゴリに増やして学習したらどうか？また、いかにしたら効率よく学習ができるかを検討した。結果はSingle Model（従来のようなCNNによるカテゴリ識別学習）にするよりも複数のスペシャリスト識別器から知識蒸留を行う方が効率よく、精度よく学習ができた。本論文で使用したEFT（Entity-Foto-Tree）データセットはカテゴリ数でImageNetの100倍、JFT-300Mの5倍である。
    .item4
      .text
        h1 コメント・リンク集
        p 読んだだけでGoogleの研究所であることがわかってしまう論文。実行ができるかどうかはさておき、大規模データの扱いやCNN学習の効率化という意味でも精読すべき論文。
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07607.pdf") 論文
          li
            a(href="https://jiyanggao.github.io/") 著者
          li
            a(href="http://codecrafthouse.jp/p/2018/01/knowledge-distillation/") 知識蒸留の参考
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 20:04:42


  +slide
    .title We Are Humor Beings: Understanding and Prediction Visual Humor
    .info
      .authors Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Virginia Tech, TTI-Chicago, Facebook AI Research
      .conference CVPR 2016
    .item1
      h1 概要
      .text
        ul
          li 視覚とユーモアの関係をモデル化(不調和説に基づく)
          li アニメ画像の面白さを推定
          li 画像のオブジェクトと面白さの関連性を推定
          li データセットの作成
          li 抽象的なシーンを使用したユーモアを引き起こすシーンの理解
    .item2
      img(src=figpath+"180205HumorBeings.jpg")
    .item3
      h1 手法1
      .text
        p 面白さ推定
        ul
          li 特徴抽出し，重み付き誤差平均(面白さによって誤差を変える)
          li Abstract Scenesのアニメ画像(VQAの画像とクラウドソーシングで依頼した面白い画像)に5段階評価したデータセット(AVH)
    .item4
      h1 手法2
      .text
        p 面白い画像・面白くない画像の変換
        ul
          li オブジェクトを変更することで，面白い⇔面白くない画像に相互変換
          li AVHから面白いものを選び，クラウドソーシングで依頼した面白くない画像を使い/面白い画像ペアを作成
          li どのオブジェクトが面白さに影響しているか調査
        p 結果：特に人や動物などのオブジェクトが面白さに影響
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Munetaka Minoguchi

  +slide
    .title Vehicle Three-Dimensional Pose and Shape Estimation from Multiple Monocular Vision
    .info
      .authors Wenhao Ding, Shuaijun Li, Guilin Zhang, Xiangyu Lei, Huihuan Qian, Yangsheng Xu
      .conference arXiv:1802.03515
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p オーバーラップが少ない複数視点カメラから自動車の3次元姿勢や形状を復元する研究。CNNにより自動車のキーポイントや姿勢/3次元形状を出力する。これら情報をヒントに、カメラ視点を推定する。2D画像上でのキーポイント推定にはconv-de-convを4回繰り返すhourglassアーキテクチャを採用、3次元姿勢や形状の推定にはCross Projection Optimization (CPO)を採用し2D-3Dの投影誤差を最小化した。
    .item2
      .text
        p
          img(src=`${figpath}180302Vehicle3DPose.png`,alt="180302Vehicle3DPose")
    .item3
      .text
        h1 新規性・結果
        p 物体のキーポイント検出においてState-of-the-art。6DoF推定手法 (Pavlakos, ICRA17)では12キーポイントの平均誤差が37.88であったが、提案手法では10.48まで低減した。また、回転/並進誤差も3DVP (Xiang+, CVPR15)では11.18/N/Aであったが、2.87/4.73まで向上させた。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1802.03515.pdf") PDF
          li
            a(href="https://arxiv.org/abs/1703.04670") 6-DoF Object Pose from Semantic Keypoints
          li
            a(href="http://cvgl.stanford.edu/papers/xiang_cvpr15_3dvp.pdf") Data-Driven 3D Voxel Patterns for Object Category Recognition
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 10:15:30


  +slide
    .title Joint Event Detection and Description in Continuous Video Streams
    .info
      .authors Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko
      .conference arXiv:1802.10250
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p End-to-Endでイベント検出（行動の時系列セグメント化）とキャプショニングを実行するタスクを提供する。モデルには3D Convolutionや階層的LSTM（two-level hierarchical LSTM）を採用した。基本的には
          a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
          |をベースにして研究を行なっているが、{Controller, Captioner} LSTMの二段階により前の候補のセンテンスやビデオコンテキストを考慮しつつ状態を更新（Controller LSTM）し、候補領域の特徴を参照しつつキャプションを生成（Captioner LSTM）する。時系列候補領域とキャプションはmulti-task学習、End-to-Endで学習される。
    .item2
      .text
        p
          img(src=`${figpath}180302VideoCaption.png`,alt="180302VideoCaption")
    .item3
      .text
        h1 新規性・結果
        p 候補領域生成やキャプショニングの精度を検証した。また、データセットにはActivityNet CaptionsやTACoS Datasetを用いた。候補領域については従来法のDAPが30, multi-scale DAPが38 @AUC (IoU>0.8)に対して提案法であるJEDDi-Netは58.21を記録した。また、キャプショニングについては従来法が{17.95, 4.82, 17.29} (各BLEU1, METEOR, CIDEr)の問題に対して{19.97, 8.58, 19.88}を記録した。
    .item4
      .text
        h1 コメント/リンク集
        p 動画のタスクはカテゴリのみでなく言語やより表現力豊かな認識ができなければいけない時期になって来た？時系列表現にもまだまだ課題が多いので、これからさらに動画認識にチャレンジすべき。また、キャプショニングの問題は感性評価に対する知見や確固たる評価方法が確立されるとさらに面白くなるのではないか。
        ul
          li
            a(href="https://arxiv.org/pdf/1802.10250.pdf") PDF
          li
            a(href="https://www.bu.edu/cs/profiles/kate-saenko/") Kate Saenko
          li
            a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
          li
            a(href="https://github.com/kenshohara/3D-ResNets-PyTorch") 3D Convolution (3D-ResNets-PyTorch)
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 09:36:26

  +slide
    .title Neural Aesthetic Image Reviewer
    .info
      .authors W. Wang, et al.
      .conference arXiv:1802.10240
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p AIが写真の感性評価やコメント生成を行なってくれる。写真とそのコメントが対応づけられた大規模DBであるAVA-Reviews dataset（52,118画像、312,708コメント）を学習することで写真を入力して図の(1)Predictionや(2)Commentsのようなものが得られる。モデルはCNNにより感性評価（Low-/High-Aesthetic category）を、CNN+LSTM（RNN）によりコメント（e.g. Fastastic colors）を出力する。
    .item2
      .text
        p
          img(src=`${figpath}180301NeuralAesthetic.png`,alt="180301NeuralAesthetic")
    .item3
      .text
        h1 新規性・結果
        p (i) 人間のような画像に対する感性評価（image aesthetics）をコンピュータに実装した。(ii)自然言語の出力により人間の高次な認知能力を実現。(iii) 画像-言語の組み合わせによるデータセットAVA-Reviews datasetを新規に構築した。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1802.10240.pdf") PDF
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 08:39:04


  +slide
    .title Interpreting CNN Knowledge via an Explanatory Graph
    .info
      .authors Q. Zhang, et al.
      .conference AAAI 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 深層学習の解釈性に関する論文であり、畳み込み層の特徴マップの応答を外的に解析して対応する反応を可視化。
          |畳み込みの各フィルタが異なる部位（e.g. 馬の耳や目）に反応するので、グラフにより解析して元画像の対象位置にアクセス。

    .item2
      .text
        p
          img(src=`${figpath}180301interpretability.png`,alt="180301interpretability.png")
    .item3
      .text
        h1 新規性・結果
        p Ground-truthなしに各部位に関する解釈性を与えたことが新規性である。
          |図に示すように入力画像に対するパーツごとの解析をフィルタの反応やグラフの解析から可視化することができる。
          |さらに、異なる画像間においても一貫性のある反応を得ることができた。
    .item4
      .text
        h1 自由記述欄
        p 深層学習は教師なしによる解釈性を獲得しているが、まだ反応している部分の可視化や部分ごとの解析が進んでいるにすぎない。
          |さらなる発展のためには、言語的な解釈や人間にわかりやすい加工（イラストとか？）が必要になるのではないだろうか。
    .slide_index #{getSlideIndex()}

  +slide
    .title HemaApp: Noninvasive Blood Screening of Hemoglobin using Smartphone Cameras
    .info
      .authors E. J. Wang et al.,
      .conference Ubicomp 2016
    .slide_editor Kensho Hara

    .item1
      .text
        h1 概要
        p スマホカメラを使って非侵襲なヘモグロビン濃度の測定を実現．
          |血中の酸素飽和度の測定などはこれまでにもあったがヘモグロビン濃度まで測定できているものはなかった．
          |照明条件とRGBの変化からヘモグロビン濃度を推定するためのアルゴリズムを提案．
    .item2
      .text
        p
          img(src=`${figpath}180228_hemaapp.png`,alt="180228_hemaapp.png")
    .item3
      .text
        h1 新規性・結果
        p 特別な装置を使うことなく簡単にスマホカメラでヘモグロビン濃度測定を実現した点．
    .item4
      .text
        h1 自由記述欄
        p システムやユーザスタディの完成度や完全に実現できたときの有用性が評価された？
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.1 10:44:03

  +slide
    .title End-to-end Driving via Conditional Imitation Learning
    .info
      .authors Felipe Codevilla, Matthias Müller, Alexey Dosovitskiy, Antonio López, and Vladlen Koltun,
      .conference arXiv
      .paper_id 1710.02410
    .item1
      h1 概要
      .text.
        自動運転を模倣学習により行う手法を提案。実空間での学習結果をヴァーチャルな空間での自動運転にて再現することができた。RGB画像、計測（e.g. スピード）や命令（e.g. turn right）などからステアリング、アクセル、ブレーキなどのコマンドを出力して自動車を操作する。
    .item2
      img(src=figpath+"180205conditionalimitationlearning.png")
      img(src=figpath+"180205framework_imitation.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 模倣学習による自動運転を実現した。
          li 実空間とシミュレーションベースの転移を行うことにも成功。
    .item4
      h1 リンク集
      .text
        ul
          li 自動運転の学習はシミュレーションベースで完結してしまう可能性がある。
          li メタ学習/模倣学習/強化学習などはCVに徐々に取り入れられてくるはずなので、2018年は学習しておいて損はない？
          li: a(href="https://arxiv.org/pdf/1710.02410.pdf" target="blank") [論文] End-to-end Driving via Conditional Imitation Learning
          li: a(href="https://www.youtube.com/watch?v=cFtnflNe5fM" target="blank") YouTube
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

  +slide
    .title Open3D: A Modern Library for 3D Data Processing
    .info
      .authors Qian-Yi Zhou et al.,
      .conference arXiv
      .paper_id 1801.09847
    .item1
      h1 概要
      .text
        |3Dデータを取り扱い、迅速な開発を可能にする
        a(href="http://www.open3d.org/" target="blank") Open3D
        |を提供する。Open3DはC++/Pythonをサポート、並列化にも対応しており、クラウドで開発することが可能。
        |点群読み込み-ダウンサンプリング-法線の計算、シーン再構築、3次元可視化などの処理が含まれている。
    .item2
      img(src=figpath+"180205open3d.png")
    .item3
      h1 新規性・結果
      .text
        |3次元画像処理のコミュニティにて有益なオープンソースを提供し、その
        a(href="https://github.com/IntelVCL/Open3D" target="blank") コード
        |も提供されている。
    .item4
      h1 コメント・リンク集
      .text
        ul
          li: a(href="http://www.open3d.org/" target="blank") Project page
          li: a(href="https://arxiv.org/pdf/1801.09847.pdf" target="blank") [論文] Open3D: A Modern Library for 3D Data Processing
          li: a(href="https://github.com/IntelVCL/Open3D" target="blank") Code
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

  +slide
    .title Hierarchical Variational Autoencoders for Music
    .info
      .authors A. Roberts et al.,
      .conference NIPS WS on Machine Learning for Creativity and Design, 2017.
    .item1
      h1 概要
      .text
        | 音楽を生成するためのHierarchical Variational Autoencoders (VAE) を提案．
        | エンコーダとデコーダがLSTMで構成されているReccurent VAEがベース．
        a(href="https://goo.gl/twGuP2") 結果サンプル
        | 長い音楽（実験では32小節）を単純なLSTMデコーダで生成するのは難しいので，
        | この研究では複数のLSTMを階層的に重ねて，段階的に長くしていくHierarchical VAEを提案．
        | ループメロディの外挿や，メロディの生成，3ピース構成の音楽生成の実験で性能を検討．
        | 結果の音楽やコードは公開されている．
    .item2
      img(src=figpath+"hierarchical_vae.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 階層的なLSTMによるデコーダをVAEによる音楽生成に導入
          li
            a(href="https://goo.gl/twGuP2") 結果サンプル
    .item4
      h1 自由記述欄
      .text
        ul
          li これも長期的な構成を考えて生成することはできていない
          li Future Workにはインタフェースを作るとあるし，1曲まるごと作るというよりは適当にサンプルを出して作曲家のアイデアを促進することを目指しているのかな．
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title Generating the Future with Adversarial Transformers
    .info
      .authors C. Vondrick et al.,
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        未来の動画を予測して生成する手法を提案．
        4フレーム x 64画素 x 64画素のクリップを入力として，その後の16フレームの動画を生成．
        完全に新しいフレームを生成するのは難しいので，入力フレームの変換により未来のフレームを生成するのがポイント．
        論文の主張としては，きれいな動画を作るにはLow-Levelな情報が重要だけど，未来予測のためにはHigh-Levelな理解も必要で，
        その両者を一つのネットワークで一気に学習するのは難しいとしている．
        だから，Low-Levelな情報は元のフレームを変換することで引っ張ってきて，ネットワークはHigh-Levelな特徴抽出に集中させるのが良いとのこと．
        このネットワークの学習はGANベース．
        生成動画の主観評価や可視化，Generatorの特徴を利用した物体認識タスクなどで性能を評価．

    .item2
      img(src=figpath+"generating_future.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 元のフレームからの変換により未来のフレームを生成する手法を提案
          li 未来の動画生成において，敵対的学習により大規模な教師なしデータを利用した学習を実現
          li 直接動画を生成したり，回帰誤差で学習したりする手法よりも良いことを主観評価実験で確認          
    .item4
      h1 自由記述欄
      .text
        ul
          li 入力が4フレームだけだけど，もっと増やすと性能は変わるのか気になる
          li 主観評価で本物と比較すると提案手法が一番嫌われている率が高いのもちょっと気になる
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title DensePose: Dense Human Pose Estimation In The Wild
    .info
      .authors Rıza Alp Guler et al.
    .item1
      h1 概要
      .text.
        身体の表面形状まで考慮したDenseな姿勢推定手法に関する研究。サーフェイスモデルを提供するSMPLタイプとアノテーションベースのMSCOCOタイプを提供。手法はMask RCNN（w/ ResNet-50, ROI-align, Regression）をベースに構築している。

    .item2
      img(src=figpath+"180205densepose.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li DenseReg [Guler,CVPR17]は顔表面の推定に対して、本研究では身体全体の表面やデンスなポイントを回帰。
          li SMPLやDense-COCOのデータセットを構築
          li 非拘束（in the wild）の環境にてDensePoseを学習。
    .item4
      h1 自由記述欄
      .text
        ul
          li リアルが完全に崩壊した。（Face2Faceの全身モデル版が実現可能になった？）
          li CG/UIの分野との親和性がより高くなった
          li: a(href="https://arxiv.org/abs/1802.00434" target="blank") DensePose: Dense Human Pose Estimation In The Wild
          li: a(href="https://arxiv.org/abs/1612.01202" target="blank") DenseReg
          li: a(href="http://smpl.is.tue.mpg.de" target="blank") SMPL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka
    
  +slide
    .title What will Happen Next? Forecasting Player Moves in Sports Videos
    .info
      .authors Panna Felsen et al.
      .conference ICCV, 2017.
    .item1
      h1 概要
      .text.
        チームスポーツにおいて次に起こることを予測する研究。2チームに分かれたゴール型スポーツを対象とし、ボールを持つ選手の遷移やファールの有無などの推定を行った．

    .item2
      img(src=figpath+"180303whatwillhappen.PNG")
    .item3
      h1 新規性・結果
      .text
        ul
          li 水球とバスケットボールのデータセットを構築した
          li 画像から選手やボールの位置を上から見た画像に変換する手法を提案した
          li 他のスポーツで学習したものを適用した場合(例：学習→水球　テスト→バスケ)ランダムフォレストの方がニューラルネットより精度が高いことが分かった
    .item4
      h1 自由記述欄
      .text
        ul
          li この論文のようにニューラルネットがうまくいかない例を調べるのは面白そう
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
    