extends layouts/_layout

block slides
  +slide
    .title Low-resource Multi-task Audio Sensing for Mobile and Embedded Devices via Shared Deep Neural Network Representations
    .info
      .authors P. Georgiev et al.,
      .conference Ubicomp 2017
    .item1
      h1 概要
      .text.
        複数タスクを共通の層を使って特徴抽出して解くMulti-task Learningにより，
        少ない消費電力でのオーディオセンシングを実現する手法を提案．
        Amazon Echoなどのスマートスピーカで音声認識に加えてその他タスクを同時に実行する必要がある．
        そのような状況で効率的に動くためのDeep Learningの手法を提案している．
        評価実験では話者認識，感情認識，ストレス認識，環境音認識の4タスクを扱い，
        タスクごとに独立でやる場合に比べてマルチタスクで処理したときの有用性を評価．
        マルチタスクにすることで精度を大きく落とすことなく少ない消費電力での認識が可能なことを示した．

    .item2
      img(src=figpath+"180317_lowenergy.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li オーディオ認識系の複数タスクを同時に学習するためのフレームワークを提案
          li マルチタスクで解くことでの少消費電力での認識を実証
    .item4
      h1 自由記述欄
      .text
        ul
          li Session 5: Machine Learning
          li Multi-taskでやったときの電力消費とかに注目して分析してるのがUbicompっぽい
          li 技術的なContributionは主張してるけどそこまで大きくないように見える
        
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title Devices and Data and Agents, Oh My: How Smart Home Abstractions Prime End-User Mental Models
    .info
      .authors M. Clark et al.,
      .conference Ubicomp 2017
    .item1
      h1 概要
      .text.
        Smart Homeについてユーザがどのようなものを求めているかのデータを集めて提供．
        Capabilities (Data or Device) と Personification (Unmediated or Agent-mediagetd) の2軸で
        4通りのAbstractionを定義．
        扱えるデバイスを提示するか提供されるデータを提示するかの軸と，
        SiriのようなAIが介するかどうかの軸．
        4つのうちの1つをユーザに提示して，どのようなアプリケーションが欲しいかを記述してもらう．
        そのデータからユーザが何を求めているかを分析．

    .item2
      img(src=figpath+"180317_smarthome.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li Smart Home (IoT) における4つのAbstractionを提案
          li 提示するAbstractionによってユーザの求めるものは大きく変化することを実証
    .item4
      h1 自由記述欄
      .text
        ul
          li Session 3: Software Enginnering
        
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title FootStriker: An EMS-based Foot Strike Assistant for Running
    .info
      .authors F. Daiber et al.,
      .conference Ubicomp 2017
    .item1
      h1 概要
      .text.
        ランニングの支援をするウェアラブルシステムを提案．
        走るときにかかとから着地と負担が大きく怪我につながりやすいので，
        足先や中心の方から着地する方が良いとされている．
        そういう走り方を身につけるためのシステムを提案．
        インソールに仕込んだ圧力センサでかかとからの着地を検出して，
        足に付けたEMSで電気信号によりフィードバックする．
        スローモーションビデオを見せて言葉で教えるよりも，
        システムを使ったほうがかかとからの着地が減少し，
        良い走り方を身につけるのに有効なことを示した．        

    .item2
      img(src=figpath+"180317_footstriker.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li ランニング時の着地方法の改善を目的としたウェアラブルシステムを提案
          li リアルタイムフィードバックで従来のコーチング手法よりも良い結果を実現
    .item4
      h1 自由記述欄
      .text
        ul
          li Session 2: Sports 1
        
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title BlindType: Eyes-Free Text Entry on Handheld Touchpad by Leveraging Thumb’s Muscle Memory
    .info
      .authors C. Seim et al.,
      .conference Ubicomp 2017
    .item1
      h1 概要
      .text.
        入力画面を見ないでキー入力するときの入力方法に関する研究．
        大画面を見たり，HMDをつけたりしながら手元のスマホなどで入力することを想定．
        入力はQWERTYのバーチャルキーボード．
        1キーずつ独立に入力判定をする方法 (Absolute) と，
        前のタッチ位置との相対位置から入力判定する方法 (Relative) の2種類を検討．
        加えて，ユーザごとにキーボードサイズや位置を学習するPersonalと
        そうしないGeneralの2パターンを合わせて，組み合わせの4通りを比較．
        Personalizeはされている方が良いし，Relativeの方が良いという結論．
        実際にキーボードを見ながら入力するのに近い結果を得た．

    .item2
      img(src=figpath+"180317_eyefree.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li ユーザに合わせたキーボードモデルの学習手法を提案
          li Eyes-freeでのタイピングデータをユーザスタディで獲得
          li ユーザスタディの結果から，入力アルゴリズムを提案
    .item4
      h1 自由記述欄
      .text
        ul
          li Session 1: Input Methods
        
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
    .info
      .authors Mehdi Noroozi et al.
      .conference in ECCV 2016
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 
        |一枚画像から作成されるジグソーパズルを解くことで表現学習を行う。SiameseNetを用い、9×9のpermutationの識別を解く(実際にはハミング距離などが考慮された1000通りのpermutationに限定)。タイルの境界部分などの低レベルな特徴によってとかないよう、境界を消すような前処理を行う。
        //|時間の設定をもとに対応する特徴量を計算することができ、実験では仮想的なデータにおけるフレーム生成タスクにおいて、鮮明な出力が得られている
          
    .item3
      .text
        h1 手法・結果
        p 
        |既存手法として中心タイルからの相対位置８クラス分類などの表現学習方法が存在したが、難度が高すぎた。本手法ではpermutationの限定、すべてのタイルを入力として使用することで難度を調整している。様々なタスクの表現学習においてSoTA。
        
    .item2
      .text
        p
          img(src=`${figpath}jg_puzzle.png`)
    .item4
      .text
        h1 コメント・リンク
        p
        |表現学習にはタスク自体の難易度の調整は重要そう。本研究でも、permutationの数、ハミング距離などが大きく影響していた。
        
        ul
          li
            a(href="https://arxiv.org/abs/1603.09246") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Dynamic Vision Sensors for Human Activity Recognition
    .info
      .authors Stefanie Anna Baby, Bimal Vinod, Chaitanya Chinni, Kaushik Mitra
      .conference ACPR 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p フレームレートを修正できるカメラDynamic Vision Sensors (DVS）により行動認識を行う。DVSはピクセル値の変化のみによりON/OFFを切り替え画素値を記録する仕組みで、フレーム間差分を撮像するような画像を入力できる。HD画像を記録した場合でもストレージの消費が抑えられる。本論文ではモーションを記録するために画像スライス（x-y, x-t, y-tの空間）を記録する。
    .item2
      .text
        p
          img(src=`${figpath}180315DVS.png`,alt="180315DVS")
    .item3
      .text
        h1 新規性・結果
        p Motion boudary histogram (MBH)との統合により良好な精度を実現した。UCF11にて0.6727, MBHとの統合により0.7513（RGB画像では0.7933）。DVS gesture datasetではMBHとの統合により0.9880。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/abs/1803.04667") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.15 09:17:35

  +slide
    .title Future Frame Prediction for Anomaly Detection -- A New Baseline
    .info
      .authors Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 先の（未来の）フレーム予測と異常検知を同時に行う手法を提案する論文。予測したフレームと異常検知の正解値により誤差を計算して最適化を行う。図に本論文で提案するネットワークアーキテクチャの図を示す。U-Netにより画像予測やさらにオプティカルフロー推定を行い、RGB空間、オプティカルフロー空間にて誤差を計算しGANの枠組みでそれらがリアルかフェイクかを判定する。同フレームを用いて異常検知を実施する。
    .item2
      .text
        p
          img(src=`${figpath}180315PredictionAnomaly.png`,alt="180315PredictionAnomaly")
    .item3
      .text
        h1 新規性・結果
        p 従来は現在フレームを入力として異常検知を行う手法は存在したが、未来フレームを予測して異常検知を行う枠組みは本論文による初めての試みである。異常値の正解値を与えることで画像予測にもフィードバックされるため、画像予測と異常検知の相互学習に良い影響を与える。オープンデータベースにてベンチマークした結果、何れもState-of-the-artな精度を達成。
    .item4
      .text
        h1 コメント・リンク集
        p 生成ベースで画像予測+X（Xは任意タスク）というものはSoTAが出せるくらいにはなってきた。
        ul
          li
            a(href="https://arxiv.org/pdf/1712.09867.pdf") 論文
          li
            a(href="https://github.com/StevenLiuWen/ano_pred_cvpr2018") Project
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.15 09:04:03

  +slide
    .title 3D Human Pose Estimation in RGBD Images for Robotic Task Learning
    .info
      .authors Christian Zimmermann, Tim Welschehold, Christian Dornhege, Wolfram Burgard, Thomas Brox
      .conference ICRA 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p RGB-Dセンサによる入力から、人物の3次元キーポイントを検出して、可能であれば手領域の法線ベクトルを抽出する。手領域はロボット操作に活用してデモンストレーションを実行する。図に示すアーキテクチャでは、主に姿勢推定（2D Keypoint Detector）、3次元への投影（VoxelPoseNet）、手領域の法線ベクトル推定（HandNormalNet）から構成される。姿勢推定はOpenPoseを活用、VoxelPoseNetは3次元のL2ノルム誤差により計算する。
    .item2
      .text
        p
          img(src=`${figpath}1803153DHumanPose.png`,alt="1803153DHumanPose")
    .item3
      .text
        h1 新規性・結果
        p 実環境におけるデモンストレーションではロボットPR2を用いて人物の把持行動を教師としてマニピューレーションタスクを模倣した。実験はMulti View Kinect DatasetやCaptury Datasetにておこなった。
    .item4
      .text
        h1 コメント・リンク集
        p コンピュータビジョンを用いてロボット操作を実現できる敷居が下がって来た？逆に、ICRA2018なんかではコンピュータビジョン研究者が一気に増えて分野間のシームレス化が進んでいるのでは？
        ul
          li
            a(href="https://arxiv.org/pdf/1803.02622.pdf") 論文
          li
            a(href="https://lmb.informatik.uni-freiburg.de/projects/rgbd-pose3d/") Project
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.15 08:43:30

  +slide
    .title Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN
    .info
      .authors Shuai Li, et al.
      .conference CVPR 2018
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 新しいRNN手法であるindependently recurrent neural network (IndRNN)の提案。一枚のレイヤ内のニューロンが独立しており、レイヤ間で接続されている。これにより、勾配消失問題や爆発問題を防ぎ、より長期的なデータを学習することができる。また、IndRNNは複数積み重ねることができるため、既存のRNNよりも深いネットワークを構築できる。
    .item2
      .text
        p
          img(src=`${figpath}180314IndRNN.jpg`)
    .item3
      .text
        h1 新規性
        p 本手法によって下記の従来手法の問題を解決。
        p RNNは、勾配の消失や爆発の問題、長期パターンの学習が困難である。LSTMやGRUは、上記のRNNの問題を解決すべく開発されたが、層の勾配が減衰してしまう問題がある。また、RNNは全てのニューロンが接続されているため、挙動の解釈が困難。
    .item4
      .text
        h1 結果・リンク集
        p かなり長いシーケンス(5000回以上の時間ステップ)を処理でき、かなり深いネットワーク（実験では21レイヤー）を構築できる。
        ul
          li
            a(href="https://arxiv.org/pdf/1803.04831.pdf") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Improving Object Localization with Fitness NMS and Bounded IoU Loss
    .info
      .authors Lachlan Tychsen-Smith, et al.
      .conference CVPR 2018
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 既存のNon-Max Supressionを改良したFitness NMSの提案。Soft NMSも同時に使用するとより効果的。
        p 勾配降下法の収束特性(滑らかさ、堅牢性など)を維持しつつ、IoUを最大化するという目標により適した損失関数であるBounded IoU Loss の提案。これをRoIクラスタリングと組み合わせることで精度が向上する。
    .item2
      .text
        p
          img(src=`${figpath}180314FitnessNMS.jpg`)
    .item3
      .text
        h1 新規性
        p バウンディングボックスのスコアを算出する関数を拡張する。具体的には、グランドトゥルースとのIoUと、クラスの期待値を追加する。これにより、IoUの重なり推定値と、クラス確率の両方が高いバウンディングボックスを優先して学習することができる。
    .item4
      .text
        h1 結果・リンク集
        p MSCOCO、Titan X(Maxwell)使用時では、精度33.6％-79Hzまたは41.8％-5Hz。本論文ではDeNetでテストしたが、別の手法でも精度向上が望めるよう。
        ul
          li
            a(href="https://arxiv.org/pdf/1711.00164.pdf") 論文
          li
            a(href="https://github.com/lachlants/denet") ソースコード
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Object Recognition with and without Objects
    .info
      .authors Zhuotun Zhu, Lingxi Xie, Alan L. Yuille
      .conference IJCAI 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 物体認識を行う際にコンテキスト情報（ここでは背景領域の特徴）が少なからずヒントとして効いているのでは？という疑問を解決するための検証。通常の物体検出データセット（OrigSet）をbboxのアノテーションを参考にして前景領域（FGSet）と背景領域（BGSet）に分けて精度を確認した。コンテキスト情報は特徴として非常に強く、物体が隠されている場面でも物体認識ができることを示唆した。
    .item2
      .text
        p
          img(src=`${figpath}180314WithoutOBJ.png`,alt="180314WithoutOBJ")
    .item3
      .text
        h1 新規性・結果
        p AlexNetを用いた場合、OrigSetよりもBGSet（背景領域のみ）の方が精度が高いこともあることが判明した。さらに、BGSetはFGSet（前景領域のみ）よりも大体において良好な性能を実現している。このことからも背景領域におけるコンテキストは無視できないものとなった。
    .item4
      .text
        h1 コメント・リンク集
        p 行動認識における調査"Human Action Recognition without Human"も合わせて読んでおきたい。
        ul
          li
            a(href="https://www.ijcai.org/proceedings/2017/0505.pdf") 論文
          li
            a(href="https://arxiv.org/abs/1608.07876") Human Action Recognition without Human (ECCV 2016 WS)
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.14 12:12:33

  +slide
    .title Rethinking Feature Distribution for Loss Functions in Image Classification
    .info
      .authors Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen
      .conference CVPR 2018 (spotlight)
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 本論文ではLarge-margin Gaussian Mixture (L-GM) Lossを提案して画像識別タスクに応用する。Softmax Lossとの違いは、学習セットにおけるディープ特徴の混合ガウス分布をフォローしつつ仮説を設定するところである。識別境界や尤度正則化においてL-GM Lossは非常に高いパフォーマンスを実現している。
    .item2
      .text
        p
          img(src=`${figpath}180314LGM.png`,alt="180314LGM")
    .item3
      .text
        h1 新規性・結果
        p L-GM Lossは画像識別においてSoftmax Lossよりも精度が高いことはもちろん、特徴分布を考慮するため例えばAdversarial Examples（摂動ノイズ）などにおいても対応できる。MNIST, CIFAR, ImageNet, LFWにおける識別や摂動ノイズを加えた実験においても良好な性能を確かめた。
    .item4
      .text
        h1 コメント・リンク集
        p Softmax Lossよりも有意に精度向上が見られている。導入が簡単なら取り入れて精度向上したい。
        ul
          li
            a(href="https://arxiv.org/pdf/1803.02988.pdf") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.14 11:04:45

  +slide
    .title Domain Adaptive Faster R-CNN for Object Detection in the Wild
    .info
      .authors Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ドメイン変換について、ゲームなどのCG映像から実際の交通シーンに対応して物体検出を行うための学習方法を提案する。本論文では(i) 画像レベルのドメイン変換、(ii) インスタンス（ある物体）に対してのドメイン変換、の二種類の方法を提案し、整合性をとるように正規化する（図のConsistency Regularization; Global/Localな特徴変換を考慮）。ここで、物体検出はFaster R-CNNをベースとしてドメイン変換の手法も二種類（H-divergence、敵対的学習）用意する。
    .item2
      .text
        p
          img(src=`${figpath}180314DomainFRCNN.png`,alt="180314DomainFRCNN")
    .item3
      .text
        h1 新規性・結果
        p CGで学習し実環境における自動運転などで使えるドメイン変換の手法を提案した。実験はCityscapes, KITTI, SIM10Kなどで行い、ロバストな物体検出を実行することができた。例えばCityscapesとKITTIの相互ドメイン変換でベースラインのFaster R-CNNが30.2 (K->C)、53.5 (C->K)のところ、Domain Adaptive Faster R-CNNでは38.5 (K->C)、64.1 (C->K)であった。
    .item4
      .text
        h1 コメント・リンク集
        p データ収集は手動から自動の時代になって来た？データを手作業で集める時代からアルゴリズムを駆使して収集する時代へ移行。
        ul
          li
            a(href="https://arxiv.org/pdf/1803.03243.pdf") 論文
          li
            a(href="http://www.vision.ee.ethz.ch/~liwenw/") 著者
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.14 08:43:53

  +slide
    .title Guided Labeling using Convolutional Neural Networks
    .info
      .authors Sebastian Stabinger, et al. 
      .conference CVPR 2018
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p ラベルの付いていないデータに対して、どの画像にラベルを付けてデータセットを構成すればよいかを判断するguided labelingの提案。ラベル付けを行う必要があるサンプルを見定めることで、データセットの量を大幅に減らすことができる。
    .item2
      .text
        p
          img(src=`${figpath}180313GuidedLabeling.jpg`)
    .item3
      .text
        h1 新規性
        p 大規模データセットにおいて、手動でのラベル付けは大変。選別してラベル付けを行えば、作業を最小限に抑えられる。また、ある意味良いデータを選別できるため、場合によっては精度も向上。
    .item4
      .text
        p MNISTは、データセットのサイズを1/16に、CIFAR10は1/2に減らすことが可能に。また、MNISTの場合は、全部使った時よりも識別精度が向上した。普遍性を妨げる不必要なデータを取り除けたことが精度向上につながった？
        ul
          li
            a(href="https://arxiv.org/pdf/1712.02154.pdf") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Pose-Robust Face Recognition via Deep Residual Equivariant Mapping
    .info
      .authors Kaidi Cao, Yu Rong, Cheng Li, Xiaoou Tang, Chen Change Loy
      .conference CVPR 2018
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 横顔の認識精度を高めるためにDeep Residual EquivAriant Mapping (DREAM)の提案。正面と側面の顔間のマッピングを行うことで特徴空間を対応付ける。これにより、横顔を正面の姿勢に変換して認識を単純化。
    .item2
      .text
        p
          img(src=`${figpath}180313DREAM_1.jpg`)
    .item3
      .text
        h1 新規性・手法・リンク集
        p 正面と側面のトレーニング数の不均衡から、現代の顔認識モデルの多くは、正面と比べて横顔を処理するのが比較的貧弱。本手法は姿勢変動を伴う顔認識に限定されない顔認識が可能で、横顔のデータを増やさなくても精度向上。
        p 上図より、DREAMをCNNに追加し、入力に残差を動的に追加。下図はマッピングによる姿勢変換の例。
        ul
          li
            a(href="https://arxiv.org/pdf/1803.00839.pdf") 論文
          li
            a(href="http://mmlab.ie.cuhk.edu.hk/projects/DREAM") ソースコード
    .item4
      .text
        p
          img(src=`${figpath}180313DREAM_2.jpg`)
    .slide_index #{getSlideIndex()}

  +slide
    .title SPICE: Semantic Propositional Image Caption Evaluation
    .info
      .authors Peter Anderson, et al.
      .conference ECCV 2016
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 画像の意味や内容が、キャプション評価の重要な要素であると仮定し、シーングラフへのマッピングを用いて評価するSemantic Propositional Image Caption Evaluation(SPICE)の提案。これにより、出力したキャプションがオブジェクト、属性およびそれらの関係をいかに表現できていいるかを測ることができる。
    .item2
      .text
        p
          img(src=`${figpath}180312SPICE.jpg`)
    .item3
      .text
        h1 新規性・手法
        p 既存の評価では、主に人間によるキャプションに近いかどうかをシミュレートするタスクとしては微妙なところ。
        p 図中の、上の依存性解析ツリーからオブジェクト(赤色)、属性(緑色)、および関係(青色)を取得し、右のシーングラフにマッピングする。候補シーングラフと参照シーングラフのタプルで計算されたFスコアを用いてキャプションのクオリティを算出。
    .item4
      .text
        h1 結果・リンク集
        p Bleu、METEOR、ROUGE-L、CIDErなどの既存のn-gramメトリクスよりも、人間が判断したかのように評価することができる。しかし、課題はまだまだある。
        ul
          li
            a(href="https://arxiv.org/pdf/1607.08822.pdf") 論文
          li
            a(href="http://www.panderson.me/spice") サイト・ソースコードなど
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Semantic Compositional Networks for Visual Captioning
    .info
      .authors Zhe Gan, et al. 
      .conference CVPR 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 意味的概念(タグ)を画像から取得し、各タグの確率をLSTMのパラメータとして使用することでイメージキャプショニングを行うSemantic Compositional Network (SCN)の提案。SCNは、LSTMの重みをタグの情報を含んだ重みに拡張する。タグには確率が設けてあり、大きければ大きいほどタグをLSTMに反映させる。
    .item2
      .text
        p
          img(src=`${figpath}180312SCN.jpg`)
    .item3
      .text
        h1 新規性・手法
        p 従来のRNNに、意味的特徴を追加したイメージ。LSTMに“単語”と“状態”を入力する際に、意味的特徴を追加していく。通常の入力と意味的情報を追加したものを重み行列のアンサンブルと呼んでいる。
    .item4
      .text
        h1 結果・リンク集
        p COCO、Flickr30k、Youtube2Textの3つのデータセットで、最先端の手法よりも優れている。
        ul
          li
            a(href="https://arxiv.org/pdf/1611.08002.pdf") 論文
          li
            a(href="https://github.com/zhegan27/Semantic_Compositional_Nets") ソースコード
    .slide_index #{getSlideIndex()}
    
  +slide
    .title TSSD: Temporal Single-Shot Detector Based on Attention and LSTM for Robotic Intelligent Perception
    .info
      .authors Xingyu Chen, Zhengxing Wu, Junzhi Yu
      .conference Submitted to IROS 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p アテンションとConvLSTM構造を参考にして時系列のROI検出器であるTemporal Single-Shot Detector (TSSD)を構築して、ロボットビジョンに応用する。ConvLSTMでは階層的な時系列特徴を取り扱い、High-levelからLow-levelな特徴を処理できるようにした。
    .item2
      .text
        p
          img(src=`${figpath}180311TSSD.png`,alt="180311TSSD")
    .item3
      .text
        h1 新規性・結果
        ul
          li 初めてOne-shotで時系列情報（ビデオ）から検出を行なった。
          li 階層的に時系列情報を取り扱い、High-levelとLow-levelの情報をSSDの構造内で統合することに成功、時系列検出を高精度に実行することができた。
          li 冗長な処理部分を削減してアテンション構造を取り入れた。
          li ImageNet-VID（ビデオに対する検出）にて64.5% @mAPを達成、なおかつ27fpsで処理することができた。TPNやD&Tと比較すると精度は劣るが、リアルタイム性でいうと有効性があると考える。
    .item4
      .text
        h1 コメント・リンク集
        p ビデオに対する検出処理は静止画のそれとは若干異なる？SSD vs. TSSDを比較しても最大5%くらい差があるし、最先端手法であるTPNやD&Tと比較するとTSSDからさらに10%以上も差がついている。ただ単にConvLSTMなど時系列手法を導入しても性能は思うように上がらないということか。
        ul
          li
            a(href="https://arxiv.org/pdf/1803.00197.pdf") 論文
          li
            a(href="https://arxiv.org/abs/1702.06355") TPN
          li
            a(href="https://arxiv.org/abs/1710.03958") D&T
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.11 23:53:27

  +slide
    .title Boosting Image Captioning with Attributes
    .info
      .authors Ting Yao, et al.
      .conference ICCV 2016
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p CNN-RNNのキャプション生成モデルに、属性推定を追加したLong Short-Term Memory with Attributes(LSTM-A)を提案。属性間の相関を、 Multiple Instance Learning(MIL)で統合することにより、属性間相関を探索し、文章生成。
    .item2
      .text
        p
          img(src=`${figpath}180310LSTMA.jpg`)
    .item3
      .text
        h1 新規性・手法
        p LSTM-Aの5つの変種について研究。
        p 3つのモデルは属性をどこに追加するか：LSTM-A1は属性のみを利用、LSTM-A2は最初に画像表現を挿入する、LSTM-A3は最初に属性を供給。
        p 2つのモデルはLSTMに属性や画像表現をどのタイミングで入力するか： LSTMA4は各時間ステップでの画像表現の入力、LSTM-A5は各時間ステップでの属性の入力。
    .item4
      .text
        h1 結果・リンク集
        p 属性推定を追加することでキャプションのパフォーマンスの向上。
        ul
          li
            a(href="https://arxiv.org/pdf/1611.01646.pdf") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training
    .info
      .authors Rakshith Shetty, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 人間によるキャプションに匹敵するキャプション生成モデルの提案。人間のキャプションに近づけるために、ガンベル分布を組み込み、GANを採用。ガンベル分布を使用してソフトマックス分布からソフトサンプルを取得し、サンプルをバックプロパゲーションする。
    .item2
      .text
        p
          img(src=`${figpath}180310SpeakingTheSameLanguage.jpg`)
    .item3
      .text
        h1 新規性
        p 生成された単語の分布、語彙のサイズの欠如、頻繁なキャプションに対するジェネレータの偏りなど、従来のイメージキャプショニング手法と人間によるキャプションの差がある。そこで、イメージキャプショニングの学習目的を、正解キャプション生成から、人間のキャプションと区別できないキャプションの出力に変更。
    .item4
      .text
        h1 結果・リンク集
        p キャプションの正確さは最先端技術に匹敵する性能。ただし、本手法はキャプションに偏りが少なく、人間よりの出力。
        ul
          li
            a(href="https://arxiv.org/pdf/1703.10476.pdf") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Scene Graph Generation from Objects, Phrases and Region Captions
    .info
      .authors Yikang Li, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 物体検出、フレーズ、キャプショニングの3つのタスクを共同で学習し、シーン理解を行うMulti-level Scene Description Network(MSDN)の提案。入力画像から異なる意味を持つ領域間をリンクさせるために、グラフを動的に構築。これにより、様々なタスクを整理しながら学習する。
    .item2
      .text
        p
          img(src=`${figpath}180310MSDN_1.jpg`)
    .item3
      .text
        h1 手法・リンク集
        p VGG16で特徴量を抽出。領域はRoIプーリングで推定。オブジェクト(赤)、フレーズ(緑色)、領域(黄色)から動的グラフを構築。Feature Refiningでは、1つの特徴を、他の2種類の特徴を使ってアップデートさせていくイメージ。
        ul
          li
            a(href="https://arxiv.org/pdf/1707.09700.pdf") 論文
          li
            a(href="https://github.com/yikang-li/MSDN") ソースコード
    .item4
      .text
        p
          img(src=`${figpath}180310MSDN_2.jpg`)
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Areas of Attention for Image Captioning
    .info
      .authors Marco Pedersoli, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 画像キャプショニングのための、アテンションベースモデルであるAreas of Attentionの提案。画像領域、キャプションワード、RNNの状態の間の依存関係をモデル化。従来の画像領域のみをRNN状態に関連付けるモデルとは異なり、キャプションワードと画像領域との間の直接的な関連付けを行う。
    .item2
      .text
        p
          img(src=`${figpath}180310AreasOfAttention.jpg`)
    .item3
      .text
        h1 新規性・手法
        p CNN-RNNをベースラインとする。RNNの状態が与えられたときに、次の単語と対応する領域を各タイムステップで共同して予測するアテンションメカニズム。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1612.01033.pdf") 論文
          li
            a(href="https://github.com/marcopede/AreasOfAttention") ソースコード
    .slide_index #{getSlideIndex()}
    
  +slide
    .title An Empirical Study of Language CNN for Image Captioning
    .info
      .authors Jiuxiang Gu, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p CNNでイメージキャプショニングを行う言語モデルの提案。1つの単語と状態(state)に基づいて時系列的に次の単語を予測するRNNとは異なり、Language CNNは以前に推定された全ての単語を入力とすることで、画像キャプションとして重要な単語の長期依存性をモデル化できる。
    .item2
      .text
        p
          img(src=`${figpath}180310LCNN.jpg`)
    .item3
      .text
        h1 新規性・手法
        p 以前の単語の忘却を防ぐために、RNNにCNNLを追加して文章を生成する。全ての時間枠で重みが共有されるのがミソ。
        p 画像特徴抽出のためのCNNI、言語モデリングのためCNNL、CNNIとCNNLを接続するマルチモーダル層（M）、単語予測のための再帰ネットワーク（RNNやLSTMなど）の4部構成。
    .item4
      .text
        h1 コメント・リンク集
        p バニラのRNNよりも優れた性能。同じことを何度も言ったり、変な文章になりにくいのでは？
        ul
          li
            a(href="https://arxiv.org/abs/1612.07086") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Pedestrian Detection at Day/Night Time with Visible and FIR Cameras: A Comparison
    .info
      .authors Alejandro Gonzalez et al.
      .conference Sensors Journal 2016
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p Far Infrared（FIR）カメラを用いて昼夜問わず歩行者を検出するための取り組み。同タスクを解決すべく、FIRカメラによる歩行者検出データベースを構築、複数の歩行者検出モデルーHolistic, Part-basedアプローチ, Patch-basedアプローチーを比較してベンチマークした。また、RGB画像、FIR画像、その両方を用いて歩行者検出を試行した。
    .item2
      .text
        p
          img(src=`${figpath}180309FIRDB.png`,alt="180309FIRDB")
    .item3
      .text
        h1 新規性・結果
        p 識別器としてSVM/DPM/RF、特徴量はHOG/LBP/HOG+LBP、Day/Night、Visible/FIRの組み合わせを調査した。Day/Night問わずエラー率はFIRが良く、特徴を組み合わせるHOG+LBPの方が良い結果を示した。Dayの場合にはHOG+LBP+RF（ランダムフォレスト）が、Nightの場合にはHOG+LBP+SVMがもっとも精度がよかった。
    .item4
      .text
        h1 コメント・リンク集
        p RGBのデータセットよりもFIRの方が精度が良く昼夜問わず検出ができるなら、今後はFIRカメラで収集し始めた方がよい？（最近査読でFIRで集めろと言われていたのはこういう背景があったからだった。。）
        ul
          li
            a(href="http://www.mdpi.com/1424-8220/16/6/820") 論文
          li
            a(href="http://adas.cvc.uab.es/") 研究室
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 22:34:11

  +slide
    .title Dense-Captioning Events in Videos
    .info
      .authors Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos Niebles Stanford University
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p ビデオ中の事象の検出と記述、両方を含む高密度キャプションイベントのタスク。つまり、ビデオ内で検出された複数のイベントを自然言語で同時に記述しながら、全てのイベントを識別できる新しいモデルを提案。ビデオ内のイベント間の依存関係を取得するために、過去と未来のイベントのコンテキスト情報を使用し、すべてのイベントを共同して記述するキャプションモジュールを導入。高密度キャプションイベントの大規模データセットのActivityNet Captionsも提案。
    .item2
      .text
        p
          img(src=`${figpath}180309DenseCaptioning.jpg`)
    .item3
      .text
        h1 新規性
        p ビデオには多数のイベントが含まれている。例えば、「ピアノを弾く男」のビデオでは、「別の男の踊り」や「群衆の拍手」が含まれてたりなど。これらすべてのイベントについて、ビデオを1回パスするだけで記述する。
        p ActivityNetには、100kのキャプション、849時間の動画20k本が含まれる。
    .item4
      .text
        h1 結果・リンク集
        p 複数イベントのキャプションを行えるので、複数人同時に行動認識ができる。人間が見落としがちな細かいシーンも見逃さないのでは？その点では人間を超えてる？
        ul
          li
            a(href="https://arxiv.org/pdf/1705.00754.pdf") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Common Subspace for Model and Similarity:Phrase Learning for Caption Generation from Images
    .info
      .authors Yoshitaka Ushiku, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p Common Subspace for Model and Similarity (CoSMoS)によるフレーズの学習方法の提案。
        p 1.同一フレーズに関連付けられたすべての特徴ベクトルを近接するようにマッピング
        p 2.各フレーズの分類子を学習
        p 3.フレーズ間でトレーニングサンプルが共有される部分空間を得る
    .item2
      .text
        p
          img(src=`${figpath}180309CoSMoS.jpg`)
    .item3
      .text
        h1 新規性・手法
        p “単語”ではなく、オブジェクト、属性、イベント、およびそれらの関係を表現する“フレーズ”に着目している。フレーズ数はさまざまな単語の組み合わせであるため、シングルワードの数よりはるかに多くなる。よって、フレーズはトレーニングサンプルが少なく、正確な推定が困難。そこで、モデルと類似性の共通部分空間を学習する。
    .item4
      .text
        h1 結果・リンク集
        p ウェブのデータセットを増やすことで精度が向上する見込み
        p 今後は、CNNとRNNによるキャプション生成にもCoSMoSを組み込む予定
        ul
          li
            a(href="http://openaccess.thecvf.com/content_iccv_2015/papers/Ushiku_Common_Subspace_for_ICCV_2015_paper.pdf") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title ASePPI: Robust Privacy Protection Against De-Anonymization Attacks
    .info
      .authors Natacha Ruchaud, Jean-Luc Dugelay
      .conference CVPR 2017 Workshop on CV-COPS
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 監視カメラ映像などにおいて人物領域のde-anonymization（匿名にしていた情報をオープンにされること？）を防ぐための研究。RoIに対して実行することで人物再同定（Person Re-identification）の精度を落とすことに成功している。
    .item2
      .text
        p
          img(src=`${figpath}180309AsePPI.png`,alt="180309AsePPI")
    .item3
      .text
        h1 新規性・結果
        p 本提案手法であるAdaptive Scrambling enabling Privacy Protection and Intelligibility (ASePPI)により、匿名性が保たれることが明らかになった。これは、従来法よりも優れている。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Dugelay_ASePPI_Robust_Privacy_CVPR_2017_paper.pdf") 論文
          li
            a(href="http://www.eurecom.fr/en/publication/5283/detail/aseppi-robust-privacy-protection-against-de-anonymization-attacks") Project
          li
            a(href="https://github.com/NatachaRuchaud/ASePPI") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 18:05:28

  +slide
    .title An Autonomous Dynamic Camera Method for Effective Remote Teleoperation
    .info
      .authors Daniel Rakita, Bilge Mutlu and Michael Gleicher
      .conference HRI’18
      .paper_id pp. 325-333
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p 遠隔操作ロボットのための，ロボットカメラの自動姿勢決定．
          |作業野を見やすくするカメラ姿勢を自動で決定する．
        ul
          li どのような遠隔操作インタフェースでもOK
          li 操作者の操作を予測することで実現
            ul
              li 遮蔽のない視野
              li 作業部とカメラ間の距離を適切に保つ
          li 方向をわからなくさせないように自動調整
    .item2
      .text
        p
          img(src=`${figpath}An_Autonomous_Dynamic_Camera_Method_for_Effective_Remote_Teleoperation_Figure1.png`,alt="Figure1")
    .item3
      .text
        h1 評価点
        p 外科手術などに実用性ありそうでいい．
        p 適切なロボット動作のダイナミクスを定義して実装し，
          |ユーザ評価もちゃんと行っている．
        p BestPaper Nominee．ところで被験者に1時間拘束で10ドル払ってるらしい．
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171279") ACM Library
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 16:21:51

  +slide
    .title Blur vs. Block: Investigating the Effectiveness of Privacy-Enhancing Obfuscation for Images
    .info
      .authors Yifang Li, Nishant Vishwamitra, Bart P. Knijnenburg, Hongxin Hu, Kelly Caine
      .conference CVPR 2017 Workshop on CV-COPS
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 他人に情報が拡散しないよう、「ブラー」と「ブロック」というふたつの（人間に対する）難読化法を検証。53名のユーザについて画像の満足度、情報量などの側面から調査した。
    .item2
      .text
        p
          img(src=`${figpath}180309BlurBlock.png`,alt="180309BlurBlock")
    .item3
      .text
        h1 新規性・結果
        p 結果から、ブロック（blocking）の方がブラー（blurring）よりも特定されにくいということが判明した。しかし、画像の質やSNSなどに投稿するための満足度（e.g. satisfaction, enjoyment, social presence, likability）としては欠落してしまう。
          |将来的にはユーザのプライバシーを保護するための手法が求められる。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Caine_Blur_vs._Block_CVPR_2017_paper.pdf") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 16:07:03

  +slide
    .title Communicating Robot Motion Intent with Augmented Reality
    .info
      .authors Michael Walker, Hooman Hedayati, Jennifer Lee and Daniel Szafir
      .conference HRI’18
      .paper_id pp. 316-324
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p ロボットの移動意思をARで伝える方法について，
          |図の4種類の方法を実装して比較してみた．
        p ロボットの身体の方向のみに関連した情報提示がベースラインよりも
          |作業効率を顕著に向上させた．
          |また，ロボットとの共同作業感と動きのわかりやすさの間にトレードオフが発生することも
          |分かった．
    .item2
      .text
        p
          img(src=`${figpath}Communicating_Robot_Motion_Intent_with_Augmented_Reality_Figure1.png`,alt="Figure1")
        p (a) 経路のチェックポイントを見せる
          |(b) 経路を矢印で書く
          |(c) ARエージェントが移動方向を見てる
          |(d) ユーザに対するロボットの位置を示唆する
    .item3
      .text
        h1 評価点
        p 移動方向の表示に関する研究は継続的に行われているが，
          |AR上での表示における調査をちゃんと（網羅的に）行っていることと，
          |結果が面白い．
        p BestPaper Nominee.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171253") ACM Library
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 16:01:10

  +slide
    .title Planning with Trust for Human-Robot Collaboration
    .info
      .authors Min Chen, Stefanos Nikolaidis, Harold Soh, David Hsu and Siddhartha Srinivasa
      .conference HRI’18
      .paper_id pp. 307-315
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p ロボットの意思決定に，人間とロボットの共同作業の信頼度を組み込んだ．
          |手法的には，部分観測可能マルコフ決定過程(POMDP)の機械学習に信頼度をパラメータとして混ぜた．
          |それにより,ロボットは
          |（１）人からの信頼度を推定
          |（２）人の行動における自分の行動の影響の理由付け
          |（３）長期的にみたチームパフォーマンス最大化可能な行動の選択
          |が可能に．
        p 実際にパフォーマンスを高められることを確認した．
          |なお，信頼度を最大化してもパフォーマンスは改善しなかった．
    .item2
      .text
        p
          img(src=`${figpath}Planning_with_Trust_for_Human-Robot_Collaboration_Figure1.png`,alt="Planning_with_Trust_for_Human-Robot_Collaboration_Figure1.png")
    .item3
      .text
        h1 評価点
        p 信頼度という観点が面白く，理論モデルに基づく実装までこぎつけているのがよい．
          |信頼度最大化がパフォーマンスを改善しないことも面白い．
        p BestPaper Nominee.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171264") ACM Library
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 15:34:59

  +slide
    .title From Dusk till Dawn: Modeling in the Dark
    .info
      .authors F. Radenovic et al.,
      .conference CVPR2016
    .item1
      h1 概要
      .text.
        画像データベースからの3次元復元の研究．
        昼のデータと夜のデータを混ぜて扱うと，輝度などの違いが大きくマッチングが上手くいかずに失敗する場合がある．
        そのため，昼と夜のデータをクラスタリングにより分割し，それぞれで3D Modelを作った後に統合する．
        昼と夜でははっきり見える部分が違うので，統合することで両者が相補的に働き，
        より高精細な3次元復元ができることを示している．

    .item2
      img(src=figpath+"180307_inthedark.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 昼と夜の画像を自動的に分けるための手法を提案
          li 昼と夜を分けてモデリングし，その後統合することの優位性を示した
    .item4
      h1 自由記述欄
      .text
        ul
          li かっこいい論文タイトル
          li 夜のデータに注目してちゃんとやってる論文は意外と珍しい気がする
        
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara
  +slide
    .title Multiple Instance Learning for Soft Bags via Top Instances
    .info
      .authors W. Li et al.,
      .conference CVPR 2015
    .item1
      h1 概要
      .text.
        Multiple Instance Learningにおいて，ラベルノイズも考慮した手法を提案．
        例えば画像認識において，画像中に対象が含まれていてもメインの被写体でなければNegativeとされることが問題と指摘．
        提案手法は明確にPositive Bag, Negative Bagを分ける (Hard Bag) のではなく，Softに両者を分ける．
        これにより，ノイズの影響を抑制し，精度を向上させることができている．

    .item2
      img(src=figpath+"180307_milsoftbag.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li MILにSoft Bagという概念を導入
          li 実験的にSoft Bagによる精度向上を確認
    .item4
      h1 自由記述欄
      .text
        ul
          li 理論的にかっちり実装されている印象
          li Deepじゃない論文
        
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara
  +slide
    .title Robust Loss Functions under Label Noise for Deep Neural Networks
    .info
      .authors A. Ghosh et al.,
      .conference AAAI 2017
    .item1
      h1 概要
      .text.
        ラベルノイズに頑健な損失関数を提案．
        理論的に，Cross entropyやMean Square ErrorよりもMean Absolute Error (MAE)の方がラベルノイズに対して頑健であることを証明し，実験的にもその優位性を確認．
        しかし，MAEによる学習は遅いので，MAEを使う場合に適した最適化手法の検討を行う必要あり．

    .item2
      img(src=figpath+"180307_robustloss.png",alt="180307_robustloss.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 従来研究だと2クラス識別の問題設定での検討が多かったが，この研究では多クラス識別での解析を行っている．
          li MAEを用いることとラベルノイズに頑健であることを示した．
    .item4
      h1 自由記述欄
      .text.
        MNISTなどの実験で主張通りの結果になっていることを確認しているが，
        もっとデータセットが実世界のものに近づいていっても同じ結果が出てくるのか気になるところ．
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara
  +slide
    .title Indirect Match Highlights Detection with Deep Convolutional Neural Networks
    .info
      .authors M. Godi et al.,
      .conference ICIPA Workshop (Social Signal Processing and Beyond), 2017
    .item1
      h1 概要
      .text.
        スポーツのハイライトシーンの検出をフィールドを撮影した動画からではなく観客の動画から間接的に行おうという研究．
        観客動画を3D CNNに入力してハイライトの尤度を推定する手法を提案．
        観客動画をCropして入力することで，どの辺りの観客が盛り上がっているかの推定も可能．

    .item2
      img(src=figpath+"180307_indirectmatch.png",alt="180307_indirectmatch.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 観客動画のみに基づいてハイライトシーンを検出
          li プレイシーンを一切見ずに検出可能
    .item4
      h1 自由記述欄
      .text.
        盛り上がっているシーンをハイライトとすると，観客が盛り上がっているのを見ることが果たして間接的なのかどうか難しい．
        ある意味直接的な認識である気も．
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara
  +slide
    .title Combining Image Regions and Human Activity for Indirect Object Recognition in Indoor Wide-Angle Views
    .info
      .authors P. Peursum et al.,
      .conference ICCV 2005
    .item1
      h1 概要
      .text.
        人の行動から間接的に物体を検出する手法を提案．
        物体がはっきり撮影できていなくても，それを使う（インタラクションする）人の行動が見えていれば，
        どのような物体を使っているかは推定できるので，それを元にして物体の検出をする．
        ベイジアンネットワークをベースにしてこのアイデアを実装．

    .item2
      img(src=figpath+"180307_combining_1.png",alt="180307_combining_1.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 人物行動に基いて物体のアピアランスに関わらず物体を認識可能
    .item4
      img(src=figpath+"180307_combining_2.png",alt="180307_combining_2.png")
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara
  +slide
    .title "Thank You for Sharing that Interesting Fact!": Effects of Capability and Context on Indirect Speech Act Use in Task-Based Human-Robot Dialogue
    .info
      .authors Tom Williams, Daria Thames, Julia Novakoff and Matthias Scheutz
      .conference HRI’18
      .paper_id pp. 298-306
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p 間接言語行為（ISA）の有無によって，タスクベースの人間-ロボット間対話においてどれほど役に立つのか，
          |ISAの理解能力なしにロボットはどれだけ機能するか調査した．
        p WoZによる実験をしてみた．
          |各条件について，人によるISAの使われっぷりを見る．
        ul
          li 慣例的社会規範(conventionalized social norms，慣習？)の有無
          li ロボットISAの理解能力の有無の条件において，ロボットのISAの使用と知覚の両方について分析
    .item2
      .text
        h1 結果
        ol
          li タスクベース人間ロボット対話において，ISAが理解できないことをちゃんと示してあっても，人はISAを普通に使う．
          li 慣例的社会規範があった場合，ISA使用は更に普通．
          li ロボットのISAのできなさは，ロボットのタスク効率と人間のロボットの知覚の両方において悪影響がある．
    .item3
      .text
        h1 評価点
        p 人がロボットに対してどう非言語行動をとるかについてはまだまだ未調査の部分が多いが，
          |そのうちの一つ，ISAについての道を示した重要な論文．
        p BestPaper Nominee.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171246") ACM Library
        p ※注　ISAとは，発話しながらも間接的に意味を伝える行為．
          |「車に乗れ」「空手の稽古があるの」＞間接的に乗車を拒否．
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 15:11:52

  +slide
    .title Social Robots for Engagement in Rehabilitative Therapies: Design Implications from a Study with Therapists
    .info
      .authors Katie Winkle, Praminda Caleb-Solly, Ailie Turton and Paul Bremner
      .conference HRI’18
      .paper_id pp. 289-297
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p 理学療法，スポーツリハビリ等の各種セラピーに支援ロボットを導入した場合に
          |どうインタラクションしたらよいかについて，セラピストへのヒアリングを基に論じる．
        p ロボットのセラピーへの従事の利益があるという我々の仮説を裏付けるため，まず下の二つを聞いてみた．
        ol
          li セラピーにおいて，自己訓練の重要な点とは？典型的な支援方法は？
          li リハビリ療法支援において，どのようにロボットが役に立つと思われる？
        p さらに，有効なHRI戦略を導くため，聞いてみた．
        ol(start="3")
          li どのように作業を評価する？
          li 患者の作業に影響を及ぼす，セラピストの役割とは？
          li 患者それぞれにロボットの行動を仕立てるための方法論？
    .item2
      .text
        h1 結果
        p ロボットは，患者の自律的エクササイズへの意識の低さに対して支援を行える．
          |スマホなどの既存手法よりも先回り的な支援を行える．
        p この結果を踏まえ，HRI戦略のデザインの方法論を示す.
    .item3
      .text
        h1 評価点
        p 聞き取り調査の結果を論文にしたいならこの論文を読むのがよい．
        p BestPaper Nominee.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171273") ACM Library
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 14:34:36

  +slide
    .title Guiding the Long-Short Term Memory model for Image Caption Generation
    .info
      .authors Xu Jia, Efstratios Gavves, Basura Fernando, Tinne Tuytelaars
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p LSTMの拡張するgLSTMの提案。画像から抽出された意味情報をLSTMの各ユニットに入力として追加し、モデルと画像コンテンツを密接に結合させる。また、短い文に偏らないように、ビーム探索時に正規化。
    .item2
      .text
        p
          img(src=`${figpath}180309gLSTM.jpg`)
    .item3
      .text
        h1 新規性・手法
        p LSTMは畳み込みで得られた画像情報から単語ごとに文章を生成する。しかし、長い分の場合には、プロセスが継続するにつれて画像情報が薄くなる。これは、シーケンスの最初に出力された単語も同様。そこで、ゲートとセル状態の計算にグローバルな意味情報を追加。意味情報は、画像とその説明から抽出し、単語列生成の過程でガイドとして使用する。
    .item4
      .text
        h1 結果・リンク集
        p Flickr8K、Flickr30K、MS COCOなどのデータセットで、現在(2017)の最先端技術と同等またはそれ以上の精度。
        ul
          li
            a(href="https://arxiv.org/pdf/1509.04942.pdf") 論文
    .slide_index #{getSlideIndex()}
  +slide
    .title Cartooning for Enhanced Privacy in Lifelogging and Streaming Videos
    .info
      .authors Eman T. Hassan, Rakibul Hasan, Patrick Shaffer, David Crandall, Apu Kapadia
      .conference CVPR 2017 Workshop on CV-COPS
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 一人称視点カメラによるライフログをアニメ調に変換することでプライバシー性を高める研究。セグメンテーションとブレンディングのみならず、エッジ強調、さらには物体（e.g. テレビ、本）をクリップアートに置き換えることでより理解しやすくプライバシーを保護するアニメ調に変換。
    .item2
      .text
        p
          img(src=`${figpath}180309CartooningFPV.png`,alt="180309CartooningFPV")
    .item3
      .text
        h1 新規性・結果
        p AMTによりユーザスタディも行なった結果、プライバシーを保ちつつ視覚的にも理解しやすい（e.g. 行動認識）動画ストリーミングを流すことに成功した。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Kapadia_Cartooning_for_Enhanced_CVPR_2017_paper.pdf") 論文
          li
            a(href="http://homes.soic.indiana.edu/emhassan/pages/Projects.html#cvcops") Project
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 13:45:20

  +slide
    .title Protecting Visual Secrets Using Adversarial Nets
    .info
      .authors Nisarg Raval, Ashwin Machanavajjhala, Landon P. Cox
      .conference CVPR 2017 Workshop on CV-COPS
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 入力画像に対して非読性を高めるため、ノイズを付与して内容がわからないように加工する（入力と出力は図を参照）。同タスクに対してGenerative Adversarial Networks (GANs)の枠組みを導入した。提案法をAdversarial Pertubation Mechanismと名付け、攻撃ネットワーク（A）と攻撃者を欺く難読化ネットワーク（O）の敵対的学習により学習を進める。学習においてプライバシーとユーティリティ（オープン化）のトレードオフはパラメータにより調整可能である。基本的な構造はDCGANに基づいていて、OはDenoising AutoEncoder。
    .item2
      .text
        p
          img(src=`${figpath}180309VisualSecrets.png`,alt="180309VisualSecrets")
    .item3
      .text
        h1 新規性・結果
        p 攻撃は画像中にQRコードが埋め込まれているかどうかで決まり、いかに敵対的ネットがQRコードの位置を検出できるかどうかで評価する。精度は75%（エラー率25%）となった。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Cox_Protecting_Visual_Secrets_CVPR_2017_paper.pdf") 論文
          li
            a(href="https://users.cs.duke.edu/~nisarg/") 著者
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 13:19:40

  +slide
    .title I Know That Person: Generative Full Body and Face De-Identification of People in Images
    .info
      .authors Karla Brkic, Ivan Sikiric, Tomislav Hrkac, Zoran Kalafatic
      .conference CVPR 2017 Workshop on CV-COPS
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 人物のセグメンテーションにより人物の検索性を低くする研究（Person De-identification）。服装レベルでのセグメンテーションについてもうまくいっている。本論文で提供するモデルは顔認識のみでなく服装や髪型といった特徴についても非読性を向上させる。手法はGANを参考に構築されており、（物体検出も組み合わせつつ）セグメンテーションを実行する。広義には人物を中心とした背景差分を行なっている。さらに、DCGANにより予め顔画像を学習する。
    .item2
      .text
        p
          img(src=`${figpath}180309DeIdentification.png`,alt="180309DeIdentification")
    .item3
      .text
        h1 新規性・結果
        p Clothing Co-Parsing (CCP)のファッションアイテムのセグメンテーション、Human3.6M datasetの背景マスクを正解として学習を行なった。結果の例は図に示すとおりである。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Kalafatic_I_Know_That_CVPR_2017_paper.pdf") 論文
          li
            a(href="http://vision.soic.indiana.edu/bright-and-dark-workshop-2017/") ワークショップ
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 12:31:03

  +slide
    .title Fribo: A Social Networking Robot for Increasing Social Connectedness through Sharing Daily Home Activities from Living Noise Data
    .info
      .authors Kwangmin Jeong, Jihyun Sung, Hae-Sung Lee, Aram Kim, Hyemi Kim, Chanmi Park, Yuin Jeong, JeeHang Lee and Jinwoo Kim
      .conference HRI’18
      .paper_id pp. 114-122
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p 生活雑音で活動量を測り，活動量を共有することのできるソーシャルネットワークなロボットを提案．
          |生活雑音を取るだけならプライバシーに配慮できて良いし，多対多でもいい感じに働く．
          |友人間ソーシャルコミュニケーションの実験してみて，プライバシー侵害を感じずに繋がってる感が出ることを確認した．
          |ついでにちゃんとしたものも作った．
    .item2
      .text
        p
          img(src=`${figpath}Fribo_A_Social_Networking_Robot_for_Increasing_Social_Connectedness_through_Sharing_Daily_Home_Activities_from_Living_Noise_Data_Figure1.png`,alt="Figure1")
    .item3
      .text
        h1 評価点
        p うまくやってる感．
          |これってロボットインタラクションなのかな？
        p BestPaper Nominee.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171254") ACM Library
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 11:59:39

  +slide
    .title What is Human-like?: Decomposing Robots' Human-like Appearance Using the Anthropomorphic roBOT (ABOT) Database
    .info
      .authors Elizabeth Phillips, Xuan Zhao, Daniel Ullman and Bertram F. Malle
      .conference HRI’18
      .paper_id pp. 105-113
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p 著者らが作った，擬人ロボットのコレクションデータベースABOTを活用して，
          |擬人ロボットの見た目について分析する．
          |ABOTは200の現実の擬人ロボットの，画像，パーツリスト，4つの観点
          |（Body-Manipulators, Surface Look, Facial Features, Mechanical Locomotion）
          |におけるスコアを含む．
        p 本研究では，調査のうえ先述の見た目に関する4つの観点を定義し，
          |またロボットの擬人性を推定しやすい特徴について解明した．
          |そのスコアリングシステムはWebで公開する．
    .item2
      .text
        p
          img(src=`${figpath}What_is_Human-like_Decomposing_Robots_Human_like_Appearance_Using_the_Anthropomorphic_roBOT_ABOT_Database_Figure1.png`,alt="Figure1")
    .item3
      .text
        h1 評価点
        p 盛りだくさん．
        ol
          li データベースABOT
          li 網羅的に定性的・定量的な評価
          li 評価ツールを公開
          li データベースの拡張性
        p BestPaper Nominee.
          |ところで被験者に0.5ドル払ったらしい．
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171268") ACM Library
          li
            a(href="http://www.abotdatabase.info/") ABOT
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 11:38:49

  +slide
    .title Characterizing the Design Space of Rendered Robot Faces
    .info
      .authors Alisa Kalegina, Grace Schroeder, Aidan Allchin, Keara Berlin, Maya Cakmak
      .conference HRI’18
      .paper_id pp. 96-104
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p “レンダリングされた”ロボット顔のデザインを仕分け．
          |デザイン空間を定義し，分布を調査する．
          |157のロボット顔を76属性のデザイン空間に落とす．
          |文脈に応じてどのようなリアルさ・具体性が好まれるのか，
          |また顔の重要なパーツの有無に対する，適したロボットの作業について論じる．
    .item2
      .text
        p
          img(src=`${figpath}Characterizing_the_Design_Space_of_Rendered_Robot_Faces_Figure1.png`,alt="Figure1")
    .item3
      .text
        h1 評価点
        p いろんなロボット顔のサーベイが大変なのは言うまでもないが，
          |ちゃんとシステマチックに論じているところが偉い．
          |まさしくワシントン大的貢献．
        p BestPaper Nominee.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171286") ACM Library
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.9 10:47:04

  +slide
    .title From Red Wine to Red Tomato: Composition with Context
    .info
      .authors Ishan Misra, Abhinav Gupta, Martial Hebert, The Robotics Institute, Carnegie Mellon University
      .conference CVPR 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 既知の視覚的概念の分類子を構成するために、コンテクストの依存性に着目した手法を提案。コアアイデアは、複数の単純な概念を組み合わせることによる複雑なコンセプトの開発。赤ワインの赤と、トマトの赤では意味は異なる。形容詞(赤)と物体(ワイン、トマト)の間にはコンテクスト依存性がある。このようなコンテクストを全てビックデータで学習することはナンセンスなため、独立した識別器を合成する。
    .item2
      .text
        p
          img(src=`${figpath}180309WineTomato.jpg`)
    .item3
      .text
        h1 手法
        p 学習： primitive（大きい、象）などの組み合わせのセットにアクセス。これらのprimitiveの各々を線形分類器（w）を学習することによってモデル化。これらの分類器を合成し、その組み合わせの分類器を生成する変換ネットワークを学習。
    .item4
      .text
        h1 コメント・リンク集
        p 闇雲にデータ数(クラス数)をあげて学習するのではなく、コンテクストの依存性に着目した予測モデルを使用するのはスマート。
        ul
          li
            a(href="https://arxiv.org/pdf/1611.05267.pdf") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Expressing Robot Incapability
    .info
      .authors Minae Kwon, Sandy H. Huang and Anca D. Dragan
      .conference HRI’18
      .paper_id pp. 87-95
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p ロボットのできないことを伝える．
          |何をやろうとしてダメで，なぜダメなのかを伝える．
          |動きの軌跡の最適化問題とみなし，成功パターンと失敗パターンの
          |類似性と差の大きさの評価を提案．
    .item2
      .text
        p
          img(src=`${figpath}Expressing_Robot_Incapability_Figure1.png`,alt="Figure1")
    .item3
      .text
        h1 評価点
        p 運動学を解くという割と確立された（けどちゃんと検討するのは面倒な）手法的な面を水平思考して，
          |ロボットの失敗談を伝えるという面白さにつなげている．
        p BestPaper Nominee.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171276") ACM Library
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.8 20:32:44

  +slide
    .title Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos
    .info
      .authors De-An Huang, el al.
      .conference CVPR 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 解説動画(お料理動画)におけるアクションとエンティティ間をリンクさせる教師なし学習を提案。グラフ表現によって、言語および視覚的モデルを共有することで、ビデオ内の視覚的・言語的曖昧さを回避する。映像中の作業に対して、言語と動画の2つのワークフローを出力し、最適化を施す。WhatsCookinデータセットより、2000の字幕付き動画を使用。
    .item2
      .text
        p
          img(src=`${figpath}180308VisualLinguistic.jpg`)
    .item3
      .text
        h1 新規性
        p 曖昧さ回避
        p 図は、動画中の曖昧なアクションとエンティティを示す。(c)の場合、3つ目のフレームで“ドレッシングを加える”とあるが、果たしてどのドレッシングなのか？というあいまいさが生じる。この場合のドレッシングはヨーグルトを混ぜたものになる。つまり、“ドレッシング”をエンティティ、“混ぜる”をアクションとしてこの2つをリンクさせる。
    .item4
      .text
        h1 結果・リンク集
        p 過去のシーンを参照することで、動画と文字列の位置合わせに有効。
        ul
          li
            a(href="https://arxiv.org/pdf/1703.02521.pdf") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Improving Collocated Robot Teleoperation with Augmented Reality
    .info
      .authors Hooman Hedayati, Michael Walker and Daniel Szafir
      .conference HRI’18
      .paper_id pp. 78-86
    .slide_editor Ryota Suzuki

    .item1
      .text
        h1 概要
        p 操作者と遠隔ロボットが部屋レイアウトを共有する空間において，ロボットの遠隔操作をARで支援する方法論についてプロトタイピングし，議論．飛行ドローンの動きを簡単にわかるようにするには？
          |図(a)視体積の表示，図(b)ロボットの吹き出し的にカメラ映像表示，図(c)端にカメラ映像を固定表示．
          |結果，3方式はカメラ映像をただ見ただけよりも遠隔操作効率が顕著に向上した．
          |また，カメラ映像を見せる方式(b)，(c)はカメラ映像とロボットの注視が分散してしまい，比較的遠隔操作効率が悪かった．
    .item2
      .text
        p
          img(src=`${figpath}Improving_Collocated_Robot_Teleoperation_with_Augmented_Reality_Figure1.png`,alt="Figure1")
        p
          img(src=`${figpath}Improving_Collocated_Robot_Teleoperation_with_Augmented_Reality_Figure2.png`,alt="Figure2")
    .item3
      .text
        h1 評価点
        p 特にハッとする面白さは感じないが，
          |各方面がなんとなくやっていたことについて，改めて調査に取り組み，
          |サーベイと実験をちゃんとやって，定量的・定性的評価をちゃんとやった点が評価されたか．
          |実験デザインも特に面白く感じないが，当たり前のことをやってちゃんと結果が出るようなデザインをしている．
        p BestPaper Nominee.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3171251") ACM Library
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.8 19:13:34

  +slide
    .title Simple Black-Box Adversarial Attacks on Deep Neural Networks
    .info
      .authors Nina Narodytska, Shiva Kasiviswanathan
      .conference CVPR 2017 Workshop on CV-COP
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ブラックボックスのAdversarial Attacks（敵対的攻撃、摂動ノイズ）を提案する。本論文での画像攻撃は局所的探索により数値的近似を行い、ネットワークの勾配に埋め込むことである。
    .item2
      .text
        p
          img(src=`${figpath}180308BlackBoxAttacks.png`,alt="180308BlackBoxAttacks")
    .item3
      .text
        h1 新規性・結果
        p 画像が結果例である。複数の画像識別が誤りを含んでいる。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="http://www.shivakasiviswanathan.com/CVPR17W.pdf") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.8 18:27:18

  +slide
    .title Deceiving Google's Cloud Video Intelligence API Built for Summarizing Videos
    .info
      .authors Hossein Hosseini, Baicen Xiao, Radha Poovendran
      .conference CVPR 2017 Workshop on CV-COP
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 動画認識タスクにおいて、Google Cloud APIの認識を騙すため動画像に対して意図的に画像挿入を行う攻撃を仕掛ける。攻撃はN秒間に一度、（動画の内容とは全く異なる）任意の画像を埋め込むことで、Google Cloud APIの出力を騙すことに成功した。
    .item2
      .text
        p
          img(src=`${figpath}180308DeceivingGoogleCloud.png`,alt="180308DeceivingGoogleCloud")
    .item3
      .text
        h1 新規性・結果
        p 実験の結果、2秒間に一度、画像挿入攻撃を仕掛けると認識誤りを引き起こすことが判明した。動画像は25FPSで構成されるため、50フレームに一度攻撃を仕掛ければ十分であった。
    .item4
      .text
        h1 コメント・リンク集
        p 本論文のような攻撃をYouTubeに埋め込まれると動画タグが自動でつけられなくなるという恐れがある一方で、例えばFacebookなどの動画に意図的かつ人の目にはわからないように画像を埋め込めると（プライバシー保護の面で）外部からは検索しづらくなる。（やはり使い方次第ということか）
        ul
          li
            a(href="https://arxiv.org/pdf/1703.09793.pdf") 論文
          li
            a(href="http://vision.soic.indiana.edu/bright-and-dark-workshop-2017/") CVPR17-COPS
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.8 18:00:10

  +slide
    .title Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection
    .info
      .authors Debidatta Dwibedi, Ishan Misra, Martial Hebert
      .conference ICCV 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p タイトルの通り、Cut, Paste and Learn（物体の切り抜き、任意画像への埋め込みにより物体検出の学習を実行）により自動アノテーションを行い、学習画像を大量に生成することに成功。ここで問題になるのは切り抜いた物体と画像埋め込みの際のアーティファクト（artifact）であり、自然な埋め込みのみならずアーティファクトを無視して学習する手法を提案した。CutのフェーズではFully-Convolutional Networks (FCN)を用いてセグメンテーションを実行するがさらに後処理にて境界線を綺麗にした。PasteのフェーズではGaussian/Poisson Blendingによりアーティファクトをできる限り削減した状態で背景画像に対して埋め込みを行う。データ拡張についても、2次元3次元の回転、オクルージョンなど行う。
    .item2
      .text
        p
          img(src=`${figpath}180308CutPasteLearn.png`,alt="180308CutPasteLearn")
    .item3
      .text
        h1 新規性・結果
        p Blendingにおいては{なし, Gaussian, Poisson}の全てを混ぜる手法が最もよくオリジナル画像のみと比較して8AP向上。データ拡張についても全ての拡張{2D rotation, 3D rotation, Truncation, Occlusion, Distractor}を行う拡張が最も良かった。ベンチマークに対して相対的に21%の向上が見られた。クロスドメインの学習においても10%の向上が見られたと報告。
    .item4
      .text
        h1 コメント・リンク集
        p セグメンテーション・ブレンディングを用いることで実画像からデータを増やすことができることがわかった。今後はCGのみでなく実画像からのデータ合成も一般的になると考えられる。
        ul
          li
            a(href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Dwibedi_Cut_Paste_and_ICCV_2017_paper.pdf") 論文
          li
            a(href="https://github.com/debidatta/syndata-generation") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.8 11:04:00

  +slide
    .title Captioning Images with Diverse Objects
    .info
      .authors Subhashini Venugopalan, et al. 
      .conference CVPR 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 既存の画像キャプションデータセットには存在しないオブジェクトカテゴリを記述できるNovel Object Captioner (NOC)の提案。物体認識データセットからの画像と、ラベル付けされていないテキストから抽出された外部ソースからの意味的情報を利用。 MSCOCOにはないImageNetのオブジェクトカテゴリのキャプションを生成。
    .item2
      .text
        p
          img(src=`${figpath}180307CaptioningWithDiverse.jpg`)
    .item3
      .text
        h1 新規性・手法
        p 画像とキャプションが対になっていないデータや、様々なソースを使って学習することができる。
        p pre-training済みモデルのembedding空間を使用できるようにし、zero-shotなデータでもキャプションを生成できる。
        p CNNベースの認識モデル、LSTMベースの言語モデル、キャプションモデルは、別々のソースで同時に学習。しかし、パラメータを共有することで、未知のオブジェクトのキャプションが可能となる。
    .item4
      .text
        h1 結果・リンク集
        p より多くのオブジェクトのキャプション生成が可能かつ、キャプションの質は同等もしくは向上している。
        ul
          li
            a(href="https://arxiv.org/pdf/1606.07770.pdf") 論文
          li
            a(href="https://vsubhashini.github.io/noc.html") ソースコード
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Neural Scene De-rendering
    .info
      .authors Jiajun Wu, et al.
      .conference CVPR 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p シーンの全体理解。オブジェクトの数とそのカテゴリ、ポーズ、位置などの情報をエンコードし、シーンのコンパクトかつ表現力豊に解釈可能な表現の提案。decoderとencoderにより、XML形式の言語表現を実現。特に、encoderは、Renderingの逆であるDe-renderingを実行することで、入力画像をscene XMLに変換する。
    .item2
      .text
        p
          img(src=`${figpath}180307NSD.jpg`)
    .item3
      .text
        h1 新規性
        p 従来研究では、encorderとdecoderベースの深層学習を使用した画像表現を提案してたが、その出力は解釈不可能であるかシーン単一のオブジェクトのみの説明である。そこで、シーン全体かつ解釈可能な表現を出力するモデルの提案。
        p マインクラフトベースの新しいデータセット。
        p de-rendering：入力画像からセグメントを生成し、オブジェクトのプロパティを解釈。推測結果を統合し、元の画像を再構成する。
    .item4
      .text
        h1 コメント・リンク集
        p 単純な全体シーン解釈は進んでいる。これからは、より複雑なシーンの解釈に移る。
        ul
          li
            a(href="http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf") 論文
          li
            a(href="http://nsd.csail.mit.edu/") NSD
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Visual Diarog
    .info
      .authors Abhishek Das, et al. 
      .conference CVPR 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p Visual Diarog：AIエージェントが人間と、画像に関した対話をするタスクを目標とする。エージェントが、画像に対する質問に、対話履歴から文脈を推測し正確に回答する。チャットデータ収集プロトコルを開発し、Visual Dialogデータセット(VisDial)を作成。COCOの120kの画像に10の質問と回答ペアを含む1つのダイアログが含まれており、合計は1.2Mのダイアログ質問回答ペア。
    .item2
      .text
        p
          img(src=`${figpath}180307VIsualDialog.jpg`)
    .item3
      .text
        h1 新規性・リンク
        p VQAとは異なる、Visual Dialogタスク。
        p 3つのエンコーダ2つのデコーダからなるVisual Diarogモデル。
        p コード、モデル、データセット、ビジュアルチャットボックスを公開中。
        ul
          li
            a(href="https://arxiv.org/pdf/1611.08669v5.pdf") 論文
          li
            a(href="https://visualdialog.org") Visual Dialog
    .item4
      .text
        p
          img(src=`${figpath}180307VIsualDialog_2.jpg`)
          
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Learning to Linearize Under Uncertainty  
    .info
      .authors Ross Goroshin  et al. 
      .conference in NIPS 2015
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 
        |「動画は単一画像を表す特徴空間上におけるManifoldとして表すことができる」という考えをもとにしている。その場合、線形な時間変化に対して各フレームを表す特徴量も線形な変位をするのが妥当であり、制約を加えることでそのような特徴空間への埋め込みを学習させる。
        //|時間の設定をもとに対応する特徴量を計算することができ、実験では仮想的なデータにおけるフレーム生成タスクにおいて、鮮明な出力が得られている
          
    .item3
      .text
        h1 手法
        p 
        |t-1, t の埋め込みベクトルzt, zt-1からt+1の埋め込みベクトルzt+1を予測し、それからt+1の画像復元を行うモデルを考えるが、以下の３つの要素を加える。（1）zの時間的変位のcos類似度が近くなるようにする、（２）max-pooling(出力m)とargmax-pooling (出力p)を行い、t+1のz(=(m, p))を求める際は、pを線形外挿により求める、（3）未来の不確実性の対処として、潜在変数δを定義。
        |argmax-poolingはソフトな近似関数を定義することで逆伝搬可能にし、δは学習時はサンプルごとにSGDにより最適化し、テスト時はランダムサンプリング。
        
    .item2
      .text
        p
          img(src=`${figpath}Linearize1.png`)
          img(src=`${figpath}Linearize2.png`)
          //img(src=`${figpath}Linearize3.png`)
          img(src=`${figpath}Linearize4.png`)
    .item4
      .text
        h1 コメント・リンク
        p
        |動画を画像表現空間上のManifold捉える視点、逆伝搬不可能な関数をソフトな関数で近似する手法、潜在変数の導入による未来の不確実性への対応が面白く、非常に参考になりそう。
        |時間の設定をもとに対応する特徴量を計算することができ、実験では仮想的なデータにおけるフレーム生成タスクにおいて、鮮明な出力が得られているが、
        |線形で動く事を前提とした仮想データや今回の特徴空間の制約が実際の識別タスクなどで有効な特徴量かの実験がないなど疑問な点もあった。

        ul
          li
            a(href="https://arxiv.org/abs/1506.03011") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Deep Image Prior 
    .info
      .authors Dmitry Ulyanov et al. 
      .conference 2017
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 
        |「CNNは理論上任意の関数を近似できるが、その構造自体に汎化性能をあげるようなPriorが含まれている」という考えのもと、ランダム初期化されたCNNを用いて高いレベルの画像復元、ノイズ除去などを行った。
        |また、CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかることも示された。深いネットワークの方が復元性能が高かった。
          
    .item3
      .text
        h1 手法
        p 
        |ノイズ画像ｚをencoder-decoderモデルに入力して、生成された画像を欠損画像にMSEを近づけるように学習。
        |注意点として、完全に学習仕切ってしまうと欠損画像と同じものが出るだけなので、学習をある程度のiterationで止めると、復元された画像が得られる。
        |CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかる。
        
    .item2
      .text
        p
          img(src=`${figpath}prior1.png`)
          img(src=`${figpath}prior2.png`)
          img(src=`${figpath}prior3.png`)
    .item4
      .text
        h1 コメント・リンク
        p
        |畳み込み処理×SGDの異常なまでの汎化性能を実験的に裏付けていると思われ非常に面白い。逆にCNNのPriorの苦手なところとして、Adversarial exampleやGANのチェッカーボード現象も関係してそう。
        |自然画像と畳み込みとの関連なのでFractal画像とも関係してそう。逆に人工データに対しては苦手とかあるのだろうか。Deformable ConvやTemporal ConvなどのPriorの気になる。

        ul
          li
            a(href="https://sites.skoltech.ru/app/data/uploads/sites/25/2017/12/deep_image_prior.pdf") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Catching the Temporal Regions-of-Interest for Video Captioning
    .info
      .authors Ziwei Yang, Yahong han, Zheng Wang
      .conference ACM MM 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 動画キャプションのため、動画中から時系列のRegions-of-Interest（RoI）を獲得する。動画中のアテンションを獲得するDual Memory Recurrent Model（DMRM）を提案して時系列の大域的構造/特徴とRoI特徴を対応づける。これにより、人間のように動画を粗く流し見することに相当するモデルが構築できる。さらに詳細に特徴を評価するため、意味的な教示（semantic supervision）を行う。
    .item2
      .text
        p
          img(src=`${figpath}180307VideoROI.png`,alt="180307VideoROI")
    .item3
      .text
        h1 新規性・結果
        p 評価にはMicrosoft Video Description Corpus (MSVD)やMotreal Video Annotation (M-VAD)を採用。動画キャプショニングにおける評価法、BLEU-4, CIDEr, METEORにてState-of-the-artな精度を得た。
    .item4
      .text
        h1 コメント・リンク集
        p 動画キャプショニングは今やると面白い？動画VQAなんかは進んでいるかも？
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3123327") 論文
          li
            a(href="https://ziweiyang.github.io/") Project
          li
            a(href="https://github.com/ziweiyang/dualMemoryModel") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.7 12:28:51

  +slide
    .title Temporal Relational Reasoning in Videos
    .info
      .authors Bolei Zhou, Alex Andonian, Antonio Torralba
      .conference arXiv:1711.08496
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 時系列の理由付け、（物体や人物行動などの）関連性を学習するTemporal Relation Network (TRN)を提案する。TRNはフレーム数を変えながら特徴表現を行い、前後の時系列を対応づけることで理由付けを行う。このネットワークを学習して時系列の対応付けを行うため、3つの動画データベースーSomething-Something（ビデオ数108,499）, Jester（148,092）, Charades（9,848）ーを用いた。
    .item2
      .text
        p
          img(src=`${figpath}180307TRN.png`,alt="180307TRN")
    .item3
      .text
        h1 新規性・結果
        p TRNは場面によりC3DやTwo-Stream ConvNetsよりも高精度。ビジュアルの結果は動画を参照。
    .item4
      .text
        h1 コメント・リンク集
        p 動画像に対しても理由付け（Reasoning）ができるようになってきた。行動検出の高精度化は待たれるが、トリミングされた動画像に対しては効果を発揮する手法。
        ul
          li
            a(href="https://arxiv.org/pdf/1711.08496.pdf") 論文
          li
            a(href="http://relation.csail.mit.edu/") Project
          li
            a(href="https://github.com/metalbubble/TRN-pytorch") GitHub
          li
            a(href="https://www.youtube.com/watch?v=D42erLb42_k") YouTube
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.7 09:23:28

  +slide
    .title Egocentric Basketball Motion Planning from a Single First-Person Image
    .info
      .authors Gedas Bertasius, Aaron Chan, Jianbo Shi
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 一人称視点の画像からゴールリングに到達するまでのバスケットボール選手の動線を生成する。本論文では3D位置や頭部方向も記録する。同タスクを実行するため、まずは画像空間から12Dのカメラ空間に投影を行うEgoCam CNNを学習。次に予測を行うCNN（Future CNN）を構築、さらに予測位置やゴールまでの位置が正確かどうかを検証するGoal Verifier CNNを用いることでより正確な推定を行うことができる。
    .item2
      .text
        p
          img(src=`${figpath}180307EgoBasketball.png`,alt="180307EgoBasketball")
    .item3
      .text
        h1 新規性・結果
        p 複数のネットワークの出力（ここではEgoCamCNNとFutureCNN）を検証するVerification Networkという考え方は面白い。他のネットワークの出力を、検証用のネットワークにより正すというのはあらゆる場面で用いることができる。RNN/LSTM/GANsなどよりも高度な推定ができることが判明した。
    .item4
      .text
        h1 コメント・リンク集
        p 結果例は動画像を参照。未来予測・３次元投影などコンポーネントがDNNにより高度にできるようになってきたからできた研究。さらに検証用のネットワークを構築することで出力自体を操作している。
        ul
          li
            a(href="https://arxiv.org/pdf/1803.01413v1.pdf") 論文
          li
            a(href="https://www.youtube.com/watch?v=wRRRl4QsUQg") YouTube
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.7 09:04:15

  +slide
    .title Beyond Context: Exploring Semantic Similarity for Tiny Face Detection
    .info
      .authors Yue Xi, Jiangbin Zheng, Xiangjian He, Wenjing Jia, Hanhui Li
      .conference arXiv:1803.01555
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p
          a(href="https://www.cs.cmu.edu/~peiyunh/tiny/") Finding Tiny Faces
          |を元ネタにして、画像中から微小な顔を検出する手法を提案。元ネタではコンテキストから小さな顔を検出していたが、本論文では画像の類似性（顔は大小に関わらず特徴が類似する）を考慮して極小な顔を検出した。手法としては、画像中から意味的に類似する領域を計算するためのMetric Learning（特徴空間の距離学習）を用いる。
    .item2
      .text
        p
          img(src=`${figpath}180307BeyondContext.png`,alt="180307BeyondContext")
    .item3
      .text
        h1 新規性・結果
        p 3つの著名な公開データに対して精度を向上させState-of-the-art（と主張しているが、結果のグラフが18/03/07現在論文に埋め込まれていない）。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1803.01555v1.pdf") 論文
          li
            a(href="https://www.cs.cmu.edu/~peiyunh/tiny/") Finding Tiny Faces
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.7 08:27:34

  +slide
    .title Toward Multimodal Image-to-Image Translation
    .info
      .authors Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman
      .conference NIPS 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ピクセル同士の画像対応を行い、画像変換を実行するBicycle GANを提案。従来のImage-to-Image (pix2pix)ではone-to-oneマッピングだったが、本提案ではマルチモーダル、すなわちある画像からあらゆるピクセルの対応関係を考慮した変換をおこなう（例として、図に示すような夜画像の入力からあらゆる日中の画像に変換するなど）。このアルゴリズムを構築するためにVAEベースやLatent RegressorのGANを組み合わせる。
    .item2
      .text
        p
          img(src=`${figpath}180306BicycleGAN.png`,alt="180306BicycleGAN")
    .item3
      .text
        h1 新規性・結果
        p pix2pixと比較して複数の結果を出力する表現力が向上した。マルチモーダルで出力しても結果画像が崩れることなく画像生成を実現した。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1711.11586.pdf") 論文
          li
            a(href="https://junyanz.github.io/BicycleGAN/") Project
          li
            a(href="https://github.com/junyanz/BicycleGAN") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.6 14:58:57

  +slide
    .title Weakly Supervised Affordance Detection
    .info
      .authors J. Sawatzky et al.,
      .conference CVPR 2017
    .item1
      h1 概要
      .text.
        物体のパーツごとのAffordanceを推定する問題の研究．
        CAD120データセットにpixel-wiseのAffordanceラベルを付けてデータセットを作成．
        CNNにより入力画像からAffordanceを推定するが，Affordanceはマルチラベル（複数のラベルを持つ画素が存在）なので，
        それに対応できるような拡張手法を提案．
        加えて，キーポイントレベルのアノテーション (Weakly label) からの学習を行う手法も提案．
        Fully supervised, Weakly supervisedの両設定においてSOTAを達成．

    .item2
      img(src=figpath+"180306_weaklyaffordance.png",alt="180306_weaklyaffordance.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li Affordance推定の問題においてKeypointアノテーションから学習する手法を提案
          li Pixel-wiseアノテーション付きで実データに近いAffordanceデータセットを提供
    .item4
      h1 自由記述欄
      .text
        ul
          li せっかくWeakly Supervisedなんだからデータをたくさん用意したらどうなるかの結果とかも見たい
          li: a(href="https://github.com/ykztawas/Weakly-Supervised-Affordance-Detection") GitHub
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title Action Recognition with Coarse-to-Fine Deep Feature Integration and Asynchronous Fusion
    .info
      .authors Weiyao Lin, Yang Mi, Jianxin Wu, Ke Lu, Hongkai Xiong
      .conference AAAI 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p AAAIに採択された、行動認識の研究。(1)より精緻な特徴量抽出、(2)異なるチャンネルの入力からの非同時性（asynchrony）を考慮して公開データベースに対する認識精度を向上させた。Coarse-, Middle-, Fine-levelの特徴量を統合して識別を実行する、さらにはそれぞれ異なる時間とチャンネル（e.g. rgb at time t & flow at time t+2）からの特徴組み合わせにより参照する尺度を変更し、特徴量をさらに強化した。
    .item2
      .text
        p
          img(src=`${figpath}180306ActionCoarseFine.png`,alt="180306ActionCoarseFine")
    .item3
      .text
        h1 新規性・結果
        p 多階層の特徴量の組み合わせや非同時性を考慮した特徴抽出により手法を構成、UCF101にて95.2%、HMDB51にて72.6%を達成した。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07430.pdf") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.6 14:31:32

  +slide
    .title MarioQA: Answering Questions by Watching Gameplay Videos
    .info
      .authors Jonghwan Mun, et al. 
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 動画によるVideoQA。マリオのプレイ動画から、発生するイベントの質疑応答を行うMarioQAを提案。イベントログを含むビデオクリップを収集し、抽出されたイベントから自動的にQAペアを生成してデータセットを構築。敵を倒す、死ぬ、ジャンプ、キック、持つなどの11個のアクションパラメータを、動画と対応させたコマンド形式で時系列にまとめたものを学習。
    .item2
      .text
        p
          img(src=`${figpath}180306MarioQA_1.jpg`)
    .item3
      .text
        h1 手法・結果・リンク
        p Gated Recurrent Unit (GRU)で質問の特徴抽出。3DFCNでビデオの特徴抽出。2つの特徴から分類。
        p NT (case 1), NT+ET (case 2) and NT+ET+HT (case 3)の3ケースについて精度を比較し、時間的推論能力を検証。ETやHTを加えた場合の方が精度が向上することを確認。
        ul
          li
            a(href="https://arxiv.org/abs/1612.01669") 論文
          li
            a(href="http://cvlab.postech.ac.kr/research/MarioQA/") MarioQA
    .item4
      .text
        p
          img(src=`${figpath}180306MarioQA_2.jpg`)
          
    .slide_index #{getSlideIndex()}

  +slide
    .title Moments in Time Dataset: one million videos for event understanding
    .info
      .authors Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, Aude Oliva
      .conference 1801.03150
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 3秒以内のラベル付けされた動画像が100万以上含まれるデータセットMoments in Time Datasetを提案。今まで動画DBでありがちであった人物のみに偏ることなく、物体や動物、自然現象なども積極的に含んでいる。
    .item2
      .text
        p
          img(src=`${figpath}180305MomentsInTime.png`,alt="180305MomentsInTime")
    .item3
      .text
        h1 新規性・結果
        p 3秒以内の瞬間的な動画にすることでノイズを含まない動画になりやすく、クラス間/クラス内のDIVERSITYを考慮、人物のみに限定せず動画像を汎用的に収集、動き自体の転移を考慮してカテゴリを定義している。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1801.03150.pdf") 論文
          li
            a(href="http://moments.csail.mit.edu/") Project
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.5 20:25:53

  +slide
    .title Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments
    .info
      .authors Peter Anderson, et al.
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 自然言語のナビゲーションを入力として、実空間の中をロボットが動き目的地に到達できるかどうかを競うベンチマーク（Visually-grounded natural language navigation in real buildings）を提案。データセットは3Dのシミュレータによりキャプチャされ、22Kのナビゲーション、文章の平均単語数は29で構成される。
    .item2
      .text
        p
          img(src=`${figpath}180305R2RNavi.png`,alt="180305R2RNavi")
    .item3
      .text
        h1 新規性・結果
        p (1) Matterport3Dデータセットを強化学習を行えるように拡張。(2) 同タスクが行えるようなベンチマークであるRoom-to-Room (R2R)を提案して言語と視覚情報から実空間にてナビができるようにした。(3) seq-to-seqをベースとしたニューラルネットによりベンチマークを構築。VQAをベースにしていて、ナビゲーション（VQAでいう質問文）と移動アクション（VQAでいう回答）という組み合わせで同問題を解決する。
    .item4
      .text
        h1 コメント・リンク集
        p 自然言語の問題はキャプションや質問回答の枠を超えて実空間、さらにいうとロボットタスクに導入されつつある。この研究はビジョン側からのアプローチだが、ロボット側のアプローチが現在どこまでできているか気になる。すでに屋内環境をある程度自由に移動するロボットが実現しているとこの実現可能性が高くなる。SLAMとの組み合わせももう実行できるレベルにある？
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07280.pdf") 論文
          li
            a(href="https://bringmeaspoon.org/") Project
          li
            a(href="https://github.com/peteanderson80/Matterport3DSimulator") GitHub
          li
            a(href="https://niessner.github.io/Matterport/") Matterport3D dataset
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.5 19:53:46

  +slide
    .title Joint Object Category and 3D Pose Estimation from 2D Images
    .info
      .authors Siddharth Mahendran, Haider Ali, Rene Vidal
      .conference arXiv:1711.07426
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 「2D画像」と「物体位置」の入力から「3D物体姿勢」と「カテゴリラベル」を出力する研究。ResNetベースのアーキテクチャを採用している。物体カテゴリが既知/未知の場合の両方で3次元物体姿勢の推定ができる。物体の回転とカテゴリ推定の同時誤差を計算する関数も定義。
    .item2
      .text
        p
          img(src=`${figpath}180305_2D23D.png`,alt="180305_2D23D")
    .item3
      .text
        h1 新規性・結果
        p 3次元物体姿勢推定とカテゴリ推定の同時回帰問題において、Pascal3D+ datasetでState-of-the-artな精度。物体カテゴリが未知の場合でもカテゴリを推定しながら3次元姿勢推定を実行することができる。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07426.pdf") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.5 19:25:04

  +slide
    .title Adversarial Attacks Beyond the Image Space
    .info
      .authors Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi Keung Tang, Alan L. Yuille
      .conference arXiv:1711.07183
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p Adversarial Examples（ネットワークを騙す摂動ノイズ）に関する研究だが、特に物体識別や質問対応（Visual Question Answering）への問題を扱う。さらに、従来の問題では2D画像を取り扱っていたが、本論文では3Dレンダリングとその2D平面投影画像に拡張する。ひとつの摂動ノイズは誤差逆伝播のエラーを直接出力の2D空間に投影すること、もうひとつは敵対的ノイズを予め2D画像に構築して物理空間からレンダリングすることである。
    .item2
      .text
        p
          img(src=`${figpath}180305VQAAttacks.png`,alt="180305VQAAttacks")
    .item3
      .text
        h1 新規性・結果
        p ここでは（１）3次元的な物理的空間を想定して摂動ノイズを加えることができるかどうかについて言及、（２）ノイズを含んだ攻撃画像が与えられた際に、それら攻撃から守るような適切な物理空間を構成できるかどうかを検討した。3次元的な物理空間の攻撃は、法線方向・光源・材質などを考慮しつつ出力に対して防衛可能であるため、2次元の画像空間よりも攻撃が難しいと主張。
    .item4
      .text
        h1 コメント・リンク集
        p 画像空間を超えてボリュームデータに対する摂動ノイズが議論され始めた。どんな空間でも埋め込める攻撃や、それらから防衛可能な手法を汎用的に考えてみたい。また、セキュリティ分野の知見はCVにもっと導入されるべき？
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07183.pdf") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.5 19:08:53


  +slide
    .title Personalized Cinemagraphs using Semantic Understanding and Collaborative Learning
    .info
      .authors T. Oh et al.,
      .conference ICCV 2017
    .item1
      h1 概要
      .text.
        Cinemagraph（画像中の一部だけ動画）を自動生成するための手法を提案．
        セマンティックセグメンテーションも導入してシーンの意味的な情報を利用し，
        高品質なCinemagraphの生成を実現する手法とした．
        さらに，動かす対象がたくさんある中でどれを選ぶとよいかをユーザごとの個人的な嗜好を学習することで，
        personalizeされた生成を実現している．
        Stablizeされている動画を入力として，
        セマンティックセグメンテーションの情報を利用したMRFの最適化によりCinemagraphを生成，
        その後学習したuser preferenceのモデルにより候補の中から選択する．

    .item2
      img(src=figpath+"180305personalizedcinemagraph.png",alt="180305personalizedcinemagraph.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li セマンティックセグメンテーションにより意味的な理解をCinemagraph生成に導入
          li 個人的な嗜好に沿ったCinemagraphの自動生成を実現
    .item4
      h1 自由記述欄
      .text
        ul
          li ユーザの嗜好を学習するためにデータにスコア付けしてもらうなど，CVよりはMultimediaっぽい論文
          li: a(href="http://web.mit.edu/taehyun/www/Research/Cinemagraph/SuppleWeb.htm") 結果サンプル
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara
  +slide
    .title Multi-Agent Cooperation and the Emergence of (Natural) Language
    .info
      .authors Angeliki Lazaridou, et al.
      .conference ICLP 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p マルチエージェント間の対話による言語学習を提案。SenderエージェントとReceiverエージェント間で簡単な画像当てゲームを実施。ゲームの正解のためにより良質なコミュニケーションが必要となり、言語を学習していく。また、ゲーム環境を変化させることで、単語の意味と画像がより良く対応するようになる。
    .item2
      .text
        p
          img(src=`${figpath}180305MultiAgent.jpg`)
    .item3
      .text
        h1 手法
        p Senderエージェントは、2枚の画像のうち1枚がtargetであると伝えられる。そして、これを伝えるためにReceiverエージェントにsymbol(メッセージ)を送信する。Receiverエージェントは、受信したsymbolの情報のみから、どちらの画像がtargetであるかを当てる。
        p SenderとReceiverに見せる画像を変える実験や、人がゲームを実施する実験を行った。
    .item4
      .text
        h1 結果・リンク
        p 人間と生産的にコミュニケーションできるAIの開発に貢献できる。言語の習得には、大量のデータだけでなく、他者との対話が重要。また、Senderが出力したsymbol(Image Netのラベルに対応したもの)を人間に見せると68%の正解率となった。
        ul
          li
            a(href="https://arxiv.org/pdf/1612.07182.pdf") 論文
          
    .slide_index #{getSlideIndex()}

  +slide
    .title Turning an Urban Scene Video into a Cinemagraph
    .info
      .authors H. Yan et al.,
      .conference CVPR 2017
    .item1
      h1 概要
      .text.
        Cinemagraph（画像中の一部だけ動画）を自動生成するための手法を提案．
        Warpingして動画中の視点を固定した後，セグメンテーションをかけてからDynamicな領域を見つけて，
        そこだけ動くようにしてCinemagraphを生成．
        街中のシーンで光やディスプレイだけが動くようなCinemagraphを自動的に生成することを可能にした．

    .item2
      img(src=figpath+"180302_turning_cinemagraph.png",alt="180302_turning_cinemagraph.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 街中で普通に撮影した動画から自動的なCinemagraphの生成を実現
          li ノイジーなWarping動画でも有効な動き解析手法を提案
          li: a(href="https://www.youtube.com/watch?v=r3yyL6qrVX4") サンプル動画
    .item4
      h1 自由記述欄
      .text
        ul
          li 特に定量的な評価はなくて，サンプルを出してうまくいっているでしょ，というやり方
          li 失敗例を出してLimitationまで議論しているけど，こういうのはCVPRだと珍しい気がする
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara
  +slide
    .title A Read-Write Memory Network for Movie Story Understanding
    .info
      .authors Seil Na, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 大規模でマルチモーダルの映画ストーリー理解のためのMovieQA を解く。新しいメモリネットワークモデルのRWMN(ReadWrite Memory Network)を提案。一連のフレームを段階的に抽象化して、より高レベルの順次情報を取得し、それをメモリスロットに格納していく。CNNを多用し、読み取りネットワークと書き込みネットワークを設計。これにより、メモリの読み書き操作に高い容量と柔軟性を持たせることができる。
    .item2
      .text
        p
          img(src=`${figpath}180304ReadWriteMemory.jpg`)
    .item3
      .text
        h1 手法
        p Embedding：ResNetとWord2Vecを用いて映画の埋め込みを行う。
        p Write： CNNを書き込みネットワークとして利用し、メモリテンソルを出力。
        p Read： CNNを使用して、一連のシーン全体をつなぎ合わせて関連付けるために、シーケンシャルメモリスロットにチャンクごとにアクセス。構成されたメモリMrを得る。
        p QA： 5つの候補中から最も信頼度のが高い回答を選ぶ。
    .item4
      .text
        h1 結果・リンク
        p ストーリーのコンテンツだけでなく、キャラクターとその行動についての理由など、より抽象的な情報を理解できる可能性を示唆。
        ul
          li
            a(href="https://arxiv.org/pdf/1709.09345.pdf") 論文
          li
            a(href="https://github.com/seilna/RWMN") ソースコード
          
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Sequence to Sequence – Video to Text
    .info
      .authors Subhashini Venugopalan, et al
      .conference ICCV 2015
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p ビデオのキャプションを生成するためのend-to-endかつ、sequence-to-sequenceモデルの提案。本手法のS2VTによって、一連のフレームを一連の単語に直接マッピングし、学習することができる。入力フレームの可変数の扱い、ビデオの時間構造を学習、自然な文法文の生成、この3点が本研究のコントリビューション。
    .item2
      .text
        p
          img(src=`${figpath}180304VideoToText.jpg`)
    .item3
      .text
        h1 手法
        p 各フレームのCNNの出力と、連続したLSTMに入力する。また、ビデオの時間構造をモデル化するためにオプティカルフローを算出し、フロー画像もCNNを介してLSTMに入力する。全てのフレームを読み込んだ後に、単語単位で文章を生成する。
        p 使用データセット：MSVD, M-VAD, MPII Movie Description
    .item4
      .text
        h1 結果・リンク
        p 評価は機械翻訳に使われるMETEORで行う。フレームの順序をランダムにした場合、スコアがかなり低減したことから、時間的構造を利用したキャプションの生成ができていることを示唆。
        ul
          li
            a(href="https://arxiv.org/pdf/1505.00487.pdf") 論文
          li
            a(href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt") ソースコード
          
    .slide_index #{getSlideIndex()}

  +slide
    .title Deformable Convolutional Networks
    .info
      .authors Jifeng Dai, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p CNNの表現力の向上を図る。CNNによる物体検出などでは、矩形を用いるために検出対象の物体だけでなく、余計な背景も含んでしまい精度低下につながる。可変可能な畳み込みとRoIプーリングを提案。これにより、画像の畳み込みを行う際に、重みに加えてセルの位置も学習する。特に、物体検出やセマンティックセグメンテーションなどのタスクに効果的。
    .item2
      .text
        p
          img(src=`${figpath}180304DeformableConvolution_1.jpg`)
    .item3
      .text
        h1 手法・結果・リンク
        p 変形可能な畳み込み：規則的(矩形)にセルをサンプリングする標準の畳み込みに、オフセットを追加することで、自由形状変形を可能にする。オフセットは追加の畳み込みレイヤを介し、前のfeature mapから学習可能。
        p 可変可能なRoIプーリング：RoIプーリング時の各binの位置にオフセットを追加する。畳み込みと同様に、前のfeature mapから学習可能。
        p 様々な条件での実験を実施。どの条件でも提案手法の精度が高い結果となった。
        ul
          li
            a(href="https://arxiv.org/pdf/1703.06211.pdf") 論文
          li
            a(href="https://github.com/msracver/Deformable-ConvNets") ソースコード
    .item4
      .text
        p
          img(src=`${figpath}180304DeformableConvolution_2.jpg`)
          
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Image Captioning with Sentiment Terms via Weakly-Supervised Sentiment Dataset
    .info
      .authors Andrew Shin, et al.
      .conference BMVC 2016
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 画像キャプショニングの中でも、画像上にはない形容詞で表現された“感情”についてのキャプションに焦点を当てる。センチメントタームを用いた画像キャプションモデルを提案。これにより、センチメントの主観的性質に対応するマルチラベル学習を実現。FlickrとDeviantArtから、2.5Mの画像と28Mのコメントを収集し，感情に対するデータセットを構築。“コメント”はキャプションとは性質が異なるが、感情を表現するために適している(否定や不適切を除く)。
    .item2
      .text
        p
          img(src=`${figpath}180304CaptioningWithSentiment.jpg`)
    .item3
      .text
        h1 手法
        p CNN→LSTMという一般的な画像キャプションの流れに、センチメント分析を行うCNNを追加する。SentiWordNetの正または負のスコアが0.5以上の単語を感情単語とする。
        p SentiWordNet：意見聴衆のための語彙リソース。正、負、客観性の3つの感情スコアを算出。
    .item4
      .text
        h1 結果・リンク
        p キャプションが適切出るかどうかと、キャプションのランク付けの2つの人間による評価。モデルからのキャプションがイメージの感情に関してより適切であるという結果となった。
        ul
          li
            a(href="http://www.bmva.org/bmvc/2016/papers/paper053/paper053.pdf") 論文
          li
            a(href="http://nmis.isti.cnr.it/sebastiani/Publications/LREC10.pdf") SentiWordNet

    .slide_index #{getSlideIndex()}

  +slide
    .title Representation Learning by Learning to Count 
    .info
      .authors Mehdi Noroozi et al. 
      .conference ICCV 2017 (Oral)
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 
        |「画像内のprimitiveを認識できることは高次の特徴を掴んでいる」という考えを基にした、self-supervisedな特徴表現学習手法。
        |画像のオリジナルとそれらを各タイルに分割したものを同じNNに入力し、出力されるタイルのprimitive数の和とオリジナルのprimitive数が一致するように学習する。
        |しかしそれでは出力を単に小さくするように学習することで損失を０にできてしまうので異なる画像も含めたcontrastiveな損失を用いる。
          
    .item3
      .text
        h1 新規性・結果
        p 
        |画像識別、物体検出、意味領域分割などのタスクで評価を行っており、識別ではSoTA。
        |学習したNNからの出力を確認すると、ノルムが大きいものは高次な物体が含まれる画像、小さいものは低次なテクスチャしか含まない画像が得られた。
        |これからNNが高次なprimitiveをcountしていることが考察できる。
        
    .item2
      .text
        p
          img(src=`${figpath}count1.png`)
          img(src=`${figpath}count2.png`)
          img(src=`${figpath}count3.png`)
        
    .item4
      .text
        h1 自由記述欄
        p
        |損失を最小化することで結果的にNNが「何かしらのprimitiveを数えていること」になり、冒頭の考えと合わせることで特徴表現学習が可能となる。 
        |何か明示的に数える対象を与えるように想像したが、実際に何を数えているかは学習ベースで、明示的には与えていない点が非常に面白い。

        ul
          li
            a(href="https://arxiv.org/abs/1708.06734") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Visual Storytelling
    .info
      .authors Ting-Hao (Kenneth) Huang, et al.
      .conference NAACL 2016
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p アルバムのような時系列画像でキャプション生成を行うためのデータセット。ストーリ性のある画像キャプションデータセット：SINDを構築。10,117個のFlickrアルバム、210,819枚の写真。各アルバムは平均20.8枚。
        p descriptions for images in isolation (DII)：画像一枚の記述
        p descriptions of images in sequence (DIS)：連続画像
        p stories for images in sequence (SIS)：ストーリー
    .item2
      .text
        p
          img(src=`${figpath}180303VisualStorytelling_1.jpg`)
    .item3
      .text
        h1 手法・リンク
        p SINDを使いキャプションをイメージごとに生成(Table5)。ストーリー性を含んだキャプションが生成できている。METEORによるスコアも向上。
        ul
          li
            a(href="https://arxiv.org/pdf/1604.03968.pdf") 論文
    .item4
      .text
        p 
          img(src=`${figpath}180303VisualStorytelling_2.jpg`)

    .slide_index #{getSlideIndex()}

  +slide
    .title Inferring and Executing Programs for Visual Reasoning
    .info
      .authors Justin Johnson, Bharath Hariharan, Laurens van der Maaten, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 理由に基づいたVQA。既存の手法では，入力を出力に直接マッピングしているため，視覚的推論の学習というよりも，データの偏りを学習しているといえる。そこで，理由を伴った視覚的推論モデルを提案。モデルは、プログラムジェネレータと実行エンジンの2部構成。CLEVRベンチマークを使用し評価。回答の柔軟性、拡張性の向上。
    .item2
      .text
        p
          img(src=`${figpath}180303VisualReasoning_1.jpg`)
    .item3
      .text
        h1 手法・リンク
        p プログラムジェネレータは、質問の読み取り、単語の羅列として表現される質問から質問に答えるためのプログラムを生成する。基本的にはLSTMのsequence-to-sequenceの考え方。
        p 実行エンジンは、予測されたプログラムをミラーリングするニューラルモジュールネットワークを構成し、実行することで画像から回答を生成。
        ul
          li
            a(href="https://arxiv.org/pdf/1705.03633.pdf") 論文
          li
            a(href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf") seq-to-seq
    .item4
      .text
        p 
          img(src=`${figpath}180303VisualReasoning_2.jpg`)

    .slide_index #{getSlideIndex()}
    
  +slide
    .title Deep mutual learning 
    .info
      .authors Ying Zhang, Tao Xiang, Timothy M. Hospedales, Huchuan Lu
      .conference 2017
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 教師モデルと生徒モデルを分けていた従来の蒸留に対してモデル同士の相互学習を提案。ハードラベルによる交差エントロピーと対象モデル以外のモデルの出力とのKL距離を最小化するように学習する。様々なモデル同士の相互学習実験や通常の蒸留との比較、相互学習を行った場合の解がより高い汎化性能を保有していることの検証実験も行っている。

        
    .item2
      .text
        p
          img(src=`${figpath}deep_mutual2.png`)
          img(src=`${figpath}deep_mutual3.png`)
          
    .item3
      .text
        h1 新規性・結果
        p 画像識別において通常の蒸留を行うよりも精度が良くなった。生徒モデルの中で相対的に小規模なモデルのみならず大規模なモデルも独立で学習を行うより精度が良かった。さらに相互学習を行うことで、wider minimaに収束しているという実験結果も得られた。特に出力される事後確率のエントロピーが大きくなるように学習されることがwider minimaへの収束を促していることがいわれている。
        
    .item4
      .text
        h1 自由記述欄
        p ハードラベルありき（ないと相互学習が正しい方向に向かわない）の手法であったが、教師なし手法に拡張できたら面白くなりそうだと感じる。
        ul
          li
            a(href="https://arxiv.org/abs/1706.00384") 論文
    .slide_index #{getSlideIndex()}
  
  +slide
    .title Learning Features by Watching Objects Move 
    .info
      .authors Deepak Pathak, Ross Girshick, Piotr Dollar, Trevor Darrell, and Bharath Hariharan
      .conference CVPR 2017
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 動き特徴を利用した前景（物体）領域情報は汎用的な表現学習に役立つという考えから、NLCなどのhand-craftな手法を組み合わせて擬似的な動体領域を作成し、それを教師として領域分割をCNNに解かせることで表現特徴を得る。物体検出、物体・行動認識、意味領域分割の問題設定において評価を行った。表現学習のデータとしてYFCCを用いている。
        
    .item2
      .text
        p
          img(src=`${figpath}watching_object.png`, alt="180302AffordanceNet")
          img(src=`${figpath}watching_object3.png`, alt="180302AffordanceNet")
          img(src=`${figpath}watching_object2.png`, alt="180302AffordanceNet")
          
    .item3
      .text
        h1 新規性・結果
        p Pascal VOCの物体検出において教師なし表現学習でSoTA。特にfine-tuningに利用するデータが少量の場合と多くの層のパラメータを固定してfine-tuinigした場合で大きな効果を発揮した。しかし、物体・行動認識、意味領域分割においては従来手法より劣っている。
        
    .item4
      .text
        h1 自由記述欄
        p 実験は丁寧に行われてる印象。表現学習の設定自体が物体検出を意識しているようにも感じられ（単一物体が写っている画像を優先的に取り出している？など）、物体検出でうまくいくのは当たり前な気もした。
        |しかし、意味領域分割で精度が出ない原因がよくわからなかった（物体部分はできているが背景の分割ができていない？）。
        ul
          li
            a(href="https://arxiv.org/abs/1612.06370") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Focal Loss for Dense Object Detection
    .info
      .authors Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar, Facebook AI Research (FAIR)
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 1-stage物体検出手法の精度向上を図る。 YOLOやSSDなどは，矩形領域における前景と背景の面積が不均衡であるため，2-stage物体検出手法に勝てないと推測。この問題を解決するためにクロスエントロピーを再構築したFocal Lossを提案。学習時のネガティブサンプルの影響を減らすことができる。
    .item2
      .text
        p
          img(src=`${figpath}180303FocalLoss.jpg`)
    .item3
      .text
        h1 手法
        p Focal Loss
        p クロスエントロピーに重みを追加
        ul
          li 正解の場合には重みを低減
          li 不正解の場合には従来通り
        p →正解になりやすい背景に引っ張られなくなる
    .item4
      .text
        h1 結果・リンク集
        p 既存の最先端2ステージ検出器(2017年現在)の全てにおいて精度を上回り，既存の1ステージ検出器の検出速度と同等
        ul
          li
            a(href="https://arxiv.org/pdf/1708.02002.pdf") 論文
          li
            a(href="https://github.com/facebookresearch/Detectron") ソースコード

    .slide_index #{getSlideIndex()}

  +slide
    .title Towards Diverse and Natural Image Descriptions via a Conditional GAN
    .info
      .authors Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 画像キャプショニングの性能向上を図る。従来のロバスト性が低いRNNに代わって，GANのフレームワークを採用することで，自然性と多様性を向上。図より，ジェネレータ(G)が文を生成し，ディスクリミネータ(E)が文や段落がどれだけうまく記述されているかを評価する。GとEを同時に学習させることにより，自然な文章を生成。
    .item2
      .text
        p
          img(src=`${figpath}180303TowardsDiverse_1.jpg`)
    .item3
      .text
        h1 結果・リンク集
        ul
          li 人間，G-MLE，G-GAN(本手法)の3つを比較して性能評価
          li ユーザー調査、定性的な例、および検索アプリケーションなどの評価により，より自然かつ多様、意味的に関連する記述を実現
        ul
          li
            a(href="https://arxiv.org/pdf/1703.06029.pdf") 論文
          li
            a(href="https://arxiv.org/abs/1411.4555") MLE
    .item4
      .text
        p
          img(src=`${figpath}180303TowardsDiverse_2.jpg`)

    .slide_index #{getSlideIndex()}

  +slide
    .title Learning to Segment Every Thing
    .info
      .authors Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ラベルが完全に手に入らない際にでも転移学習が可能なセグメンテーション手法（論文中ではPartially Supervised Training Paradigm, weight transfer functionを紹介）を提案する。条件として、bboxが手に入っている物体に対してセグメンテーション領域を学習可能。Mask R-CNNをベースとしているが、Weight Transfer Functionを追加、セグメントの重みを学習・推定して誤差計算と学習繰り返し。
    .item2
      .text
        p
          img(src=`${figpath}180303SegmentEverything.png`,alt="180303SegmentEverything")
    .item3
      .text
        h1 新規性・結果
        p Visual Genome Datasetから3,000の視覚的概念を獲得、MSCOCOから80のマスクアノテーションを獲得した。
    .item4
      .text
        h1 コメント・リンク集
        p 弱教師付き学習が現実的な精度で動作するようになってきた？アノテーションはお金や知識があっても非常に大変なタスクであり、いかに減らすかという方向に研究が進められている。（What's next?ー弱教師/教師なしの先とは？）
        ul
          li
            a(href="https://arxiv.org/pdf/1711.10370.pdf") 論文
          li
            a(href="http://ronghanghu.com/") 著者
          li
            a(href="http://kaiminghe.com/") Kaiming He
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.3 10:46:40

  +slide
    .title One-Shot Visual Imitation Learning via Meta-Learning
    .info
      .authors Chelsea Finn*, Tianhe Yu*, Tianhao Zhang, Pieter Abbeel, and Sergey Levine
      .conference NIPS 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ワンショットで人間の教示を模倣するロボットのための学習「One-shot Imitation Learning」を提案。人間が物体を把持するなど動作を教示するとロボットが特徴や動作を学習してタスクをこなす様子を学習。Model-Agnostic Meta-Learning（MAML; ICML2017）を応用したモデルを提案し、（VR空間、人間のデモによる）教示から動作を学習する。アーキテクチャはCNNをベースとしてRGB入力から特徴を抽出、中間層（全結合層直前）からロボットの動作やバイアス項を入力してロボットの行動（pre-/post-update）を出力する。
    .item2
      .text
        p
          img(src=`${figpath}180303OneshotImitation.png`,alt="180303OneshotImitation")
    .item3
      .text
        h1 新規性・結果
        p ロボットの把持タスクをシミュレーション/実空間にておこなった。シミュレーションでは提案法のMILが1ショットで85.81%、5ショットで88.75%（従来法LSTMでは各78.38%, 83.11%）。実空間では90%を実現（従来法LSTM/contextualでは25%）。詳細にはプロジェクトページやプレゼンのビデオを参照。
    .item4
      .text
        h1 コメント・リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1709.04905.pdf") 論文
          li
            a(href="https://sites.google.com/view/one-shot-imitation") Project
          li
            a(href="https://github.com/tianheyu927/mil") GitHub
          li
            a(href="https://vimeo.com/252186304") Presen
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.3 10:13:42

  +slide
    .title BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography
    .info
      .authors Michael J. Wilber, Chen Fang, Hailin Jin, Aaron Hertzmann, John Collomosse, Serge Belongie
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p CVにおけるデータセットでは，写真に対するラベル付けが一般的。写真だけでなく，イラストや風景画などに対して，以下の3属性のラベルを付加。
        ul
          li メディア：漫画、油絵、鉛筆スケッチ、水彩画などで作成した画像にラベル付け
          li 感情：視聴者に平静、幸せ/陽気、悲しい/悲観的、恐ろしい/恐れを感じさせるような画像にラベル付け
          li オブジェクト：自転車、車、猫、犬、花、人などの画像にラベル付け
    .item2
      .text
        p
          img(src=`${figpath}180302BehanceArtisticMedia.jpg`)
    .item3
      .text
        h1 手法
        p Behance Artistic Media Dataset
        ul
          li ラベル作成にhuman-in-the-loopを採用し，人間とコンピュータのハイブリッドを図る。
          li 全てのラベルについて学習し，ランク付けを行う。その後，高い順位の画像を人がラベル付け。これを4回繰り返す。
          li 基本的にはLSUNの方法に基づいている(リンク参照)
    .item4
      .text
        h1 結果・リンク集
        p 物体認識や物体検出，画像の類似度，属性推定など様々なタスクの機械学習実験を実施。定性的な評価にはなるが，明らかにVOCやImageNetなどの既存のデータセットよりも広い表現の画像で多くのタスクが処理可能となる。
        ul
          li
            a(href="https://arxiv.org/pdf/1704.08614.pdf") 論文
          li
            a(href="http://behance.net") Behence
          li
            a(href="https://arxiv.org/abs/1506.03365") LSUN
    .slide_index #{getSlideIndex()}

  +slide
    .title AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection
    .info
      .authors Thanh-Toan Do, Anh Nguyen, Ian Reid, Darwin G. Caldwell, Nikos G. Tsagarakis
      .conference ICRA 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 物体検出とアフォーダンス（というよりは機能？）のセグメントを同時に回帰するネットワーク、AffordanceNetに関する論文。ロボットへの把持位置/機能教示を行うことができる。基本的なモデルはMask R-CNNを適用していて、物体検出のためのbboxと物体に対する機能セグメントを正解として学習する。多タスクの誤差関数は物体カテゴリ、座標、機能セグメントの3つに関するものである。
    .item2
      .text
        p
          img(src=`${figpath}180302AffordanceNet.png`,alt="180302AffordanceNet")
    .item3
      .text
        h1 新規性・結果
        p 従来、物体検知と機能推定は別個に学習・認識されていたが、本研究では多タスク学習の枠組みで、単一モデルにてEnd-to-End学習した。IIT-AFF Datasetにて73.35（SoTAは69.62）、UMD Datasetにて79.9（SoTAは77.0）。モデルも公開されており、誰もがAffordanceNetを実装できるようにしている。
    .item4
      .text
        h1 コメント・リンク集
        p 任意のセグメンテーションラベルさえあれば、物体検知とあらゆる高次なセグメンテーションモデルが実現可能となった。
        ul
          li
            a(href="https://arxiv.org/pdf/1709.07326.pdf") 論文
          li
            a(href="https://github.com/nqanh/affordance-net") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 20:38:13


  +slide
    .title Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN
    .info
      .authors Jiyang Gao, Zijian (James) Guo, Zhen Li, Ram Nevatia
      .conference arXiv:1711.07607
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 画像識別における知識蒸留（Knowledge Distillation）の内容である。本論文ではある概念（e.g. 動物認識、人工物認識）ごとに教師となる識別器を事前学習しておき、それらの知識を単一の識別器に学習（これをKnowledge Concentrationと呼称）する。いわば複数の先生がある生徒に教えるという流れで学習する。
    .item2
      .text
        p
          img(src=`${figpath}180302KnowledgeConcentration.png`,alt="180302KnowledgeConcentration")
    .item3
      .text
        h1 新規性・結果
        p よくも悪くも、現状のCVはImageNetの1,000カテゴリに頼っているが、これを100倍の100,000カテゴリに増やして学習したらどうか？また、いかにしたら効率よく学習ができるかを検討した。結果はSingle Model（従来のようなCNNによるカテゴリ識別学習）にするよりも複数のスペシャリスト識別器から知識蒸留を行う方が効率よく、精度よく学習ができた。本論文で使用したEFT（Entity-Foto-Tree）データセットはカテゴリ数でImageNetの100倍、JFT-300Mの5倍である。
    .item4
      .text
        h1 コメント・リンク集
        p 読んだだけでGoogleの研究所であることがわかってしまう論文。実行ができるかどうかはさておき、大規模データの扱いやCNN学習の効率化という意味でも精読すべき論文。
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07607.pdf") 論文
          li
            a(href="https://jiyanggao.github.io/") 著者
          li
            a(href="http://codecrafthouse.jp/p/2018/01/knowledge-distillation/") 知識蒸留の参考
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 20:04:42


  +slide
    .title We Are Humor Beings: Understanding and Prediction Visual Humor
    .info
      .authors Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Virginia Tech, TTI-Chicago, Facebook AI Research
      .conference CVPR 2016
    .item1
      h1 概要
      .text
        ul
          li 視覚とユーモアの関係をモデル化(不調和説に基づく)
          li アニメ画像の面白さを推定
          li 画像のオブジェクトと面白さの関連性を推定
          li データセットの作成
          li 抽象的なシーンを使用したユーモアを引き起こすシーンの理解
    .item2
      img(src=figpath+"180205HumorBeings.jpg")
    .item3
      h1 手法1
      .text
        p 面白さ推定
        ul
          li 特徴抽出し，重み付き誤差平均(面白さによって誤差を変える)
          li Abstract Scenesのアニメ画像(VQAの画像とクラウドソーシングで依頼した面白い画像)に5段階評価したデータセット(AVH)
    .item4
      h1 手法2
      .text
        p 面白い画像・面白くない画像の変換
        ul
          li オブジェクトを変更することで，面白い⇔面白くない画像に相互変換
          li AVHから面白いものを選び，クラウドソーシングで依頼した面白くない画像を使い/面白い画像ペアを作成
          li どのオブジェクトが面白さに影響しているか調査
        p 結果：特に人や動物などのオブジェクトが面白さに影響
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Munetaka Minoguchi

  +slide
    .title Vehicle Three-Dimensional Pose and Shape Estimation from Multiple Monocular Vision
    .info
      .authors Wenhao Ding, Shuaijun Li, Guilin Zhang, Xiangyu Lei, Huihuan Qian, Yangsheng Xu
      .conference arXiv:1802.03515
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p オーバーラップが少ない複数視点カメラから自動車の3次元姿勢や形状を復元する研究。CNNにより自動車のキーポイントや姿勢/3次元形状を出力する。これら情報をヒントに、カメラ視点を推定する。2D画像上でのキーポイント推定にはconv-de-convを4回繰り返すhourglassアーキテクチャを採用、3次元姿勢や形状の推定にはCross Projection Optimization (CPO)を採用し2D-3Dの投影誤差を最小化した。
    .item2
      .text
        p
          img(src=`${figpath}180302Vehicle3DPose.png`,alt="180302Vehicle3DPose")
    .item3
      .text
        h1 新規性・結果
        p 物体のキーポイント検出においてState-of-the-art。6DoF推定手法 (Pavlakos, ICRA17)では12キーポイントの平均誤差が37.88であったが、提案手法では10.48まで低減した。また、回転/並進誤差も3DVP (Xiang+, CVPR15)では11.18/N/Aであったが、2.87/4.73まで向上させた。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1802.03515.pdf") PDF
          li
            a(href="https://arxiv.org/abs/1703.04670") 6-DoF Object Pose from Semantic Keypoints
          li
            a(href="http://cvgl.stanford.edu/papers/xiang_cvpr15_3dvp.pdf") Data-Driven 3D Voxel Patterns for Object Category Recognition
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 10:15:30


  +slide
    .title Joint Event Detection and Description in Continuous Video Streams
    .info
      .authors Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko
      .conference arXiv:1802.10250
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p End-to-Endでイベント検出（行動の時系列セグメント化）とキャプショニングを実行するタスクを提供する。モデルには3D Convolutionや階層的LSTM（two-level hierarchical LSTM）を採用した。基本的には
          a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
          |をベースにして研究を行なっているが、{Controller, Captioner} LSTMの二段階により前の候補のセンテンスやビデオコンテキストを考慮しつつ状態を更新（Controller LSTM）し、候補領域の特徴を参照しつつキャプションを生成（Captioner LSTM）する。時系列候補領域とキャプションはmulti-task学習、End-to-Endで学習される。
    .item2
      .text
        p
          img(src=`${figpath}180302VideoCaption.png`,alt="180302VideoCaption")
    .item3
      .text
        h1 新規性・結果
        p 候補領域生成やキャプショニングの精度を検証した。また、データセットにはActivityNet CaptionsやTACoS Datasetを用いた。候補領域については従来法のDAPが30, multi-scale DAPが38 @AUC (IoU>0.8)に対して提案法であるJEDDi-Netは58.21を記録した。また、キャプショニングについては従来法が{17.95, 4.82, 17.29} (各BLEU1, METEOR, CIDEr)の問題に対して{19.97, 8.58, 19.88}を記録した。
    .item4
      .text
        h1 コメント/リンク集
        p 動画のタスクはカテゴリのみでなく言語やより表現力豊かな認識ができなければいけない時期になって来た？時系列表現にもまだまだ課題が多いので、これからさらに動画認識にチャレンジすべき。また、キャプショニングの問題は感性評価に対する知見や確固たる評価方法が確立されるとさらに面白くなるのではないか。
        ul
          li
            a(href="https://arxiv.org/pdf/1802.10250.pdf") PDF
          li
            a(href="https://www.bu.edu/cs/profiles/kate-saenko/") Kate Saenko
          li
            a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
          li
            a(href="https://github.com/kenshohara/3D-ResNets-PyTorch") 3D Convolution (3D-ResNets-PyTorch)
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 09:36:26

  +slide
    .title Neural Aesthetic Image Reviewer
    .info
      .authors W. Wang, et al.
      .conference arXiv:1802.10240
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p AIが写真の感性評価やコメント生成を行なってくれる。写真とそのコメントが対応づけられた大規模DBであるAVA-Reviews dataset（52,118画像、312,708コメント）を学習することで写真を入力して図の(1)Predictionや(2)Commentsのようなものが得られる。モデルはCNNにより感性評価（Low-/High-Aesthetic category）を、CNN+LSTM（RNN）によりコメント（e.g. Fastastic colors）を出力する。
    .item2
      .text
        p
          img(src=`${figpath}180301NeuralAesthetic.png`,alt="180301NeuralAesthetic")
    .item3
      .text
        h1 新規性・結果
        p (i) 人間のような画像に対する感性評価（image aesthetics）をコンピュータに実装した。(ii)自然言語の出力により人間の高次な認知能力を実現。(iii) 画像-言語の組み合わせによるデータセットAVA-Reviews datasetを新規に構築した。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1802.10240.pdf") PDF
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 08:39:04


  +slide
    .title Interpreting CNN Knowledge via an Explanatory Graph
    .info
      .authors Q. Zhang, et al.
      .conference AAAI 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 深層学習の解釈性に関する論文であり、畳み込み層の特徴マップの応答を外的に解析して対応する反応を可視化。
          |畳み込みの各フィルタが異なる部位（e.g. 馬の耳や目）に反応するので、グラフにより解析して元画像の対象位置にアクセス。

    .item2
      .text
        p
          img(src=`${figpath}180301interpretability.png`,alt="180301interpretability.png")
    .item3
      .text
        h1 新規性・結果
        p Ground-truthなしに各部位に関する解釈性を与えたことが新規性である。
          |図に示すように入力画像に対するパーツごとの解析をフィルタの反応やグラフの解析から可視化することができる。
          |さらに、異なる画像間においても一貫性のある反応を得ることができた。
    .item4
      .text
        h1 自由記述欄
        p 深層学習は教師なしによる解釈性を獲得しているが、まだ反応している部分の可視化や部分ごとの解析が進んでいるにすぎない。
          |さらなる発展のためには、言語的な解釈や人間にわかりやすい加工（イラストとか？）が必要になるのではないだろうか。
    .slide_index #{getSlideIndex()}

  +slide
    .title HemaApp: Noninvasive Blood Screening of Hemoglobin using Smartphone Cameras
    .info
      .authors E. J. Wang et al.,
      .conference Ubicomp 2016
    .slide_editor Kensho Hara

    .item1
      .text
        h1 概要
        p スマホカメラを使って非侵襲なヘモグロビン濃度の測定を実現．
          |血中の酸素飽和度の測定などはこれまでにもあったがヘモグロビン濃度まで測定できているものはなかった．
          |照明条件とRGBの変化からヘモグロビン濃度を推定するためのアルゴリズムを提案．
    .item2
      .text
        p
          img(src=`${figpath}180228_hemaapp.png`,alt="180228_hemaapp.png")
    .item3
      .text
        h1 新規性・結果
        p 特別な装置を使うことなく簡単にスマホカメラでヘモグロビン濃度測定を実現した点．
    .item4
      .text
        h1 自由記述欄
        p システムやユーザスタディの完成度や完全に実現できたときの有用性が評価された？
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.1 10:44:03

  +slide
    .title End-to-end Driving via Conditional Imitation Learning
    .info
      .authors Felipe Codevilla, Matthias Müller, Alexey Dosovitskiy, Antonio López, and Vladlen Koltun,
      .conference arXiv
      .paper_id 1710.02410
    .item1
      h1 概要
      .text.
        自動運転を模倣学習により行う手法を提案。実空間での学習結果をヴァーチャルな空間での自動運転にて再現することができた。RGB画像、計測（e.g. スピード）や命令（e.g. turn right）などからステアリング、アクセル、ブレーキなどのコマンドを出力して自動車を操作する。
    .item2
      img(src=figpath+"180205conditionalimitationlearning.png")
      img(src=figpath+"180205framework_imitation.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 模倣学習による自動運転を実現した。
          li 実空間とシミュレーションベースの転移を行うことにも成功。
    .item4
      h1 リンク集
      .text
        ul
          li 自動運転の学習はシミュレーションベースで完結してしまう可能性がある。
          li メタ学習/模倣学習/強化学習などはCVに徐々に取り入れられてくるはずなので、2018年は学習しておいて損はない？
          li: a(href="https://arxiv.org/pdf/1710.02410.pdf" target="blank") [論文] End-to-end Driving via Conditional Imitation Learning
          li: a(href="https://www.youtube.com/watch?v=cFtnflNe5fM" target="blank") YouTube
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

  +slide
    .title Open3D: A Modern Library for 3D Data Processing
    .info
      .authors Qian-Yi Zhou et al.,
      .conference arXiv
      .paper_id 1801.09847
    .item1
      h1 概要
      .text
        |3Dデータを取り扱い、迅速な開発を可能にする
        a(href="http://www.open3d.org/" target="blank") Open3D
        |を提供する。Open3DはC++/Pythonをサポート、並列化にも対応しており、クラウドで開発することが可能。
        |点群読み込み-ダウンサンプリング-法線の計算、シーン再構築、3次元可視化などの処理が含まれている。
    .item2
      img(src=figpath+"180205open3d.png")
    .item3
      h1 新規性・結果
      .text
        |3次元画像処理のコミュニティにて有益なオープンソースを提供し、その
        a(href="https://github.com/IntelVCL/Open3D" target="blank") コード
        |も提供されている。
    .item4
      h1 コメント・リンク集
      .text
        ul
          li: a(href="http://www.open3d.org/" target="blank") Project page
          li: a(href="https://arxiv.org/pdf/1801.09847.pdf" target="blank") [論文] Open3D: A Modern Library for 3D Data Processing
          li: a(href="https://github.com/IntelVCL/Open3D" target="blank") Code
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

  +slide
    .title Hierarchical Variational Autoencoders for Music
    .info
      .authors A. Roberts et al.,
      .conference NIPS WS on Machine Learning for Creativity and Design, 2017.
    .item1
      h1 概要
      .text
        | 音楽を生成するためのHierarchical Variational Autoencoders (VAE) を提案．
        | エンコーダとデコーダがLSTMで構成されているReccurent VAEがベース．
        a(href="https://goo.gl/twGuP2") 結果サンプル
        | 長い音楽（実験では32小節）を単純なLSTMデコーダで生成するのは難しいので，
        | この研究では複数のLSTMを階層的に重ねて，段階的に長くしていくHierarchical VAEを提案．
        | ループメロディの外挿や，メロディの生成，3ピース構成の音楽生成の実験で性能を検討．
        | 結果の音楽やコードは公開されている．
    .item2
      img(src=figpath+"hierarchical_vae.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 階層的なLSTMによるデコーダをVAEによる音楽生成に導入
          li
            a(href="https://goo.gl/twGuP2") 結果サンプル
    .item4
      h1 自由記述欄
      .text
        ul
          li これも長期的な構成を考えて生成することはできていない
          li Future Workにはインタフェースを作るとあるし，1曲まるごと作るというよりは適当にサンプルを出して作曲家のアイデアを促進することを目指しているのかな．
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title Generating the Future with Adversarial Transformers
    .info
      .authors C. Vondrick et al.,
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        未来の動画を予測して生成する手法を提案．
        4フレーム x 64画素 x 64画素のクリップを入力として，その後の16フレームの動画を生成．
        完全に新しいフレームを生成するのは難しいので，入力フレームの変換により未来のフレームを生成するのがポイント．
        論文の主張としては，きれいな動画を作るにはLow-Levelな情報が重要だけど，未来予測のためにはHigh-Levelな理解も必要で，
        その両者を一つのネットワークで一気に学習するのは難しいとしている．
        だから，Low-Levelな情報は元のフレームを変換することで引っ張ってきて，ネットワークはHigh-Levelな特徴抽出に集中させるのが良いとのこと．
        このネットワークの学習はGANベース．
        生成動画の主観評価や可視化，Generatorの特徴を利用した物体認識タスクなどで性能を評価．

    .item2
      img(src=figpath+"generating_future.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 元のフレームからの変換により未来のフレームを生成する手法を提案
          li 未来の動画生成において，敵対的学習により大規模な教師なしデータを利用した学習を実現
          li 直接動画を生成したり，回帰誤差で学習したりする手法よりも良いことを主観評価実験で確認          
    .item4
      h1 自由記述欄
      .text
        ul
          li 入力が4フレームだけだけど，もっと増やすと性能は変わるのか気になる
          li 主観評価で本物と比較すると提案手法が一番嫌われている率が高いのもちょっと気になる
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title DensePose: Dense Human Pose Estimation In The Wild
    .info
      .authors Rıza Alp Guler et al.
    .item1
      h1 概要
      .text.
        身体の表面形状まで考慮したDenseな姿勢推定手法に関する研究。サーフェイスモデルを提供するSMPLタイプとアノテーションベースのMSCOCOタイプを提供。手法はMask RCNN（w/ ResNet-50, ROI-align, Regression）をベースに構築している。

    .item2
      img(src=figpath+"180205densepose.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li DenseReg [Guler,CVPR17]は顔表面の推定に対して、本研究では身体全体の表面やデンスなポイントを回帰。
          li SMPLやDense-COCOのデータセットを構築
          li 非拘束（in the wild）の環境にてDensePoseを学習。
    .item4
      h1 自由記述欄
      .text
        ul
          li リアルが完全に崩壊した。（Face2Faceの全身モデル版が実現可能になった？）
          li CG/UIの分野との親和性がより高くなった
          li: a(href="https://arxiv.org/abs/1802.00434" target="blank") DensePose: Dense Human Pose Estimation In The Wild
          li: a(href="https://arxiv.org/abs/1612.01202" target="blank") DenseReg
          li: a(href="http://smpl.is.tue.mpg.de" target="blank") SMPL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka
    
  +slide
    .title What will Happen Next? Forecasting Player Moves in Sports Videos
    .info
      .authors Panna Felsen et al.
      .conference ICCV, 2017.
    .item1
      h1 概要
      .text.
        チームスポーツにおいて次に起こることを予測する研究。2チームに分かれたゴール型スポーツを対象とし、ボールを持つ選手の遷移やファールの有無などの推定を行った．

    .item2
      img(src=figpath+"180303whatwillhappen.PNG")
    .item3
      h1 新規性・結果
      .text
        ul
          li 水球とバスケットボールのデータセットを構築した
          li 画像から選手やボールの位置を上から見た画像に変換する手法を提案した
          li 他のスポーツで学習したものを適用した場合(例：学習→水球　テスト→バスケ)ランダムフォレストの方がニューラルネットより精度が高いことが分かった
    .item4
      h1 自由記述欄
      .text
        ul
          li この論文のようにニューラルネットがうまくいかない例を調べるのは面白そう
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Visual Forecasting by Imitating Dynamics in Natural Sequences
    .info
      .authors Zeng et al.
      .conference ICCV, 2017.
    .item1
      h1 概要
      .text.
        動画シークエンスから未来を予測する研究。フレーム間の遷移モデルを考え，次のフレームや行動を推定する。適用対象はフレームの生成から次のシーンの選択など幅広い。

    .item2
      img(src=figpath+"180303visualforecasting.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li ドメイン知識やhandcrafted特徴無しにinverse reinforcement learningとして学習させる
          li フレーム生成、行動予測、ストーリー予測全てにおいて精度の向上に成功した
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://arxiv.org/abs/1708.05827" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
    
  +slide
    .title A Domain Based Approach to Social Relation Recognition
    .info
      .authors Qianru Sun et al.
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        画像中に写っている人々の関係を推測する研究。社会心理学に基づいた16の関係(親子、友人など)を識別する。それぞれの人物から抽出された特徴を入力とするネットワークにより判定する。

    .item2
      img(src=figpath+"180306relation.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 社会心理学に基づいた理論をコンピュータビジョンに導入した
          li 画像に関係性などのラベルを付けることで、より広い用途で用いることができるデータベースを提案
          li 社会心理学に基づき、セマンティックなアトリビュートを収集した
    .item4
      h1 自由記述欄
      .text
        ul
          li 社会学系の理論をCVに持ってくるのは面白そう
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto

  +slide
    .title Forecasting Interactive Dynamics of Pedestrians with Fictitious Play
    .info
      .authors Ma et al.
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        画像中に写っている人々の歩行ルートを予測する手法。各歩行者に対して歩行モデルを決定し、他の人とぶつからないようによけるなど他者の行動を考慮した上で歩行ルートを決定していく。

    .item2
      img(src=figpath+"180306pedestrians1.png")
      img(src=figpath+"180306pedestrians2.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li ゲーム理論に基づき、他の歩行者の進行方向を予測した上でルートを決定する
          li 年齢などの情報を抽出し、各歩行者の歩行速度などを決定する
          li 既存手法と比べて長期的な予測の精度が向上
    .item4
      h1 自由記述欄
      .text
        ul
          li ゲーム理論の応用は興味深い
          li どれくらいの人数までできるのだろうか？（人ゴミは無理？）
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title DeepNav: Learning to Navigate Large Cities
    .info
      .authors Brahmbhatt and Hays
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        目的地までのルートを推測する研究。ストリートビューの画像から、どの方向に進めば銀行やガソリンスタンドなどの目的地に近付けるかを決定していく。ネットワークとしては、目的地までの距離、最も最短となる方角、2枚の画像のどちらが目的地に近いかの3種類を提案。

    .item2
      img(src=figpath+"180306Deepnav.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li アメリカ10都市を対象にストリートビューのデータセットを構築
          li 3種類のCNNネットワークを構築し，hand-crafted特徴及びSVRベースの手法より精度が向上した
          li ラベル付けを効率化するメカニズムを提案した
    .item4
      h1 自由記述欄
      .text
        ul
          li それぞれの目的地に対してどのような特徴を持った方向が選ばれているのか気になった
          li 場合によっては同じ場所を何度も回るだけになってしまう？
          li: a(href="https://arxiv.org/abs/1701.09135" target="blank") 論文URL
          li: a(href="http://mcdonalds.csail.mit.edu/" target="blank") 比較論文
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Forecasting Human Dynamics from Static Images
    .info
      .authors Chao et al.
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        1枚の画像から、人間のモーションを推定する研究。画像から2次元の姿勢を推定し，その結果を3次元に変換することで出力を得る。学習は3段階に分かれており、2次元姿勢推定部は2次元姿勢データベースを使用して学習をし、3次元姿勢推定部はモーションキャプチャデータを2次元投影することにより学習を行い、最後に全体を通して学習を行う。

    .item2
      img(src=figpath+"180306humandynamics1.png")
      img(src=figpath+"180306humandynamics2.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 従来研究とは異なり、RNNを用いることにより静止画からモーションを推定することを可能とした
          li 推定した2次元の姿勢から3次元の情報を復元するネットワークを提案した
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://arxiv.org/abs/1704.03432" target="blank") 論文URL
          li: a(href="http://www-personal.umich.edu/~ywchao/image-play/" target="blank") プロジェクトページ
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto

  +slide
    .title Toward Geometric Deep SLAM
    .authors Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich
    .conference arXiv
    .paper_id arXiv:1707.07410
    .slide_editor Yoshihiro Fukuhara
    .item1
      .text
        h1 概要
        p 2つのCNNを用いた高速かつ頑強な物体追跡手法を提案. 
          |1つ目のCNN(MagicPoint)で入力画像から特徴点を抽出し,
          |2つ目のCNN(MagicWarp)で抽出された特徴点の位置情報のみから２つの画像間のホモグラフィー行列の推定を行う.
    .item2
      .text
        p
          img(src=`${figpath}20180306--TowardGeometricDeepSLAM.png`,alt="20180306--TowardGeometricDeepSLAM.png")
    .item3
      .text
        h1 新規性・結果
        p MagicPointは幾何学的に安定した点（物体の角や辺など）のみを抽出するため, ノイズに頑強である. 
          |また, MagicWarpを用いることで従来手法のように特徴量の記述子（descriptor） を計算する必要がなくなるため, 高速な動作が可能となった. 
          |作成した単純形状のデータセット（Synthetic Shapes Dataset）では FAST, Haris, Shi よりも高精度.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/abs/1707.07410") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Pedestrian Travel Time Estimation in Crowded Scenes
    .info
      .authors Yi et al.
      .conference ICCV, 2015.
    .item1
      h1 概要
      .text.
        画像中に写っている群衆が、目的地にたどり着くまでの時間を推測する手法。目的地まで歩行するにあたり、他の歩行者の流れや立ち止まっている人の存在によって歩行ルートは変化する。このように、目的地まで最短距離で向かうことができず人によってルートが変化する状況における歩行時間の推定を行う。

    .item2
      img(src=figpath+"180307pedestrian.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 目的地までの所要時間を統計的に推定する手法を提案
          li 人の流れの妨げになっている場所や異常行動の検出が可能に
          li 個人の移動時間に着目している既存研究と比べ、大衆に着目することで精度が向上
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yi_Pedestrian_Travel_Time_ICCV_2015_paper.pdf" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
    
  +slide
    .title Emotional Filters: Automatic Image Transformation for Inducing Affect
    .info
      .authors Afsheen Rafaqat Ali, Mohsen Ali.
      .conference BMVC, 2017.
    .item1
      h1 概要
      .text.
        入力された感情ヒストグラム(anger, disgust, fear, joy, sadness, surprise, neutral)を想起するように入力画像をカラートランスファーを行う手法を提案。ユーザは参照画像を用意することなく、入力された感情ヒストグラムと画像を元にデータベースから選択される。オブジェクト検出、シーン識別のそれぞれに対して訓練されたCNNに画像を入力することで、それぞれのトップレイヤーから特徴量を抽出。この特徴量と入力感情ヒストグラムを元にデータベースから、参照画像を10枚選択し、Poulのアルゴリズムを元にカラートランスファーを行う。

    .item2
      img(src=figpath+"bmvc2017_emotional_filters.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li ユーザスタディの結果、多くの画像で入力感情を想起するようにカラートランスファーを行うことができた。
          li 失敗例1 joy成分が強いヒストグラムを入力したところ、生成画像のjoy成分が強く感じたユーザは少なかった。
          li 失敗例2 画像が高次元なコンテキストを含んでいる場合(泣いている少女など)には良い結果は得られなかった。
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://arxiv.org/pdf/1707.08148.pdf" target="blank") 論文URL
          li: a(href="http://im.itu.edu.pk/affective-image-transfer/" target="blank") Project
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kazuki Inoue

  +slide
    .title A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions Supplementary Material
    .info
      .authors Kuan-Chuan Peng, Tsuhan Chen, Amir Sadovnik, Andrew Gallagher.
      .conference CVPR, 2015.
    .item1
      h1 概要
      .text.
        一人の人間は一枚の画像に対して様々な感情を抱くことから、一枚の画像に対する感情のアノテーションをある一つの感情ラベルにするのではなく、ヒストグラム(各成分はanger, disgust, fear, joy, sadness, surprise, neutral)として扱い、データセットを構築。このデータセットに対して、感情ヒストグラムを推定するCNNRというモデルを提案。また、ターゲットとなる感情ヒストグラムをもつ参照画像を用いて、生成画像がターゲット感情ヒストグラムに近くなるようなカラートランスファーを提案。
    .item2
      img(src=figpath+"cvpr2015_a_mixed_bag.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 画像1980枚に対して感情ヒストグラム(各7つの成分は概要参照)と感情を表すVA値がアノテーションされているEmotion6データセットデータベースを構築
          li 入力を画像とし、感情ヒストグラムをSVRとCNNよりも精度が高いCNNRを提案
          li 入力画像のもつ感情ヒストグラムに対して、ターゲット感情ヒストグラムに近くなるようなカラートランスファーを提案。
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Peng_A_Mixed_Bag_2015_CVPR_paper.pdf" target="blank") 論文URL
          li: a(href="http://chenlab.ece.cornell.edu/Publication/Kuan-Chuan/CVPR15_emotion_supp.pdf" target="blank") Supplementary
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kazuki Inoue

  +slide
    .title Predicting Actions from Static Scenes
    .info
      .authors Tuan-Hung Vu et al.
      .conference ECCV, 2014.
    .item1
      h1 概要
      .text.
        画像に対して、キッチンで料理するなどのようにそのシーン(場所)にふさわしいアクションを検出する研究。クラウドソーシングにより提示した画像に対する適切なアクションを答えてもらうことにより、シーンに対するアクションのラベル付けを行った。学習にはSVMを使用して画像に対するアクションの推定を行った。また、位置情報を持った画像を使用することによりXをするのに適切な場所を探すといった応用が可能となった。

    .item2
      img(src=figpath+"180310action.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li クラウドソーシングにより各画像に対するアクションのアノテーションを行った
          li アノテーション結果を分析することにより、人間はシーンからアクションが推測可能であることを示した
          li 位置情報と組み合わせることで、パリで泳ぐにはどこが適切か？などといった目的の活動ができる場所を探すことが可能に
    .item4
      h1 自由記述欄
      .text
        ul
          li 手法自体には新規性が全くなく、データの構築及び分析がメインの論文
          li: a(href="http://www.di.ens.fr/willow/research/actionsfromscenes/" target="blank") プロジェクトページ
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Predicting Object Dynamics in Scenes
    .info
      .authors David F. Fouhey et al.
      .conference CVPR, 2014.
    .item1
      h1 概要
      .text.
        画像に写っている物が、どのように移動していくかを予測する研究。接触している物体同士(例：人間と帽子)は一緒に移動するなど、周囲との相互作用などを考慮して過去のフレームからの変化を確率モデルとして考える。

    .item2
      img(src=figpath+"180310dynamics.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li CRFを用いて前のフレームからの遷移を定式化した
          li CG画像、自然画像どちらに対しても適用が可能
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="http://larryzitnick.org/publication/fouhey_zitnick_dynamics.pdf" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Event Detection using Multi-Level Relevance Labels and Multiple Features
    .info
      .authors Zhongwen Xu et al.
      .conference CVPR, 2014.
    .item1
      h1 概要
      .text.
        部分的に関連するexemplarを用いてイベント検出する手法の提案。例えば、”乗り物のタイヤを交換する”というイベントに対して、”車を運転する”というイベントはタイヤを交換という要素は含んでいるが乗り物という要素を含んでいる。このように部分的に関連する動画を探し重みづけすることによりイベント検出を行う。

    .item2
      img(src=figpath+"180311event.png")
    .item3
      h1 新規性・結果
      .text.
        対象となるイベントに対して，候補となる動画を列挙する。それらの候補に対して，関連度合いにより重みつけをする手法を提案。
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="http://zhongwen.ai/pdf/multi-level.pdf" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Some like it hot - visual guidance for preference prediction
    .info
      .authors Rasmus Rothe et al.
      .conference CVPR, 2016.
    .item1
      h1 概要
      .text.
        初対面の人物に対してどのような第一印象を抱くかを推定する手法。CNNによる抽出した特徴量を利用して、年齢や性別などのattributeを抽出する。また、抽出した特徴や過去のレーティングを基に、特定の個人が異性に対して良い印象と悪い印象どちらを第一印象で抱くかを推定する。

    .item2
      img(src=figpath+"180311like.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 個人の好みに基づいて印象の良し悪しを推定する
          li 同じフレームワークを用いて映画のレーティングも可能
          li デートサイトにおけるレーティングを用いて76%の精度で予測に成功した
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://arxiv.org/pdf/1510.07867.pdf" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Understanding Pedestrian Behaviors from Stationary Crowd Groups
    .info
      .authors Yi et al.
      .conference CVPR, 2015.
    .item1
      h1 概要
      .text.
        写っている人物の歩行ルートなどを検出する研究。動画のフレームから歩行が可能であるかのエネルギーマップを推定することにより目的地までのルートを推定する。エネルギーマップの作成には、噴水などの元々通行が不可能な箇所だけでなく、他の人の情報も考慮し、歩行者ごとに作成する。

    .item2
      img(src=figpath+"180312pedestrian.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 立ち止まっている人による影響を考慮した歩行のモデル化を行った
          li 12000人の歩行者の歩行ルートをannotationしたデータセットを構築
          li 歩行ルートや目的地の推定など、様々な歩行に関連した情報の抽出が可能に
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yi_Pedestrian_Travel_Time_ICCV_2015_paper.pdf" target="blank") 同じ著者による関連研究
          li: a(href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Yi_Understanding_Pedestrian_Behaviors_2015_CVPR_paper.pdf" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Let there be Color!: Joint End-to-end Learning of Global and Loacal Image Priors for Automatic Image Colorization with Simultaneous Classification
    .info
      .authors Satoshi Iizuka et al.
      .conference SIGGRAPH, 2016.
    .item1
      h1 概要
      .text.
        白黒画像からカラー画像を推定する手法。従来手法とは異なりend-to-endのネットワークを構築した。色付けする際，シーンの時間帯などの情報を含む大域的な特徴と物体などの情報を含んだ局所的な特徴の2つを組み合わせるこ。これにより，夜のシーンにもかかわらず画像の一部が明るいといったことを防ぐ。

    .item2
      img(src=figpath+"180312colorization.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 局所特徴と大域特徴を組み合わせることにより，画像全体が自然になるようなネットワークを構築した
          li 過去に描かれた白黒画像に色付けすることも可能
          li ベースラインでは70%前後であった自然さが、90%以上に向上した
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup
    .info
      .authors Edgar Simo-Serra et al.
      .conference ACM SIGGRAPH, 2017.
    .item1
      h1 概要
      .text.
        ラフ画の欠損部分を補完して線画にする手法を提案。全層が畳み込み層によって構築されているネットワークを構築し、任意の解像度の入力から線画への変換を可能とする。学習にはラフ画と線画のペアを使用し，重み付きの一致土をロスとして学習をする．学習の際には，データセットの一部をクロップする，トーンを変える，ノイズを乗せるなどによって学習データの数を増やしている。

    .item2
      img(src=figpath+"180318sketch.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 従来手法とは異なり、ユーザーの介入を必要としない全自動の手法を提案した
          li データセットとして68枚の線画とラフ画のペアを作成。その際，精度を向上するために提案ネットワークのようにラフ画→線画ではなく線画→ラフ画という順で作成。
          li ユーザースタディによって既存手法と比べて精度を向上したことを確認
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="http://hi.cs.waseda.ac.jp/~iizuka/projects/inpainting/ja/" target="blank") プロジェクトページ
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Semantic Shape Editing Using Deformation Handles
    .info
      .authors Mehmet Ersin Yumer et al.
      .conference ACM SIGGRAPH, 2015.
    .item1
      h1 概要
      .text.
        物体の3次元形状を，ComfortableやFighterなどより高次な特徴に基づいて変形する手法の提案。クラウドソーシングによって，2つの三次元形状を比較してもらうことでattributeと形状の関係を調べ、どのように編集するかを決定する。

    .item2
      img(src=figpath+"180318shape.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li クラウドソーシングによってattributeとgeometryの対応を定式化し、attributeに基づく変形を可能に
          li ユーザースタディによって、多くのユーザーが編集結果に満足したという結果を得た。
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://dl.acm.org/citation.cfm?id=2766908" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title AttribIt: Content Creation with Semantic Attributes
    .info
      .authors Siddhartha Chaudhuri et al.
      .conference UIST, 2013.
    .item1
      h1 概要
      .text.
        dangerousやscaryのように、言葉によって表現される特徴に基づきデザインを編集する手法。クラウドソーシングにより特徴とデザインの関係を調査し、特徴に基づいてデザインを編集するインターフェースを構築した。

    .item2
      img(src=figpath+"180318attriblt.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 3次元モデリングとwebデザインの2つの事例に対して適用した
          li 編集結果が、目的の特徴を保持しているか、もっともらしいかの2つの観点においてユーザースタディにより提案手法の方が高い評価を得ることができた
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="http://gfx.cs.princeton.edu/pubs/Chaudhuri_2013_ACC/index.php" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Scene Graph Generation by Iterative Message Passing
    .info
      .authors Danfei Xu et al.
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        画像に写っている物体同士の関係を表したscene graphを生成する手法。グラフのnodeが物体、edgeが関係を表す。初めに、入力画像から物体検出によって画像に写っている物体(=node)を検出する。続いて、RNNにより検出された物体同士の関係を決定していく。

    .item2
      img(src=figpath+"20180325scenegraph.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 従来手法ではグラフの各要素を独立して求めていたのに対して、RNNによりiterativeに関係性を決定していくEnd-to-Endのネットワークを構築した。
          li Visual Genome Datasetを基に作成した新たなデータセットを導入し、評価を行った。
          li 従来手法と比較して、aboveやbehindなど多くの物体同士の関係性において精度が大幅に向上した。
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://cs.stanford.edu/~danfei/scene-graph/" target="blank") プロジェクトページ
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title The shape of art history in the eyes of the machine
    .info
      .authors Ahmed Elgammal et al.
      .conference AAAI, 2018.
    .item1
      h1 概要
      .text.
        機械学習による絵画の分類に関する検証。VGGやＲｅｓＮｅｔなどのネットワークを用いて、絵画を分類するタスクを学習し、その中間出力を特徴量として考えることで分析を行っている。得られた特徴量を次元削減しその分布を可視化する、各軸と芸術分野において提唱されている分野との相関関係の算出などを行った。

    .item2
      img(src=figpath+"180325shapeofart.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 物体検出のタスクで学習したネットワークはgaborフィルターのようなものを見ているのに対して、絵画の分類を目的として学習したネットワークはより大域的な特徴を見ていることがわかった。
          li 得られた特徴量を可視化した結果、機械学習による絵画の識別によって絵画の時系列情報が得られていることが判明した。
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://arxiv.org/abs/1801.07729" target="blank") 論文
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Unsupervised Learning of Depth and Ego-Motion from Video
    .info
      .authors Tinghui Zhou et al.
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        教師無し学習によってdepthとカメラの相対位置(R,t)を推定する手法を提案。ネットワークは、1枚画像からのdepth推定と2枚画像からのカメラの相対位置の推定により構築されている。隣接するフレームtとsを考え、推定した情報を用いてsからtへの変換を行う。変換結果が、tと一致するように学習をしていく。

    .item2
      img(src=figpath+"180325depth.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 学習時には2つのネットワークを同時に学習しているが、テスト時にはそれぞれを単独で使用することが可能となる。
          li 従来手法では何かしらの教師データが必要であったのに対して、ラベルを含まない動画のみで学習が可能に。
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://people.eecs.berkeley.edu/%7Etinghuiz/projects/SfMLearner/" target="blank") プロジェクトページ
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title CAN: Creative Adversarial Networks Generating "Art" by Learning About Styles and Deviating from Style Norms
    .info
      .authors Ahmed Elgammal et al.
      .conference ICCC, 2017.
    .item1
      h1 概要
      .text.
        GANを用いてコンピュータに新たな芸術作品を創らせる研究。心理学と美学の知見を取り入れ、これまでの芸術作品と比べると新しいが新しすぎない画像を生成する。過去の作品と比べて新しいか否かは、芸術作品の分類を考えどのクラスにも分類されないものを従来のものとは異なると判定している。それに加え、GANによって芸術である画像を生成するよううに学習している。

    .item2
      img(src=figpath+"180325CAN.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 新しいが新しすぎない絵画を生成するための評価関数を構築した。
          li 評価にはDCGANにより学習したもの及び実際の芸術作品との比較をユーザースタディによって行った。
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://arxiv.org/abs/1706.07068" target="blank") 論文
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Personalized Image Aesthetics
    .info
      .authors Jian Ren et al.
      .conference ICCV, 2017.
    .item1
      h1 概要
      .text.
        個人の好みを考慮した画像の評価システムを提案した。画像のスコア付けに関する研究は数多く存在するが、実際には人によって高い点数をつける画像は大きく異なる。そこで、クラウドソーシングを用いて一般的な画像の評価を行うネットワークに加え、個人の画像の好みを推定することによって好みに応じた画像の評価システムを構築した。個人の好みの推定には、contentとattributeの2つを特徴として抽出して行う。個人の好みの学習には、active learningを導入することによって少ない学習データでも学習を可能にした。

    .item2
      img(src=figpath+"180325personalize.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li クラウドソーシングによってスコア付けしたgeneralな評価用のデータベースと撮影者がスコア付けを行った個人性の学習用のデータベースの2つのデータベースを構築。
          li 個人の好みを直接学習するより、generalな評価と個人の好みを組み合わせた方が精度が高いことを示した
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Ren_Personalized_Image_Aesthetics_ICCV_2017_paper.pdf" target="blank") 論文
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto