extends layouts/_layout

block slides

  +slide
    .title AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection
    .info
      .authors Thanh-Toan Do, Anh Nguyen, Ian Reid, Darwin G. Caldwell, Nikos G. Tsagarakis
      .conference ICRA 2018
    .slide_editor Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 物体検出とアフォーダンス（というよりは機能？）のセグメントを同時に回帰するネットワーク、AffordanceNetに関する論文。ロボットへの把持位置/機能教示を行うことができる。基本的なモデルはMask R-CNNを適用していて、物体検出のためのbboxと物体に対する機能セグメントを正解として学習する。多タスクの誤差関数は物体カテゴリ、座標、機能セグメントの3つに関するものである。
    .item2
      .text
        p
          img(src=`${figpath}180302AffordanceNet.png`,alt="180302AffordanceNet")
    .item3
      .text
        h1 新規性・結果
        p 従来、物体検知と機能推定は別個に学習・認識されていたが、本研究では多タスク学習の枠組みで、単一モデルにてEnd-to-End学習した。IIT-AFF Datasetにて73.35（SoTAは69.62）、UMD Datasetにて79.9（SoTAは77.0）。モデルも公開されており、誰もがAffordanceNetを実装できるようにしている。
    .item4
      .text
        h1 コメント・リンク集
        p 任意のセグメンテーションラベルさえあれば、物体検知とあらゆる高次なセグメンテーションモデルが実現可能となった。
        ul
          li
            a(href="https://arxiv.org/pdf/1709.07326.pdf") 論文
          li
            a(href="https://github.com/nqanh/affordance-net") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 20:38:13


  +slide
    .title Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN
    .info
      .authors Jiyang Gao, Zijian (James) Guo, Zhen Li, Ram Nevatia
      .conference arXiv:1711.07607
    .slide_editor Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 画像識別における知識蒸留（Knowledge Distillation）の内容である。本論文ではある概念（e.g. 動物認識、人工物認識）ごとに教師となる識別器を事前学習しておき、それらの知識を単一の識別器に学習（これをKnowledge Concentrationと呼称）する。いわば複数の先生がある生徒に教えるという流れで学習する。
    .item2
      .text
        p
          img(src=`${figpath}180302KnowledgeConcentration.png`,alt="180302KnowledgeConcentration")
    .item3
      .text
        h1 新規性・結果
        p よくも悪くも、現状のCVはImageNetの1,000カテゴリに頼っているが、これを100倍の100,000カテゴリに増やして学習したらどうか？また、いかにしたら効率よく学習ができるかを検討した。結果はSingle Model（従来のようなCNNによるカテゴリ識別学習）にするよりも複数のスペシャリスト識別器から知識蒸留を行う方が効率よく、精度よく学習ができた。本論文で使用したEFT（Entity-Foto-Tree）データセットはカテゴリ数でImageNetの100倍、JFT-300Mの5倍である。
    .item4
      .text
        h1 コメント・リンク集
        p 読んだだけでGoogleの研究所であることがわかってしまう論文。実行ができるかどうかはさておき、大規模データの扱いやCNN学習の効率化という意味でも精読すべき論文。
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07607.pdf") 論文
          li
            a(href="https://jiyanggao.github.io/") 著者
          li
            a(href="http://codecrafthouse.jp/p/2018/01/knowledge-distillation/") 知識蒸留の参考
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 20:04:42


  +slide
    .title We Are Humor Beings: Understanding and Prediction Visual Humor
    .info
      .authors Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Virginia Tech, TTI-Chicago, Facebook AI Research
    .item1
      h1 概要
      .text
        ul
          li 視覚とユーモアの関係をモデル化(不調和説に基づく)
          li アニメ画像の面白さを推定
          li 画像のオブジェクトと面白さの関連性を推定
          li データセットの作成
          li 抽象的なシーンを使用したユーモアを引き起こすシーンの理解
    .item2
      img(src=figpath+"180205HumorBeings.jpg")
    .item3
      h1 手法1
      .text
        ul
          li 面白さ推定
          li 特徴抽出し，重み付き誤差平均(面白さによって誤差を変える)
          li Abstract Scenesのアニメ画像(VQAの画像とクラウドソーシングで依頼した面白い画像)に5段階評価したデータセット(AVH)
    .item4
      h1 手法2
      .text
        ul
          li 面白い画像・面白くない画像の変換
          li オブジェクトを変更することで，面白い⇔面白くない画像に相互変換
          li AVHから面白いものを選び，クラウドソーシングで依頼した面白くない画像を使い/面白い画像ペアを作成
          li どのオブジェクトが面白さに影響しているか調査
          li 結果：特に人や動物などのオブジェクトが面白さに影響
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Munetaka Minoguchi

  +slide
    .title Vehicle Three-Dimensional Pose and Shape Estimation from Multiple Monocular Vision
    .info
      .authors Wenhao Ding, Shuaijun Li, Guilin Zhang, Xiangyu Lei, Huihuan Qian, Yangsheng Xu
      .conference arXiv:1802.03515
    .slide_editor Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p オーバーラップが少ない複数視点カメラから自動車の3次元姿勢や形状を復元する研究。CNNにより自動車のキーポイントや姿勢/3次元形状を出力する。これら情報をヒントに、カメラ視点を推定する。2D画像上でのキーポイント推定にはconv-de-convを4回繰り返すhourglassアーキテクチャを採用、3次元姿勢や形状の推定にはCross Projection Optimization (CPO)を採用し2D-3Dの投影誤差を最小化した。
    .item2
      .text
        p
          img(src=`${figpath}180302Vehicle3DPose.png`,alt="180302Vehicle3DPose")
    .item3
      .text
        h1 新規性・結果
        p 物体のキーポイント検出においてState-of-the-art。6DoF推定手法 (Pavlakos, ICRA17)では12キーポイントの平均誤差が37.88であったが、提案手法では10.48まで低減した。また、回転/並進誤差も3DVP (Xiang+, CVPR15)では11.18/N/Aであったが、2.87/4.73まで向上させた。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1802.03515.pdf") PDF
          li
            a(href="https://arxiv.org/abs/1703.04670") 6-DoF Object Pose from Semantic Keypoints
          li
            a(href="http://cvgl.stanford.edu/papers/xiang_cvpr15_3dvp.pdf") Data-Driven 3D Voxel Patterns for Object Category Recognition
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 10:15:30


  +slide
    .title Joint Event Detection and Description in Continuous Video Streams
    .info
      .authors Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko
      .conference arXiv:1802.10250
    .slide_editor Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p End-to-Endでイベント検出（行動の時系列セグメント化）とキャプショニングを実行するタスクを提供する。モデルには3D Convolutionや階層的LSTM（two-level hierarchical LSTM）を採用した。基本的には
          a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
          |をベースにして研究を行なっているが、{Controller, Captioner} LSTMの二段階により前の候補のセンテンスやビデオコンテキストを考慮しつつ状態を更新（Controller LSTM）し、候補領域の特徴を参照しつつキャプションを生成（Captioner LSTM）する。時系列候補領域とキャプションはmulti-task学習、End-to-Endで学習される。
    .item2
      .text
        p
          img(src=`${figpath}180302VideoCaption.png`,alt="180302VideoCaption")
    .item3
      .text
        h1 新規性・結果
        p 候補領域生成やキャプショニングの精度を検証した。また、データセットにはActivityNet CaptionsやTACoS Datasetを用いた。候補領域については従来法のDAPが30, multi-scale DAPが38 @AUC (IoU>0.8)に対して提案法であるJEDDi-Netは58.21を記録した。また、キャプショニングについては従来法が{17.95, 4.82, 17.29} (各BLEU1, METEOR, CIDEr)の問題に対して{19.97, 8.58, 19.88}を記録した。
    .item4
      .text
        h1 コメント/リンク集
        p 動画のタスクはカテゴリのみでなく言語やより表現力豊かな認識ができなければいけない時期になって来た？時系列表現にもまだまだ課題が多いので、これからさらに動画認識にチャレンジすべき。また、キャプショニングの問題は感性評価に対する知見や確固たる評価方法が確立されるとさらに面白くなるのではないか。
        ul
          li
            a(href="https://arxiv.org/pdf/1802.10250.pdf") PDF
          li
            a(href="https://www.bu.edu/cs/profiles/kate-saenko/") Kate Saenko
          li
            a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
          li
            a(href="https://github.com/kenshohara/3D-ResNets-PyTorch") 3D Convolution (3D-ResNets-PyTorch)
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 09:36:26

  +slide
    .title Neural Aesthetic Image Reviewer
    .info
      .authors W. Wang, et al.
      .conference arXiv:1802.10240
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p AIが写真の感性評価やコメント生成を行なってくれる。写真とそのコメントが対応づけられた大規模DBであるAVA-Reviews dataset（52,118画像、312,708コメント）を学習することで写真を入力して図の(1)Predictionや(2)Commentsのようなものが得られる。モデルはCNNにより感性評価（Low-/High-Aesthetic category）を、CNN+LSTM（RNN）によりコメント（e.g. Fastastic colors）を出力する。
    .item2
      .text
        p
          img(src=`${figpath}180301NeuralAesthetic.png`,alt="180301NeuralAesthetic")
    .item3
      .text
        h1 新規性・結果
        p (i) 人間のような画像に対する感性評価（image aesthetics）をコンピュータに実装した。(ii)自然言語の出力により人間の高次な認知能力を実現。(iii) 画像-言語の組み合わせによるデータセットAVA-Reviews datasetを新規に構築した。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1802.10240.pdf") PDF
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 08:39:04


  +slide
    .title Interpreting CNN Knowledge via an Explanatory Graph
    .info
      .authors Q. Zhang, et al.
      .conference AAAI 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 深層学習の解釈性に関する論文であり、畳み込み層の特徴マップの応答を外的に解析して対応する反応を可視化。
          |畳み込みの各フィルタが異なる部位（e.g. 馬の耳や目）に反応するので、グラフにより解析して元画像の対象位置にアクセス。

    .item2
      .text
        p
          img(src=`${figpath}180301interpretability.png`,alt="180301interpretability.png")
    .item3
      .text
        h1 新規性・結果
        p Ground-truthなしに各部位に関する解釈性を与えたことが新規性である。
          |図に示すように入力画像に対するパーツごとの解析をフィルタの反応やグラフの解析から可視化することができる。
          |さらに、異なる画像間においても一貫性のある反応を得ることができた。
    .item4
      .text
        h1 自由記述欄
        p 深層学習は教師なしによる解釈性を獲得しているが、まだ反応している部分の可視化や部分ごとの解析が進んでいるにすぎない。
          |さらなる発展のためには、言語的な解釈や人間にわかりやすい加工（イラストとか？）が必要になるのではないだろうか。
    .slide_index #{getSlideIndex()}

  +slide
    .title HemaApp: Noninvasive Blood Screening of Hemoglobin using Smartphone Cameras
    .info
      .authors E. J. Wang et al.,
      .conference Ubicomp 2016
    .slide_editor Kensho Hara

    .item1
      .text
        h1 概要
        p スマホカメラを使って非侵襲なヘモグロビン濃度の測定を実現．
          |血中の酸素飽和度の測定などはこれまでにもあったがヘモグロビン濃度まで測定できているものはなかった．
          |照明条件とRGBの変化からヘモグロビン濃度を推定するためのアルゴリズムを提案．
    .item2
      .text
        p
          img(src=`${figpath}180228_hemaapp.png`,alt="180228_hemaapp.png")
    .item3
      .text
        h1 新規性・結果
        p 特別な装置を使うことなく簡単にスマホカメラでヘモグロビン濃度測定を実現した点．
    .item4
      .text
        h1 自由記述欄
        p システムやユーザスタディの完成度や完全に実現できたときの有用性が評価された？
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.1 10:44:03

  +slide
    .title End-to-end Driving via Conditional Imitation Learning
    .info
      .authors Felipe Codevilla, Matthias Müller, Alexey Dosovitskiy, Antonio López, and Vladlen Koltun,
      .conference arXiv
      .paper_id 1710.02410
    .item1
      h1 概要
      .text.
        自動運転を模倣学習により行う手法を提案。実空間での学習結果をヴァーチャルな空間での自動運転にて再現することができた。RGB画像、計測（e.g. スピード）や命令（e.g. turn right）などからステアリング、アクセル、ブレーキなどのコマンドを出力して自動車を操作する。
    .item2
      img(src=figpath+"180205conditionalimitationlearning.png")
      img(src=figpath+"180205framework_imitation.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 模倣学習による自動運転を実現した。
          li 実空間とシミュレーションベースの転移を行うことにも成功。
    .item4
      h1 リンク集
      .text
        ul
          li 自動運転の学習はシミュレーションベースで完結してしまう可能性がある。
          li メタ学習/模倣学習/強化学習などはCVに徐々に取り入れられてくるはずなので、2018年は学習しておいて損はない？
          li: a(href="https://arxiv.org/pdf/1710.02410.pdf" target="blank") [論文] End-to-end Driving via Conditional Imitation Learning
          li: a(href="https://www.youtube.com/watch?v=cFtnflNe5fM" target="blank") YouTube
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") slide by Hirokatsu Kataoka

  +slide
    .title Open3D: A Modern Library for 3D Data Processing
    .info
      .authors Qian-Yi Zhou et al.,
      .conference arXiv
      .paper_id 1801.09847
    .item1
      h1 概要
      .text
        |3Dデータを取り扱い、迅速な開発を可能にする
        a(href="http://www.open3d.org/" target="blank") Open3D
        |を提供する。Open3DはC++/Pythonをサポート、並列化にも対応しており、クラウドで開発することが可能。
        |点群読み込み-ダウンサンプリング-法線の計算、シーン再構築、3次元可視化などの処理が含まれている。
    .item2
      img(src=figpath+"180205open3d.png")
    .item3
      h1 新規性・結果
      .text
        |3次元画像処理のコミュニティにて有益なオープンソースを提供し、その
        a(href="https://github.com/IntelVCL/Open3D" target="blank") コード
        |も提供されている。
    .item4
      h1 コメント・リンク集
      .text
        ul
          li: a(href="http://www.open3d.org/" target="blank") Project page
          li: a(href="https://arxiv.org/pdf/1801.09847.pdf" target="blank") [論文] Open3D: A Modern Library for 3D Data Processing
          li: a(href="https://github.com/IntelVCL/Open3D" target="blank") Code
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") slide by Hirokatsu Kataoka

  +slide
    .title Hierarchical Variational Autoencoders for Music
    .info
      .authors A. Roberts et al.,
      .conference NIPS WS on Machine Learning for Creativity and Design, 2017.
    .item1
      h1 概要
      .text
        | 音楽を生成するためのHierarchical Variational Autoencoders (VAE) を提案．
        | エンコーダとデコーダがLSTMで構成されているReccurent VAEがベース．
        a(href="https://goo.gl/twGuP2") 結果サンプル
        | 長い音楽（実験では32小節）を単純なLSTMデコーダで生成するのは難しいので，
        | この研究では複数のLSTMを階層的に重ねて，段階的に長くしていくHierarchical VAEを提案．
        | ループメロディの外挿や，メロディの生成，3ピース構成の音楽生成の実験で性能を検討．
        | 結果の音楽やコードは公開されている．
    .item2
      img(src=figpath+"hierarchical_vae.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 階層的なLSTMによるデコーダをVAEによる音楽生成に導入
          li
            a(href="https://goo.gl/twGuP2") 結果サンプル
    .item4
      h1 自由記述欄
      .text
        ul
          li これも長期的な構成を考えて生成することはできていない
          li Future Workにはインタフェースを作るとあるし，1曲まるごと作るというよりは適当にサンプルを出して作曲家のアイデアを促進することを目指しているのかな．
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title Generating the Future with Adversarial Transformers
    .info
      .authors C. Vondrick et al.,
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        未来の動画を予測して生成する手法を提案．
        4フレーム x 64画素 x 64画素のクリップを入力として，その後の16フレームの動画を生成．
        完全に新しいフレームを生成するのは難しいので，入力フレームの変換により未来のフレームを生成するのがポイント．
        論文の主張としては，きれいな動画を作るにはLow-Levelな情報が重要だけど，未来予測のためにはHigh-Levelな理解も必要で，
        その両者を一つのネットワークで一気に学習するのは難しいとしている．
        だから，Low-Levelな情報は元のフレームを変換することで引っ張ってきて，ネットワークはHigh-Levelな特徴抽出に集中させるのが良いとのこと．
        このネットワークの学習はGANベース．
        生成動画の主観評価や可視化，Generatorの特徴を利用した物体認識タスクなどで性能を評価．

    .item2
      img(src=figpath+"generating_future.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 元のフレームからの変換により未来のフレームを生成する手法を提案
          li 未来の動画生成において，敵対的学習により大規模な教師なしデータを利用した学習を実現
          li 直接動画を生成したり，回帰誤差で学習したりする手法よりも良いことを主観評価実験で確認          
    .item4
      h1 自由記述欄
      .text
        ul
          li 入力が4フレームだけだけど，もっと増やすと性能は変わるのか気になる
          li 主観評価で本物と比較すると提案手法が一番嫌われている率が高いのもちょっと気になる
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title DensePose: Dense Human Pose Estimation In The Wild
    .info
      .authors Rıza Alp Guler et al.
    .item1
      h1 概要
      .text.
        身体の表面形状まで考慮したDenseな姿勢推定手法に関する研究。サーフェイスモデルを提供するSMPLタイプとアノテーションベースのMSCOCOタイプを提供。手法はMask RCNN（w/ ResNet-50, ROI-align, Regression）をベースに構築している。

    .item2
      img(src=figpath+"180205densepose.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li DenseReg [Guler,CVPR17]は顔表面の推定に対して、本研究では身体全体の表面やデンスなポイントを回帰。
          li SMPLやDense-COCOのデータセットを構築
          li 非拘束（in the wild）の環境にてDensePoseを学習。
    .item4
      h1 自由記述欄
      .text
        ul
          li リアルが完全に崩壊した。（Face2Faceの全身モデル版が実現可能になった？）
          li CG/UIの分野との親和性がより高くなった
          li: a(href="https://arxiv.org/abs/1802.00434" target="blank") DensePose: Dense Human Pose Estimation In The Wild
          li: a(href="https://arxiv.org/abs/1612.01202" target="blank") DenseReg
          li: a(href="http://smpl.is.tue.mpg.de" target="blank") SMPL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") slide by Hirokatsu Kataoka
    