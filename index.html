<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>cvpaper.challenge</title>
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/white.css">
    <link rel="stylesheet" href="css/layout.css">
    <link rel="stylesheet" href="lib/css/zenburn.css">
  </head>
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
    
  </script>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <div class="paper-abstract">
            <div class="title">ASePPI: Robust Privacy Protection Against De-Anonymization Attacks</div>
            <div class="info">
              <div class="authors">Natacha Ruchaud, Jean-Luc Dugelay</div>
              <div class="conference">CVPR 2017 Workshop on CV-COPS</div>
            </div>
            <div class="slide_editor">Hirokatsu Kataoka</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>監視カメラ映像などにおいて人物領域のde-anonymization（匿名にしていた情報をオープンにされること？）を防ぐための研究。RoIに対して実行することで人物再同定（Person Re-identification）の精度を落とすことに成功している。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180309AsePPI.png" alt="180309AsePPI"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>本提案手法であるAdaptive Scrambling enabling Privacy Protection and Intelligibility (ASePPI)により、匿名性が保たれることが明らかになった。これは、従来法よりも優れている。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Dugelay_ASePPI_Robust_Privacy_CVPR_2017_paper.pdf">論文</a></li>
                  <li><a href="http://www.eurecom.fr/en/publication/5283/detail/aseppi-robust-privacy-protection-against-de-anonymization-attacks">Project</a></li>
                  <li><a href="https://github.com/NatachaRuchaud/ASePPI">GitHub</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#1]</div>
            <div class="timestamp">2018.3.9 18:05:28</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">An Autonomous Dynamic Camera Method for Effective Remote Teleoperation</div>
            <div class="info">
              <div class="authors">Daniel Rakita, Bilge Mutlu and Michael Gleicher</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 325-333</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  遠隔操作ロボットのための，ロボットカメラの自動姿勢決定．作業野を見やすくするカメラ姿勢を自動で決定する．</p>
                <ul>
                  <li>どのような遠隔操作インタフェースでもOK</li>
                  <li>操作者の操作を予測することで実現
                    <ul>
                      <li>遮蔽のない視野</li>
                      <li>作業部とカメラ間の距離を適切に保つ</li>
                    </ul>
                  </li>
                  <li>方向をわからなくさせないように自動調整</li>
                </ul>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/An_Autonomous_Dynamic_Camera_Method_for_Effective_Remote_Teleoperation_Figure1.png" alt="Figure1"></p>
                <p>
                  (a) 経路のチェックポイントを見せる(b) 経路を矢印で書く
                  (c) ARエージェントが移動方向を見てる
                  (d) ユーザに対するロボットの位置を示唆する
                </p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>外科手術などに実用性ありそうでいい．</p>
                <p>
                  適切なロボット動作のダイナミクスを定義して実装し，ユーザ評価もちゃんと行っている．</p>
                <p>BestPaper Nominee．ところで被験者に1時間拘束で10ドル払ってるらしい．</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171279">ACM Library</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#2]</div>
            <div class="timestamp">2018.3.9 16:21:51</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Blur vs. Block: Investigating the Effectiveness of Privacy-Enhancing Obfuscation for Images</div>
            <div class="info">
              <div class="authors">Yifang Li, Nishant Vishwamitra, Bart P. Knijnenburg, Hongxin Hu, Kelly Caine</div>
              <div class="conference">CVPR 2017 Workshop on CV-COPS</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>他人に情報が拡散しないよう、「ブラー」と「ブロック」というふたつの（人間に対する）難読化法を検証。53名のユーザについて画像の満足度、情報量などの側面から調査した。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180309BlurBlock.png" alt="180309BlurBlock"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>
                  結果から、ブロック（blocking）の方がブラー（blurring）よりも特定されにくいということが判明した。しかし、画像の質やSNSなどに投稿するための満足度（e.g. satisfaction, enjoyment, social presence, likability）としては欠落してしまう。将来的にはユーザのプライバシーを保護するための手法が求められる。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Caine_Blur_vs._Block_CVPR_2017_paper.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#3]</div>
            <div class="timestamp">2018.3.9 16:07:03</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Communicating Robot Motion Intent with Augmented Reality</div>
            <div class="info">
              <div class="authors">Michael Walker, Hooman Hedayati, Jennifer Lee and Daniel Szafir</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 316-324</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  ロボットの移動意思をARで伝える方法について，図の4種類の方法を実装して比較してみた．</p>
                <p>
                  ロボットの身体の方向のみに関連した情報提示がベースラインよりも作業効率を顕著に向上させた．
                  また，ロボットとの共同作業感と動きのわかりやすさの間にトレードオフが発生することも
                  分かった．
                </p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/Communicating_Robot_Motion_Intent_with_Augmented_Reality_Figure1.png" alt="Figure1"></p>
                <p>
                  (a) 経路のチェックポイントを見せる(b) 経路を矢印で書く
                  (c) ARエージェントが移動方向を見てる
                  (d) ユーザに対するロボットの位置を示唆する
                </p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>
                  移動方向の表示に関する研究は継続的に行われているが，AR上での表示における調査をちゃんと（網羅的に）行っていることと，
                  結果が面白い．
                </p>
                <p>BestPaper Nominee.</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171253">ACM Library</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#4]</div>
            <div class="timestamp">2018.3.9 16:01:10</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Planning with Trust for Human-Robot Collaboration</div>
            <div class="info">
              <div class="authors">Min Chen, Stefanos Nikolaidis, Harold Soh, David Hsu and Siddhartha Srinivasa</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 307-315</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  ロボットの意思決定に，人間とロボットの共同作業の信頼度を組み込んだ．手法的には，部分観測可能マルコフ決定過程(POMDP)の機械学習に信頼度をパラメータとして混ぜた．
                  それにより,ロボットは
                  （１）人からの信頼度を推定
                  （２）人の行動における自分の行動の影響の理由付け
                  （３）長期的にみたチームパフォーマンス最大化可能な行動の選択
                  が可能に．
                </p>
                <p>
                  実際にパフォーマンスを高められることを確認した．なお，信頼度を最大化してもパフォーマンスは改善しなかった．</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/Planning_with_Trust_for_Human-Robot_Collaboration_Figure1.png" alt="Planning_with_Trust_for_Human-Robot_Collaboration_Figure1.png"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>
                  信頼度という観点が面白く，理論モデルに基づく実装までこぎつけているのがよい．信頼度最大化がパフォーマンスを改善しないことも面白い．</p>
                <p>BestPaper Nominee.</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171264">ACM Library</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#5]</div>
            <div class="timestamp">2018.3.9 15:34:59</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">From Dusk till Dawn: Modeling in the Dark</div>
            <div class="info">
              <div class="authors">F. Radenovic et al.,</div>
              <div class="conference">CVPR2016</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                画像データベースからの3次元復元の研究．
                昼のデータと夜のデータを混ぜて扱うと，輝度などの違いが大きくマッチングが上手くいかずに失敗する場合がある．
                そのため，昼と夜のデータをクラスタリングにより分割し，それぞれで3D Modelを作った後に統合する．
                昼と夜でははっきり見える部分が違うので，統合することで両者が相補的に働き，
                より高精細な3次元復元ができることを示している．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180307_inthedark.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>昼と夜の画像を自動的に分けるための手法を提案</li>
                  <li>昼と夜を分けてモデリングし，その後統合することの優位性を示した</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>かっこいい論文タイトル</li>
                  <li>夜のデータに注目してちゃんとやってる論文は意外と珍しい気がする</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#6]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Multiple Instance Learning for Soft Bags via Top Instances</div>
            <div class="info">
              <div class="authors">W. Li et al.,</div>
              <div class="conference">CVPR 2015</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                Multiple Instance Learningにおいて，ラベルノイズも考慮した手法を提案．
                例えば画像認識において，画像中に対象が含まれていてもメインの被写体でなければNegativeとされることが問題と指摘．
                提案手法は明確にPositive Bag, Negative Bagを分ける (Hard Bag) のではなく，Softに両者を分ける．
                これにより，ノイズの影響を抑制し，精度を向上させることができている．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180307_milsoftbag.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>MILにSoft Bagという概念を導入</li>
                  <li>実験的にSoft Bagによる精度向上を確認</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>理論的にかっちり実装されている印象</li>
                  <li>Deepじゃない論文</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#7]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Robust Loss Functions under Label Noise for Deep Neural Networks</div>
            <div class="info">
              <div class="authors">A. Ghosh et al.,</div>
              <div class="conference">AAAI 2017</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                ラベルノイズに頑健な損失関数を提案．
                理論的に，Cross entropyやMean Square ErrorよりもMean Absolute Error (MAE)の方がラベルノイズに対して頑健であることを証明し，実験的にもその優位性を確認．
                しかし，MAEによる学習は遅いので，MAEを使う場合に適した最適化手法の検討を行う必要あり．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180307_robustloss.png" alt="180307_robustloss.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>従来研究だと2クラス識別の問題設定での検討が多かったが，この研究では多クラス識別での解析を行っている．</li>
                  <li>MAEを用いることとラベルノイズに頑健であることを示した．</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                MNISTなどの実験で主張通りの結果になっていることを確認しているが，
                もっとデータセットが実世界のものに近づいていっても同じ結果が出てくるのか気になるところ．
              </div>
            </div>
            <div class="slide_index">[#8]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Indirect Match Highlights Detection with Deep Convolutional Neural Networks</div>
            <div class="info">
              <div class="authors">M. Godi et al.,</div>
              <div class="conference">ICIPA Workshop (Social Signal Processing and Beyond), 2017</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                スポーツのハイライトシーンの検出をフィールドを撮影した動画からではなく観客の動画から間接的に行おうという研究．
                観客動画を3D CNNに入力してハイライトの尤度を推定する手法を提案．
                観客動画をCropして入力することで，どの辺りの観客が盛り上がっているかの推定も可能．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180307_indirectmatch.png" alt="180307_indirectmatch.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>観客動画のみに基づいてハイライトシーンを検出</li>
                  <li>プレイシーンを一切見ずに検出可能</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                盛り上がっているシーンをハイライトとすると，観客が盛り上がっているのを見ることが果たして間接的なのかどうか難しい．
                ある意味直接的な認識である気も．
              </div>
            </div>
            <div class="slide_index">[#9]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Combining Image Regions and Human Activity for Indirect Object Recognition in Indoor Wide-Angle Views</div>
            <div class="info">
              <div class="authors">P. Peursum et al.,</div>
              <div class="conference">ICCV 2005</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                人の行動から間接的に物体を検出する手法を提案．
                物体がはっきり撮影できていなくても，それを使う（インタラクションする）人の行動が見えていれば，
                どのような物体を使っているかは推定できるので，それを元にして物体の検出をする．
                ベイジアンネットワークをベースにしてこのアイデアを実装．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180307_combining_1.png" alt="180307_combining_1.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>人物行動に基いて物体のアピアランスに関わらず物体を認識可能</li>
                </ul>
              </div>
            </div>
            <div class="item4"><img src="slides/figs/180307_combining_2.png" alt="180307_combining_2.png"></div>
            <div class="slide_index">[#10]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">"Thank You for Sharing that Interesting Fact!": Effects of Capability and Context on Indirect Speech Act Use in Task-Based Human-Robot Dialogue</div>
            <div class="info">
              <div class="authors">Tom Williams, Daria Thames, Julia Novakoff and Matthias Scheutz</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 298-306</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  間接言語行為（ISA）の有無によって，タスクベースの人間-ロボット間対話においてどれほど役に立つのか，ISAの理解能力なしにロボットはどれだけ機能するか調査した．</p>
                <p>
                  WoZによる実験をしてみた．各条件について，人によるISAの使われっぷりを見る．</p>
                <ul>
                  <li>慣例的社会規範(conventionalized social norms，慣習？)の有無</li>
                  <li>ロボットISAの理解能力の有無の条件において，ロボットのISAの使用と知覚の両方について分析</li>
                </ul>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <h1>結果</h1>
                <ol>
                  <li>タスクベース人間ロボット対話において，ISAが理解できないことをちゃんと示してあっても，人はISAを普通に使う．</li>
                  <li>慣例的社会規範があった場合，ISA使用は更に普通．</li>
                  <li>ロボットのISAのできなさは，ロボットのタスク効率と人間のロボットの知覚の両方において悪影響がある．</li>
                </ol>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>
                  人がロボットに対してどう非言語行動をとるかについてはまだまだ未調査の部分が多いが，そのうちの一つ，ISAについての道を示した重要な論文．</p>
                <p>BestPaper Nominee.</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171246">ACM Library</a></li>
                </ul>
                <p>
                  ※注　ISAとは，発話しながらも間接的に意味を伝える行為．「車に乗れ」「空手の稽古があるの」＞間接的に乗車を拒否．</p>
              </div>
            </div>
            <div class="slide_index">[#11]</div>
            <div class="timestamp">2018.3.9 15:11:52</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Social Robots for Engagement in Rehabilitative Therapies: Design Implications from a Study with Therapists</div>
            <div class="info">
              <div class="authors">Katie Winkle, Praminda Caleb-Solly, Ailie Turton and Paul Bremner</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 289-297</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  理学療法，スポーツリハビリ等の各種セラピーに支援ロボットを導入した場合にどうインタラクションしたらよいかについて，セラピストへのヒアリングを基に論じる．</p>
                <p>ロボットのセラピーへの従事の利益があるという我々の仮説を裏付けるため，まず下の二つを聞いてみた．</p>
                <ol>
                  <li>セラピーにおいて，自己訓練の重要な点とは？典型的な支援方法は？</li>
                  <li>リハビリ療法支援において，どのようにロボットが役に立つと思われる？</li>
                </ol>
                <p>さらに，有効なHRI戦略を導くため，聞いてみた．</p>
                <ol start="3">
                  <li>どのように作業を評価する？</li>
                  <li>患者の作業に影響を及ぼす，セラピストの役割とは？</li>
                  <li>患者それぞれにロボットの行動を仕立てるための方法論？</li>
                </ol>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <h1>結果</h1>
                <p>
                  ロボットは，患者の自律的エクササイズへの意識の低さに対して支援を行える．スマホなどの既存手法よりも先回り的な支援を行える．</p>
                <p>この結果を踏まえ，HRI戦略のデザインの方法論を示す.</p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>聞き取り調査の結果を論文にしたいならこの論文を読むのがよい．</p>
                <p>BestPaper Nominee.</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171273">ACM Library</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#12]</div>
            <div class="timestamp">2018.3.9 14:34:36</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Guiding the Long-Short Term Memory model for Image Caption Generation</div>
            <div class="info">
              <div class="authors">Xu Jia, Efstratios Gavves, Basura Fernando, Tinne Tuytelaars</div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>LSTMの拡張するgLSTMの提案。画像から抽出された意味情報をLSTMの各ユニットに入力として追加し、モデルと画像コンテンツを密接に結合させる。また、短い文に偏らないように、ビーム探索時に正規化。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180309gLSTM.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・手法</h1>
                <p>LSTMは畳み込みで得られた画像情報から単語ごとに文章を生成する。しかし、長い分の場合には、プロセスが継続するにつれて画像情報が薄くなる。これは、シーケンスの最初に出力された単語も同様。そこで、ゲートとセル状態の計算にグローバルな意味情報を追加。意味情報は、画像とその説明から抽出し、単語列生成の過程でガイドとして使用する。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>結果・リンク集</h1>
                <p>Flickr8K、Flickr30K、MS COCOなどのデータセットで、現在(2017)の最先端技術と同等またはそれ以上の精度。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1509.04942.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#13]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Cartooning for Enhanced Privacy in Lifelogging and Streaming Videos</div>
            <div class="info">
              <div class="authors">Eman T. Hassan, Rakibul Hasan, Patrick Shaffer, David Crandall, Apu Kapadia</div>
              <div class="conference">CVPR 2017 Workshop on CV-COPS</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>一人称視点カメラによるライフログをアニメ調に変換することでプライバシー性を高める研究。セグメンテーションとブレンディングのみならず、エッジ強調、さらには物体（e.g. テレビ、本）をクリップアートに置き換えることでより理解しやすくプライバシーを保護するアニメ調に変換。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180309CartooningFPV.png" alt="180309CartooningFPV"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>AMTによりユーザスタディも行なった結果、プライバシーを保ちつつ視覚的にも理解しやすい（e.g. 行動認識）動画ストリーミングを流すことに成功した。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Kapadia_Cartooning_for_Enhanced_CVPR_2017_paper.pdf">論文</a></li>
                  <li><a href="http://homes.soic.indiana.edu/emhassan/pages/Projects.html#cvcops">Project</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#14]</div>
            <div class="timestamp">2018.3.9 13:45:20</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Protecting Visual Secrets Using Adversarial Nets</div>
            <div class="info">
              <div class="authors">Nisarg Raval, Ashwin Machanavajjhala, Landon P. Cox</div>
              <div class="conference">CVPR 2017 Workshop on CV-COPS</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>入力画像に対して非読性を高めるため、ノイズを付与して内容がわからないように加工する（入力と出力は図を参照）。同タスクに対してGenerative Adversarial Networks (GANs)の枠組みを導入した。提案法をAdversarial Pertubation Mechanismと名付け、攻撃ネットワーク（A）と攻撃者を欺く難読化ネットワーク（O）の敵対的学習により学習を進める。学習においてプライバシーとユーティリティ（オープン化）のトレードオフはパラメータにより調整可能である。基本的な構造はDCGANに基づいていて、OはDenoising AutoEncoder。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180309VisualSecrets.png" alt="180309VisualSecrets"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>攻撃は画像中にQRコードが埋め込まれているかどうかで決まり、いかに敵対的ネットがQRコードの位置を検出できるかどうかで評価する。精度は75%（エラー率25%）となった。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Cox_Protecting_Visual_Secrets_CVPR_2017_paper.pdf">論文</a></li>
                  <li><a href="https://users.cs.duke.edu/~nisarg/">著者</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#15]</div>
            <div class="timestamp">2018.3.9 13:19:40</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">I Know That Person: Generative Full Body and Face De-Identification of People in Images</div>
            <div class="info">
              <div class="authors">Karla Brkic, Ivan Sikiric, Tomislav Hrkac, Zoran Kalafatic</div>
              <div class="conference">CVPR 2017 Workshop on CV-COPS</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>人物のセグメンテーションにより人物の検索性を低くする研究（Person De-identification）。服装レベルでのセグメンテーションについてもうまくいっている。本論文で提供するモデルは顔認識のみでなく服装や髪型といった特徴についても非読性を向上させる。手法はGANを参考に構築されており、（物体検出も組み合わせつつ）セグメンテーションを実行する。広義には人物を中心とした背景差分を行なっている。さらに、DCGANにより予め顔画像を学習する。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180309DeIdentification.png" alt="180309DeIdentification"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>Clothing Co-Parsing (CCP)のファッションアイテムのセグメンテーション、Human3.6M datasetの背景マスクを正解として学習を行なった。結果の例は図に示すとおりである。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Kalafatic_I_Know_That_CVPR_2017_paper.pdf">論文</a></li>
                  <li><a href="http://vision.soic.indiana.edu/bright-and-dark-workshop-2017/">ワークショップ</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#16]</div>
            <div class="timestamp">2018.3.9 12:31:03</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Fribo: A Social Networking Robot for Increasing Social Connectedness through Sharing Daily Home Activities from Living Noise Data</div>
            <div class="info">
              <div class="authors">Kwangmin Jeong, Jihyun Sung, Hae-Sung Lee, Aram Kim, Hyemi Kim, Chanmi Park, Yuin Jeong, JeeHang Lee and Jinwoo Kim</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 114-122</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  生活雑音で活動量を測り，活動量を共有することのできるソーシャルネットワークなロボットを提案．生活雑音を取るだけならプライバシーに配慮できて良いし，多対多でもいい感じに働く．
                  友人間ソーシャルコミュニケーションの実験してみて，プライバシー侵害を感じずに繋がってる感が出ることを確認した．
                  ついでにちゃんとしたものも作った．
                </p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/Fribo_A_Social_Networking_Robot_for_Increasing_Social_Connectedness_through_Sharing_Daily_Home_Activities_from_Living_Noise_Data_Figure1.png" alt="Figure1"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>
                  うまくやってる感．これってロボットインタラクションなのかな？</p>
                <p>BestPaper Nominee.</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171254">ACM Library</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#17]</div>
            <div class="timestamp">2018.3.9 11:59:39</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">What is Human-like?: Decomposing Robots' Human-like Appearance Using the Anthropomorphic roBOT (ABOT) Database</div>
            <div class="info">
              <div class="authors">Elizabeth Phillips, Xuan Zhao, Daniel Ullman and Bertram F. Malle</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 105-113</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  著者らが作った，擬人ロボットのコレクションデータベースABOTを活用して，擬人ロボットの見た目について分析する．
                  ABOTは200の現実の擬人ロボットの，画像，パーツリスト，4つの観点
                  （Body-Manipulators, Surface Look, Facial Features, Mechanical Locomotion）
                  におけるスコアを含む．
                </p>
                <p>
                  本研究では，調査のうえ先述の見た目に関する4つの観点を定義し，またロボットの擬人性を推定しやすい特徴について解明した．
                  そのスコアリングシステムはWebで公開する．
                </p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/What_is_Human-like_Decomposing_Robots_Human_like_Appearance_Using_the_Anthropomorphic_roBOT_ABOT_Database_Figure1.png" alt="Figure1"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>盛りだくさん．</p>
                <ol>
                  <li>データベースABOT</li>
                  <li>網羅的に定性的・定量的な評価</li>
                  <li>評価ツールを公開</li>
                  <li>データベースの拡張性</li>
                </ol>
                <p>
                  BestPaper Nominee.ところで被験者に0.5ドル払ったらしい．</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171268">ACM Library</a></li>
                  <li><a href="http://www.abotdatabase.info/">ABOT</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#18]</div>
            <div class="timestamp">2018.3.9 11:38:49</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Characterizing the Design Space of Rendered Robot Faces</div>
            <div class="info">
              <div class="authors">Alisa Kalegina, Grace Schroeder, Aidan Allchin, Keara Berlin, Maya Cakmak</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 96-104</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  “レンダリングされた”ロボット顔のデザインを仕分け．デザイン空間を定義し，分布を調査する．
                  157のロボット顔を76属性のデザイン空間に落とす．
                  文脈に応じてどのようなリアルさ・具体性が好まれるのか，
                  また顔の重要なパーツの有無に対する，適したロボットの作業について論じる．
                </p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/Characterizing_the_Design_Space_of_Rendered_Robot_Faces_Figure1.png" alt="Figure1"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>
                  いろんなロボット顔のサーベイが大変なのは言うまでもないが，ちゃんとシステマチックに論じているところが偉い．
                  まさしくワシントン大的貢献．
                </p>
                <p>BestPaper Nominee.</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171286">ACM Library</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#19]</div>
            <div class="timestamp">2018.3.9 10:47:04</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">From Red Wine to Red Tomato: Composition with Context</div>
            <div class="info">
              <div class="authors">Ishan Misra, Abhinav Gupta, Martial Hebert, The Robotics Institute, Carnegie Mellon University</div>
              <div class="conference">CVPR 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>既知の視覚的概念の分類子を構成するために、コンテクストの依存性に着目した手法を提案。コアアイデアは、複数の単純な概念を組み合わせることによる複雑なコンセプトの開発。赤ワインの赤と、トマトの赤では意味は異なる。形容詞(赤)と物体(ワイン、トマト)の間にはコンテクスト依存性がある。このようなコンテクストを全てビックデータで学習することはナンセンスなため、独立した識別器を合成する。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180309WineTomato.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法</h1>
                <p>学習： primitive（大きい、象）などの組み合わせのセットにアクセス。これらのprimitiveの各々を線形分類器（w）を学習することによってモデル化。これらの分類器を合成し、その組み合わせの分類器を生成する変換ネットワークを学習。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>闇雲にデータ数(クラス数)をあげて学習するのではなく、コンテクストの依存性に着目した予測モデルを使用するのはスマート。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1611.05267.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#20]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Expressing Robot Incapability</div>
            <div class="info">
              <div class="authors">Minae Kwon, Sandy H. Huang and Anca D. Dragan</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 87-95</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  ロボットのできないことを伝える．何をやろうとしてダメで，なぜダメなのかを伝える．
                  動きの軌跡の最適化問題とみなし，成功パターンと失敗パターンの
                  類似性と差の大きさの評価を提案．
                </p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/Expressing_Robot_Incapability_Figure1.png" alt="Figure1"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>
                  運動学を解くという割と確立された（けどちゃんと検討するのは面倒な）手法的な面を水平思考して，ロボットの失敗談を伝えるという面白さにつなげている．</p>
                <p>BestPaper Nominee.</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171276">ACM Library</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#21]</div>
            <div class="timestamp">2018.3.8 20:32:44</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos</div>
            <div class="info">
              <div class="authors">De-An Huang, el al.</div>
              <div class="conference">CVPR 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>解説動画(お料理動画)におけるアクションとエンティティ間をリンクさせる教師なし学習を提案。グラフ表現によって、言語および視覚的モデルを共有することで、ビデオ内の視覚的・言語的曖昧さを回避する。映像中の作業に対して、言語と動画の2つのワークフローを出力し、最適化を施す。WhatsCookinデータセットより、2000の字幕付き動画を使用。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180308VisualLinguistic.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性</h1>
                <p>曖昧さ回避</p>
                <p>図は、動画中の曖昧なアクションとエンティティを示す。(c)の場合、3つ目のフレームで“ドレッシングを加える”とあるが、果たしてどのドレッシングなのか？というあいまいさが生じる。この場合のドレッシングはヨーグルトを混ぜたものになる。つまり、“ドレッシング”をエンティティ、“混ぜる”をアクションとしてこの2つをリンクさせる。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>結果・リンク集</h1>
                <p>過去のシーンを参照することで、動画と文字列の位置合わせに有効。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1703.02521.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#22]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Improving Collocated Robot Teleoperation with Augmented Reality</div>
            <div class="info">
              <div class="authors">Hooman Hedayati, Michael Walker and Daniel Szafir</div>
              <div class="conference">HRI’18</div>
              <div class="paper_id">pp. 78-86</div>
            </div>
            <div class="slide_editor">Ryota Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  操作者と遠隔ロボットが部屋レイアウトを共有する空間において，ロボットの遠隔操作をARで支援する方法論についてプロトタイピングし，議論．飛行ドローンの動きを簡単にわかるようにするには？図(a)視体積の表示，図(b)ロボットの吹き出し的にカメラ映像表示，図(c)端にカメラ映像を固定表示．
                  結果，3方式はカメラ映像をただ見ただけよりも遠隔操作効率が顕著に向上した．
                  また，カメラ映像を見せる方式(b)，(c)はカメラ映像とロボットの注視が分散してしまい，比較的遠隔操作効率が悪かった．
                </p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/Improving_Collocated_Robot_Teleoperation_with_Augmented_Reality_Figure1.png" alt="Figure1"></p>
                <p><img src="slides/figs/Improving_Collocated_Robot_Teleoperation_with_Augmented_Reality_Figure2.png" alt="Figure2"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>評価点</h1>
                <p>
                  特にハッとする面白さは感じないが，各方面がなんとなくやっていたことについて，改めて調査に取り組み，
                  サーベイと実験をちゃんとやって，定量的・定性的評価をちゃんとやった点が評価されたか．
                  実験デザインも特に面白く感じないが，当たり前のことをやってちゃんと結果が出るようなデザインをしている．
                </p>
                <p>BestPaper Nominee.</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3171251">ACM Library</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#23]</div>
            <div class="timestamp">2018.3.8 19:13:34</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Simple Black-Box Adversarial Attacks on Deep Neural Networks</div>
            <div class="info">
              <div class="authors">Nina Narodytska, Shiva Kasiviswanathan</div>
              <div class="conference">CVPR 2017 Workshop on CV-COP</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>ブラックボックスのAdversarial Attacks（敵対的攻撃、摂動ノイズ）を提案する。本論文での画像攻撃は局所的探索により数値的近似を行い、ネットワークの勾配に埋め込むことである。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180308BlackBoxAttacks.png" alt="180308BlackBoxAttacks"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>画像が結果例である。複数の画像識別が誤りを含んでいる。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="http://www.shivakasiviswanathan.com/CVPR17W.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#24]</div>
            <div class="timestamp">2018.3.8 18:27:18</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Deceiving Google's Cloud Video Intelligence API Built for Summarizing Videos</div>
            <div class="info">
              <div class="authors">Hossein Hosseini, Baicen Xiao, Radha Poovendran</div>
              <div class="conference">CVPR 2017 Workshop on CV-COP</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>動画認識タスクにおいて、Google Cloud APIの認識を騙すため動画像に対して意図的に画像挿入を行う攻撃を仕掛ける。攻撃はN秒間に一度、（動画の内容とは全く異なる）任意の画像を埋め込むことで、Google Cloud APIの出力を騙すことに成功した。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180308DeceivingGoogleCloud.png" alt="180308DeceivingGoogleCloud"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>実験の結果、2秒間に一度、画像挿入攻撃を仕掛けると認識誤りを引き起こすことが判明した。動画像は25FPSで構成されるため、50フレームに一度攻撃を仕掛ければ十分であった。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>本論文のような攻撃をYouTubeに埋め込まれると動画タグが自動でつけられなくなるという恐れがある一方で、例えばFacebookなどの動画に意図的かつ人の目にはわからないように画像を埋め込めると（プライバシー保護の面で）外部からは検索しづらくなる。（やはり使い方次第ということか）</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1703.09793.pdf">論文</a></li>
                  <li><a href="http://vision.soic.indiana.edu/bright-and-dark-workshop-2017/">CVPR17-COPS</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#25]</div>
            <div class="timestamp">2018.3.8 18:00:10</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</div>
            <div class="info">
              <div class="authors">Debidatta Dwibedi, Ishan Misra, Martial Hebert</div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>タイトルの通り、Cut, Paste and Learn（物体の切り抜き、任意画像への埋め込みにより物体検出の学習を実行）により自動アノテーションを行い、学習画像を大量に生成することに成功。ここで問題になるのは切り抜いた物体と画像埋め込みの際のアーティファクト（artifact）であり、自然な埋め込みのみならずアーティファクトを無視して学習する手法を提案した。CutのフェーズではFully-Convolutional Networks (FCN)を用いてセグメンテーションを実行するがさらに後処理にて境界線を綺麗にした。PasteのフェーズではGaussian/Poisson Blendingによりアーティファクトをできる限り削減した状態で背景画像に対して埋め込みを行う。データ拡張についても、2次元3次元の回転、オクルージョンなど行う。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180308CutPasteLearn.png" alt="180308CutPasteLearn"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>Blendingにおいては{なし, Gaussian, Poisson}の全てを混ぜる手法が最もよくオリジナル画像のみと比較して8AP向上。データ拡張についても全ての拡張{2D rotation, 3D rotation, Truncation, Occlusion, Distractor}を行う拡張が最も良かった。ベンチマークに対して相対的に21%の向上が見られた。クロスドメインの学習においても10%の向上が見られたと報告。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>セグメンテーション・ブレンディングを用いることで実画像からデータを増やすことができることがわかった。今後はCGのみでなく実画像からのデータ合成も一般的になると考えられる。</p>
                <ul>
                  <li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Dwibedi_Cut_Paste_and_ICCV_2017_paper.pdf">論文</a></li>
                  <li><a href="https://github.com/debidatta/syndata-generation">GitHub</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#26]</div>
            <div class="timestamp">2018.3.8 11:04:00</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Captioning Images with Diverse Objects</div>
            <div class="info">
              <div class="authors">Subhashini Venugopalan, et al. </div>
              <div class="conference">CVPR 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>既存の画像キャプションデータセットには存在しないオブジェクトカテゴリを記述できるNovel Object Captioner (NOC)の提案。物体認識データセットからの画像と、ラベル付けされていないテキストから抽出された外部ソースからの意味的情報を利用。 MSCOCOにはないImageNetのオブジェクトカテゴリのキャプションを生成。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180307CaptioningWithDiverse.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・手法</h1>
                <p>画像とキャプションが対になっていないデータや、様々なソースを使って学習することができる。</p>
                <p>pre-training済みモデルのembedding空間を使用できるようにし、zero-shotなデータでもキャプションを生成できる。</p>
                <p>CNNベースの認識モデル、LSTMベースの言語モデル、キャプションモデルは、別々のソースで同時に学習。しかし、パラメータを共有することで、未知のオブジェクトのキャプションが可能となる。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>結果・リンク集</h1>
                <p>より多くのオブジェクトのキャプション生成が可能かつ、キャプションの質は同等もしくは向上している。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1606.07770.pdf">論文</a></li>
                  <li><a href="https://vsubhashini.github.io/noc.html">ソースコード</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#27]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Neural Scene De-rendering</div>
            <div class="info">
              <div class="authors">Jiajun Wu, et al.</div>
              <div class="conference">CVPR 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>シーンの全体理解。オブジェクトの数とそのカテゴリ、ポーズ、位置などの情報をエンコードし、シーンのコンパクトかつ表現力豊に解釈可能な表現の提案。decoderとencoderにより、XML形式の言語表現を実現。特に、encoderは、Renderingの逆であるDe-renderingを実行することで、入力画像をscene XMLに変換する。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180307NSD.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性</h1>
                <p>従来研究では、encorderとdecoderベースの深層学習を使用した画像表現を提案してたが、その出力は解釈不可能であるかシーン単一のオブジェクトのみの説明である。そこで、シーン全体かつ解釈可能な表現を出力するモデルの提案。</p>
                <p>マインクラフトベースの新しいデータセット。</p>
                <p>de-rendering：入力画像からセグメントを生成し、オブジェクトのプロパティを解釈。推測結果を統合し、元の画像を再構成する。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>単純な全体シーン解釈は進んでいる。これからは、より複雑なシーンの解釈に移る。</p>
                <ul>
                  <li><a href="http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf">論文</a></li>
                  <li><a href="http://nsd.csail.mit.edu/">NSD</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#28]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Visual Diarog</div>
            <div class="info">
              <div class="authors">Abhishek Das, et al. </div>
              <div class="conference">CVPR 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>Visual Diarog：AIエージェントが人間と、画像に関した対話をするタスクを目標とする。エージェントが、画像に対する質問に、対話履歴から文脈を推測し正確に回答する。チャットデータ収集プロトコルを開発し、Visual Dialogデータセット(VisDial)を作成。COCOの120kの画像に10の質問と回答ペアを含む1つのダイアログが含まれており、合計は1.2Mのダイアログ質問回答ペア。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180307VIsualDialog.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・リンク</h1>
                <p>VQAとは異なる、Visual Dialogタスク。</p>
                <p>3つのエンコーダ2つのデコーダからなるVisual Diarogモデル。</p>
                <p>コード、モデル、データセット、ビジュアルチャットボックスを公開中。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1611.08669v5.pdf">論文</a></li>
                  <li><a href="https://visualdialog.org">Visual Dialog</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <p><img src="slides/figs/180307VIsualDialog_2.jpg"></p>
              </div>
            </div>
            <div class="slide_index">[#29]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Learning to Linearize Under Uncertainty  </div>
            <div class="info">
              <div class="authors">Ross Goroshin  et al. </div>
              <div class="conference">in NIPS 2015</div>
            </div>
            <div class="slide_editor">Tomoyuki Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p> </p>「動画は単一画像を表す特徴空間上におけるManifoldとして表すことができる」という考えをもとにしている。その場合、線形な時間変化に対して各フレームを表す特徴量も線形な変位をするのが妥当であり、制約を加えることでそのような特徴空間への埋め込みを学習させる。
                <!--|時間の設定をもとに対応する特徴量を計算することができ、実験では仮想的なデータにおけるフレーム生成タスクにおいて、鮮明な出力が得られている-->
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法</h1>
                <p> </p>t-1, t の埋め込みベクトルzt, zt-1からt+1の埋め込みベクトルzt+1を予測し、それからt+1の画像復元を行うモデルを考えるが、以下の３つの要素を加える。（1）zの時間的変位のcos類似度が近くなるようにする、（２）max-pooling(出力m)とargmax-pooling (出力p)を行い、t+1のz(=(m, p))を求める際は、pを線形外挿により求める、（3）未来の不確実性の対処として、潜在変数δを定義。
                argmax-poolingはソフトな近似関数を定義することで逆伝搬可能にし、δは学習時はサンプルごとにSGDにより最適化し、テスト時はランダムサンプリング。
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/Linearize1.png"><img src="slides/figs/Linearize2.png">
                  <!--img(src=`${figpath}Linearize3.png`)--><img src="slides/figs/Linearize4.png">
                </p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク</h1>
                <p></p>動画を画像表現空間上のManifold捉える視点、逆伝搬不可能な関数をソフトな関数で近似する手法、潜在変数の導入による未来の不確実性への対応が面白く、非常に参考になりそう。
                時間の設定をもとに対応する特徴量を計算することができ、実験では仮想的なデータにおけるフレーム生成タスクにおいて、鮮明な出力が得られているが、
                線形で動く事を前提とした仮想データや今回の特徴空間の制約が実際の識別タスクなどで有効な特徴量かの実験がないなど疑問な点もあった。
                <ul>
                  <li><a href="https://arxiv.org/abs/1506.03011">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#30]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Deep Image Prior </div>
            <div class="info">
              <div class="authors">Dmitry Ulyanov et al. </div>
              <div class="conference">2017</div>
            </div>
            <div class="slide_editor">Tomoyuki Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p> </p>「CNNは理論上任意の関数を近似できるが、その構造自体に汎化性能をあげるようなPriorが含まれている」という考えのもと、ランダム初期化されたCNNを用いて高いレベルの画像復元、ノイズ除去などを行った。
                また、CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかることも示された。深いネットワークの方が復元性能が高かった。
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法</h1>
                <p> </p>ノイズ画像ｚをencoder-decoderモデルに入力して、生成された画像を欠損画像にMSEを近づけるように学習。
                注意点として、完全に学習仕切ってしまうと欠損画像と同じものが出るだけなので、学習をある程度のiterationで止めると、復元された画像が得られる。
                CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかる。
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/prior1.png"><img src="slides/figs/prior2.png"><img src="slides/figs/prior3.png"></p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク</h1>
                <p></p>畳み込み処理×SGDの異常なまでの汎化性能を実験的に裏付けていると思われ非常に面白い。逆にCNNのPriorの苦手なところとして、Adversarial exampleやGANのチェッカーボード現象も関係してそう。
                自然画像と畳み込みとの関連なのでFractal画像とも関係してそう。逆に人工データに対しては苦手とかあるのだろうか。Deformable ConvやTemporal ConvなどのPriorの気になる。
                <ul>
                  <li><a href="https://sites.skoltech.ru/app/data/uploads/sites/25/2017/12/deep_image_prior.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#31]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Catching the Temporal Regions-of-Interest for Video Captioning</div>
            <div class="info">
              <div class="authors">Ziwei Yang, Yahong han, Zheng Wang</div>
              <div class="conference">ACM MM 2017</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>動画キャプションのため、動画中から時系列のRegions-of-Interest（RoI）を獲得する。動画中のアテンションを獲得するDual Memory Recurrent Model（DMRM）を提案して時系列の大域的構造/特徴とRoI特徴を対応づける。これにより、人間のように動画を粗く流し見することに相当するモデルが構築できる。さらに詳細に特徴を評価するため、意味的な教示（semantic supervision）を行う。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180307VideoROI.png" alt="180307VideoROI"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>評価にはMicrosoft Video Description Corpus (MSVD)やMotreal Video Annotation (M-VAD)を採用。動画キャプショニングにおける評価法、BLEU-4, CIDEr, METEORにてState-of-the-artな精度を得た。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>動画キャプショニングは今やると面白い？動画VQAなんかは進んでいるかも？</p>
                <ul>
                  <li><a href="https://dl.acm.org/citation.cfm?id=3123327">論文</a></li>
                  <li><a href="https://ziweiyang.github.io/">Project</a></li>
                  <li><a href="https://github.com/ziweiyang/dualMemoryModel">GitHub</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#32]</div>
            <div class="timestamp">2018.3.7 12:28:51</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Temporal Relational Reasoning in Videos</div>
            <div class="info">
              <div class="authors">Bolei Zhou, Alex Andonian, Antonio Torralba</div>
              <div class="conference">arXiv:1711.08496</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>時系列の理由付け、（物体や人物行動などの）関連性を学習するTemporal Relation Network (TRN)を提案する。TRNはフレーム数を変えながら特徴表現を行い、前後の時系列を対応づけることで理由付けを行う。このネットワークを学習して時系列の対応付けを行うため、3つの動画データベースーSomething-Something（ビデオ数108,499）, Jester（148,092）, Charades（9,848）ーを用いた。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180307TRN.png" alt="180307TRN"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>TRNは場面によりC3DやTwo-Stream ConvNetsよりも高精度。ビジュアルの結果は動画を参照。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>動画像に対しても理由付け（Reasoning）ができるようになってきた。行動検出の高精度化は待たれるが、トリミングされた動画像に対しては効果を発揮する手法。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1711.08496.pdf">論文</a></li>
                  <li><a href="http://relation.csail.mit.edu/">Project</a></li>
                  <li><a href="https://github.com/metalbubble/TRN-pytorch">GitHub</a></li>
                  <li><a href="https://www.youtube.com/watch?v=D42erLb42_k">YouTube</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#33]</div>
            <div class="timestamp">2018.3.7 09:23:28</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Egocentric Basketball Motion Planning from a Single First-Person Image</div>
            <div class="info">
              <div class="authors">Gedas Bertasius, Aaron Chan, Jianbo Shi</div>
              <div class="conference">CVPR 2018</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>一人称視点の画像からゴールリングに到達するまでのバスケットボール選手の動線を生成する。本論文では3D位置や頭部方向も記録する。同タスクを実行するため、まずは画像空間から12Dのカメラ空間に投影を行うEgoCam CNNを学習。次に予測を行うCNN（Future CNN）を構築、さらに予測位置やゴールまでの位置が正確かどうかを検証するGoal Verifier CNNを用いることでより正確な推定を行うことができる。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180307EgoBasketball.png" alt="180307EgoBasketball"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>複数のネットワークの出力（ここではEgoCamCNNとFutureCNN）を検証するVerification Networkという考え方は面白い。他のネットワークの出力を、検証用のネットワークにより正すというのはあらゆる場面で用いることができる。RNN/LSTM/GANsなどよりも高度な推定ができることが判明した。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>結果例は動画像を参照。未来予測・３次元投影などコンポーネントがDNNにより高度にできるようになってきたからできた研究。さらに検証用のネットワークを構築することで出力自体を操作している。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1803.01413v1.pdf">論文</a></li>
                  <li><a href="https://www.youtube.com/watch?v=wRRRl4QsUQg">YouTube</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#34]</div>
            <div class="timestamp">2018.3.7 09:04:15</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Beyond Context: Exploring Semantic Similarity for Tiny Face Detection</div>
            <div class="info">
              <div class="authors">Yue Xi, Jiangbin Zheng, Xiangjian He, Wenjing Jia, Hanhui Li</div>
              <div class="conference">arXiv:1803.01555</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p><a href="https://www.cs.cmu.edu/~peiyunh/tiny/">Finding Tiny Faces</a>を元ネタにして、画像中から微小な顔を検出する手法を提案。元ネタではコンテキストから小さな顔を検出していたが、本論文では画像の類似性（顔は大小に関わらず特徴が類似する）を考慮して極小な顔を検出した。手法としては、画像中から意味的に類似する領域を計算するためのMetric Learning（特徴空間の距離学習）を用いる。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180307BeyondContext.png" alt="180307BeyondContext"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>3つの著名な公開データに対して精度を向上させState-of-the-art（と主張しているが、結果のグラフが18/03/07現在論文に埋め込まれていない）。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1803.01555v1.pdf">論文</a></li>
                  <li><a href="https://www.cs.cmu.edu/~peiyunh/tiny/">Finding Tiny Faces</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#35]</div>
            <div class="timestamp">2018.3.7 08:27:34</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Toward Multimodal Image-to-Image Translation</div>
            <div class="info">
              <div class="authors">Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman</div>
              <div class="conference">NIPS 2017</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>ピクセル同士の画像対応を行い、画像変換を実行するBicycle GANを提案。従来のImage-to-Image (pix2pix)ではone-to-oneマッピングだったが、本提案ではマルチモーダル、すなわちある画像からあらゆるピクセルの対応関係を考慮した変換をおこなう（例として、図に示すような夜画像の入力からあらゆる日中の画像に変換するなど）。このアルゴリズムを構築するためにVAEベースやLatent RegressorのGANを組み合わせる。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180306BicycleGAN.png" alt="180306BicycleGAN"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>pix2pixと比較して複数の結果を出力する表現力が向上した。マルチモーダルで出力しても結果画像が崩れることなく画像生成を実現した。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1711.11586.pdf">論文</a></li>
                  <li><a href="https://junyanz.github.io/BicycleGAN/">Project</a></li>
                  <li><a href="https://github.com/junyanz/BicycleGAN">GitHub</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#36]</div>
            <div class="timestamp">2018.3.6 14:58:57</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Weakly Supervised Affordance Detection</div>
            <div class="info">
              <div class="authors">J. Sawatzky et al.,</div>
              <div class="conference">CVPR 2017</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                物体のパーツごとのAffordanceを推定する問題の研究．
                CAD120データセットにpixel-wiseのAffordanceラベルを付けてデータセットを作成．
                CNNにより入力画像からAffordanceを推定するが，Affordanceはマルチラベル（複数のラベルを持つ画素が存在）なので，
                それに対応できるような拡張手法を提案．
                加えて，キーポイントレベルのアノテーション (Weakly label) からの学習を行う手法も提案．
                Fully supervised, Weakly supervisedの両設定においてSOTAを達成．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180306_weaklyaffordance.png" alt="180306_weaklyaffordance.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>Affordance推定の問題においてKeypointアノテーションから学習する手法を提案</li>
                  <li>Pixel-wiseアノテーション付きで実データに近いAffordanceデータセットを提供</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>せっかくWeakly Supervisedなんだからデータをたくさん用意したらどうなるかの結果とかも見たい</li>
                  <li><a href="https://github.com/ykztawas/Weakly-Supervised-Affordance-Detection">GitHub</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#37]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Action Recognition with Coarse-to-Fine Deep Feature Integration and Asynchronous Fusion</div>
            <div class="info">
              <div class="authors">Weiyao Lin, Yang Mi, Jianxin Wu, Ke Lu, Hongkai Xiong</div>
              <div class="conference">AAAI 2018</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>AAAIに採択された、行動認識の研究。(1)より精緻な特徴量抽出、(2)異なるチャンネルの入力からの非同時性（asynchrony）を考慮して公開データベースに対する認識精度を向上させた。Coarse-, Middle-, Fine-levelの特徴量を統合して識別を実行する、さらにはそれぞれ異なる時間とチャンネル（e.g. rgb at time t & flow at time t+2）からの特徴組み合わせにより参照する尺度を変更し、特徴量をさらに強化した。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180306ActionCoarseFine.png" alt="180306ActionCoarseFine"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>多階層の特徴量の組み合わせや非同時性を考慮した特徴抽出により手法を構成、UCF101にて95.2%、HMDB51にて72.6%を達成した。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1711.07430.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#38]</div>
            <div class="timestamp">2018.3.6 14:31:32</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">MarioQA: Answering Questions by Watching Gameplay Videos</div>
            <div class="info">
              <div class="authors">Jonghwan Mun, et al. </div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>動画によるVideoQA。マリオのプレイ動画から、発生するイベントの質疑応答を行うMarioQAを提案。イベントログを含むビデオクリップを収集し、抽出されたイベントから自動的にQAペアを生成してデータセットを構築。敵を倒す、死ぬ、ジャンプ、キック、持つなどの11個のアクションパラメータを、動画と対応させたコマンド形式で時系列にまとめたものを学習。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180306MarioQA_1.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法・結果・リンク</h1>
                <p>Gated Recurrent Unit (GRU)で質問の特徴抽出。3DFCNでビデオの特徴抽出。2つの特徴から分類。</p>
                <p>NT (case 1), NT+ET (case 2) and NT+ET+HT (case 3)の3ケースについて精度を比較し、時間的推論能力を検証。ETやHTを加えた場合の方が精度が向上することを確認。</p>
                <ul>
                  <li><a href="https://arxiv.org/abs/1612.01669">論文</a></li>
                  <li><a href="http://cvlab.postech.ac.kr/research/MarioQA/">MarioQA</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <p><img src="slides/figs/180306MarioQA_2.jpg"></p>
              </div>
            </div>
            <div class="slide_index">[#39]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Moments in Time Dataset: one million videos for event understanding</div>
            <div class="info">
              <div class="authors">Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, Aude Oliva</div>
              <div class="conference">1801.03150</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>3秒以内のラベル付けされた動画像が100万以上含まれるデータセットMoments in Time Datasetを提案。今まで動画DBでありがちであった人物のみに偏ることなく、物体や動物、自然現象なども積極的に含んでいる。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180305MomentsInTime.png" alt="180305MomentsInTime"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>3秒以内の瞬間的な動画にすることでノイズを含まない動画になりやすく、クラス間/クラス内のDIVERSITYを考慮、人物のみに限定せず動画像を汎用的に収集、動き自体の転移を考慮してカテゴリを定義している。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1801.03150.pdf">論文</a></li>
                  <li><a href="http://moments.csail.mit.edu/">Project</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#40]</div>
            <div class="timestamp">2018.3.5 20:25:53</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</div>
            <div class="info">
              <div class="authors">Peter Anderson, et al.</div>
              <div class="conference">CVPR 2018</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>自然言語のナビゲーションを入力として、実空間の中をロボットが動き目的地に到達できるかどうかを競うベンチマーク（Visually-grounded natural language navigation in real buildings）を提案。データセットは3Dのシミュレータによりキャプチャされ、22Kのナビゲーション、文章の平均単語数は29で構成される。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180305R2RNavi.png" alt="180305R2RNavi"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>(1) Matterport3Dデータセットを強化学習を行えるように拡張。(2) 同タスクが行えるようなベンチマークであるRoom-to-Room (R2R)を提案して言語と視覚情報から実空間にてナビができるようにした。(3) seq-to-seqをベースとしたニューラルネットによりベンチマークを構築。VQAをベースにしていて、ナビゲーション（VQAでいう質問文）と移動アクション（VQAでいう回答）という組み合わせで同問題を解決する。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>自然言語の問題はキャプションや質問回答の枠を超えて実空間、さらにいうとロボットタスクに導入されつつある。この研究はビジョン側からのアプローチだが、ロボット側のアプローチが現在どこまでできているか気になる。すでに屋内環境をある程度自由に移動するロボットが実現しているとこの実現可能性が高くなる。SLAMとの組み合わせももう実行できるレベルにある？</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1711.07280.pdf">論文</a></li>
                  <li><a href="https://bringmeaspoon.org/">Project</a></li>
                  <li><a href="https://github.com/peteanderson80/Matterport3DSimulator">GitHub</a></li>
                  <li><a href="https://niessner.github.io/Matterport/">Matterport3D dataset</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#41]</div>
            <div class="timestamp">2018.3.5 19:53:46</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Joint Object Category and 3D Pose Estimation from 2D Images</div>
            <div class="info">
              <div class="authors">Siddharth Mahendran, Haider Ali, Rene Vidal</div>
              <div class="conference">arXiv:1711.07426</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>「2D画像」と「物体位置」の入力から「3D物体姿勢」と「カテゴリラベル」を出力する研究。ResNetベースのアーキテクチャを採用している。物体カテゴリが既知/未知の場合の両方で3次元物体姿勢の推定ができる。物体の回転とカテゴリ推定の同時誤差を計算する関数も定義。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180305_2D23D.png" alt="180305_2D23D"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>3次元物体姿勢推定とカテゴリ推定の同時回帰問題において、Pascal3D+ datasetでState-of-the-artな精度。物体カテゴリが未知の場合でもカテゴリを推定しながら3次元姿勢推定を実行することができる。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1711.07426.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#42]</div>
            <div class="timestamp">2018.3.5 19:25:04</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Adversarial Attacks Beyond the Image Space</div>
            <div class="info">
              <div class="authors">Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi Keung Tang, Alan L. Yuille</div>
              <div class="conference">arXiv:1711.07183</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>Adversarial Examples（ネットワークを騙す摂動ノイズ）に関する研究だが、特に物体識別や質問対応（Visual Question Answering）への問題を扱う。さらに、従来の問題では2D画像を取り扱っていたが、本論文では3Dレンダリングとその2D平面投影画像に拡張する。ひとつの摂動ノイズは誤差逆伝播のエラーを直接出力の2D空間に投影すること、もうひとつは敵対的ノイズを予め2D画像に構築して物理空間からレンダリングすることである。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180305VQAAttacks.png" alt="180305VQAAttacks"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>ここでは（１）3次元的な物理的空間を想定して摂動ノイズを加えることができるかどうかについて言及、（２）ノイズを含んだ攻撃画像が与えられた際に、それら攻撃から守るような適切な物理空間を構成できるかどうかを検討した。3次元的な物理空間の攻撃は、法線方向・光源・材質などを考慮しつつ出力に対して防衛可能であるため、2次元の画像空間よりも攻撃が難しいと主張。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>画像空間を超えてボリュームデータに対する摂動ノイズが議論され始めた。どんな空間でも埋め込める攻撃や、それらから防衛可能な手法を汎用的に考えてみたい。また、セキュリティ分野の知見はCVにもっと導入されるべき？</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1711.07183.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#43]</div>
            <div class="timestamp">2018.3.5 19:08:53</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Personalized Cinemagraphs using Semantic Understanding and Collaborative Learning</div>
            <div class="info">
              <div class="authors">T. Oh et al.,</div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                Cinemagraph（画像中の一部だけ動画）を自動生成するための手法を提案．
                セマンティックセグメンテーションも導入してシーンの意味的な情報を利用し，
                高品質なCinemagraphの生成を実現する手法とした．
                さらに，動かす対象がたくさんある中でどれを選ぶとよいかをユーザごとの個人的な嗜好を学習することで，
                personalizeされた生成を実現している．
                Stablizeされている動画を入力として，
                セマンティックセグメンテーションの情報を利用したMRFの最適化によりCinemagraphを生成，
                その後学習したuser preferenceのモデルにより候補の中から選択する．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180305personalizedcinemagraph.png" alt="180305personalizedcinemagraph.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>セマンティックセグメンテーションにより意味的な理解をCinemagraph生成に導入</li>
                  <li>個人的な嗜好に沿ったCinemagraphの自動生成を実現</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>ユーザの嗜好を学習するためにデータにスコア付けしてもらうなど，CVよりはMultimediaっぽい論文</li>
                  <li><a href="http://web.mit.edu/taehyun/www/Research/Cinemagraph/SuppleWeb.htm">結果サンプル</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#44]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Multi-Agent Cooperation and the Emergence of (Natural) Language</div>
            <div class="info">
              <div class="authors">Angeliki Lazaridou, et al.</div>
              <div class="conference">ICLP 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>マルチエージェント間の対話による言語学習を提案。SenderエージェントとReceiverエージェント間で簡単な画像当てゲームを実施。ゲームの正解のためにより良質なコミュニケーションが必要となり、言語を学習していく。また、ゲーム環境を変化させることで、単語の意味と画像がより良く対応するようになる。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180305MultiAgent.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法</h1>
                <p>Senderエージェントは、2枚の画像のうち1枚がtargetであると伝えられる。そして、これを伝えるためにReceiverエージェントにsymbol(メッセージ)を送信する。Receiverエージェントは、受信したsymbolの情報のみから、どちらの画像がtargetであるかを当てる。</p>
                <p>SenderとReceiverに見せる画像を変える実験や、人がゲームを実施する実験を行った。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>結果・リンク</h1>
                <p>人間と生産的にコミュニケーションできるAIの開発に貢献できる。言語の習得には、大量のデータだけでなく、他者との対話が重要。また、Senderが出力したsymbol(Image Netのラベルに対応したもの)を人間に見せると68%の正解率となった。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1612.07182.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#45]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Turning an Urban Scene Video into a Cinemagraph</div>
            <div class="info">
              <div class="authors">H. Yan et al.,</div>
              <div class="conference">CVPR 2017</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                Cinemagraph（画像中の一部だけ動画）を自動生成するための手法を提案．
                Warpingして動画中の視点を固定した後，セグメンテーションをかけてからDynamicな領域を見つけて，
                そこだけ動くようにしてCinemagraphを生成．
                街中のシーンで光やディスプレイだけが動くようなCinemagraphを自動的に生成することを可能にした．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180302_turning_cinemagraph.png" alt="180302_turning_cinemagraph.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>街中で普通に撮影した動画から自動的なCinemagraphの生成を実現</li>
                  <li>ノイジーなWarping動画でも有効な動き解析手法を提案</li>
                  <li><a href="https://www.youtube.com/watch?v=r3yyL6qrVX4">サンプル動画</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>特に定量的な評価はなくて，サンプルを出してうまくいっているでしょ，というやり方</li>
                  <li>失敗例を出してLimitationまで議論しているけど，こういうのはCVPRだと珍しい気がする</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#46]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">A Read-Write Memory Network for Movie Story Understanding</div>
            <div class="info">
              <div class="authors">Seil Na, et al.</div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>大規模でマルチモーダルの映画ストーリー理解のためのMovieQA を解く。新しいメモリネットワークモデルのRWMN(ReadWrite Memory Network)を提案。一連のフレームを段階的に抽象化して、より高レベルの順次情報を取得し、それをメモリスロットに格納していく。CNNを多用し、読み取りネットワークと書き込みネットワークを設計。これにより、メモリの読み書き操作に高い容量と柔軟性を持たせることができる。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180304ReadWriteMemory.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法</h1>
                <p>Embedding：ResNetとWord2Vecを用いて映画の埋め込みを行う。</p>
                <p>Write： CNNを書き込みネットワークとして利用し、メモリテンソルを出力。</p>
                <p>Read： CNNを使用して、一連のシーン全体をつなぎ合わせて関連付けるために、シーケンシャルメモリスロットにチャンクごとにアクセス。構成されたメモリMrを得る。</p>
                <p>QA： 5つの候補中から最も信頼度のが高い回答を選ぶ。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>結果・リンク</h1>
                <p>ストーリーのコンテンツだけでなく、キャラクターとその行動についての理由など、より抽象的な情報を理解できる可能性を示唆。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1709.09345.pdf">論文</a></li>
                  <li><a href="https://github.com/seilna/RWMN">ソースコード</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#47]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Sequence to Sequence – Video to Text</div>
            <div class="info">
              <div class="authors">Subhashini Venugopalan, et al</div>
              <div class="conference">ICCV 2015</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>ビデオのキャプションを生成するためのend-to-endかつ、sequence-to-sequenceモデルの提案。本手法のS2VTによって、一連のフレームを一連の単語に直接マッピングし、学習することができる。入力フレームの可変数の扱い、ビデオの時間構造を学習、自然な文法文の生成、この3点が本研究のコントリビューション。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180304VideoToText.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法</h1>
                <p>各フレームのCNNの出力と、連続したLSTMに入力する。また、ビデオの時間構造をモデル化するためにオプティカルフローを算出し、フロー画像もCNNを介してLSTMに入力する。全てのフレームを読み込んだ後に、単語単位で文章を生成する。</p>
                <p>使用データセット：MSVD, M-VAD, MPII Movie Description</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>結果・リンク</h1>
                <p>評価は機械翻訳に使われるMETEORで行う。フレームの順序をランダムにした場合、スコアがかなり低減したことから、時間的構造を利用したキャプションの生成ができていることを示唆。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1505.00487.pdf">論文</a></li>
                  <li><a href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt">ソースコード</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#48]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Deformable Convolutional Networks</div>
            <div class="info">
              <div class="authors">Jifeng Dai, et al.</div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>CNNの表現力の向上を図る。CNNによる物体検出などでは、矩形を用いるために検出対象の物体だけでなく、余計な背景も含んでしまい精度低下につながる。可変可能な畳み込みとRoIプーリングを提案。これにより、画像の畳み込みを行う際に、重みに加えてセルの位置も学習する。特に、物体検出やセマンティックセグメンテーションなどのタスクに効果的。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180304DeformableConvolution_1.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法・結果・リンク</h1>
                <p>変形可能な畳み込み：規則的(矩形)にセルをサンプリングする標準の畳み込みに、オフセットを追加することで、自由形状変形を可能にする。オフセットは追加の畳み込みレイヤを介し、前のfeature mapから学習可能。</p>
                <p>可変可能なRoIプーリング：RoIプーリング時の各binの位置にオフセットを追加する。畳み込みと同様に、前のfeature mapから学習可能。</p>
                <p>様々な条件での実験を実施。どの条件でも提案手法の精度が高い結果となった。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1703.06211.pdf">論文</a></li>
                  <li><a href="https://github.com/msracver/Deformable-ConvNets">ソースコード</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <p><img src="slides/figs/180304DeformableConvolution_2.jpg"></p>
              </div>
            </div>
            <div class="slide_index">[#49]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Image Captioning with Sentiment Terms via Weakly-Supervised Sentiment Dataset</div>
            <div class="info">
              <div class="authors">Andrew Shin, et al.</div>
              <div class="conference">BMVC 2016</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>画像キャプショニングの中でも、画像上にはない形容詞で表現された“感情”についてのキャプションに焦点を当てる。センチメントタームを用いた画像キャプションモデルを提案。これにより、センチメントの主観的性質に対応するマルチラベル学習を実現。FlickrとDeviantArtから、2.5Mの画像と28Mのコメントを収集し，感情に対するデータセットを構築。“コメント”はキャプションとは性質が異なるが、感情を表現するために適している(否定や不適切を除く)。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180304CaptioningWithSentiment.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法</h1>
                <p>CNN→LSTMという一般的な画像キャプションの流れに、センチメント分析を行うCNNを追加する。SentiWordNetの正または負のスコアが0.5以上の単語を感情単語とする。</p>
                <p>SentiWordNet：意見聴衆のための語彙リソース。正、負、客観性の3つの感情スコアを算出。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>結果・リンク</h1>
                <p>キャプションが適切出るかどうかと、キャプションのランク付けの2つの人間による評価。モデルからのキャプションがイメージの感情に関してより適切であるという結果となった。</p>
                <ul>
                  <li><a href="http://www.bmva.org/bmvc/2016/papers/paper053/paper053.pdf">論文</a></li>
                  <li><a href="http://nmis.isti.cnr.it/sebastiani/Publications/LREC10.pdf">SentiWordNet</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#50]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Representation Learning by Learning to Count </div>
            <div class="info">
              <div class="authors">Mehdi Noroozi et al. </div>
              <div class="conference">ICCV 2017 (Oral)</div>
            </div>
            <div class="slide_editor">Tomoyuki Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p> </p>「画像内のprimitiveを認識できることは高次の特徴を掴んでいる」という考えを基にした、self-supervisedな特徴表現学習手法。
                画像のオリジナルとそれらを各タイルに分割したものを同じNNに入力し、出力されるタイルのprimitive数の和とオリジナルのprimitive数が一致するように学習する。
                しかしそれでは出力を単に小さくするように学習することで損失を０にできてしまうので異なる画像も含めたcontrastiveな損失を用いる。
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p> </p>画像識別、物体検出、意味領域分割などのタスクで評価を行っており、識別ではSoTA。
                学習したNNからの出力を確認すると、ノルムが大きいものは高次な物体が含まれる画像、小さいものは低次なテクスチャしか含まない画像が得られた。
                これからNNが高次なprimitiveをcountしていることが考察できる。
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/count1.png"><img src="slides/figs/count2.png"><img src="slides/figs/count3.png"></p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>自由記述欄</h1>
                <p></p>損失を最小化することで結果的にNNが「何かしらのprimitiveを数えていること」になり、冒頭の考えと合わせることで特徴表現学習が可能となる。 
                何か明示的に数える対象を与えるように想像したが、実際に何を数えているかは学習ベースで、明示的には与えていない点が非常に面白い。
                <ul>
                  <li><a href="https://arxiv.org/abs/1708.06734">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#51]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Visual Storytelling</div>
            <div class="info">
              <div class="authors">Ting-Hao (Kenneth) Huang, et al.</div>
              <div class="conference">NAACL 2016</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>アルバムのような時系列画像でキャプション生成を行うためのデータセット。ストーリ性のある画像キャプションデータセット：SINDを構築。10,117個のFlickrアルバム、210,819枚の写真。各アルバムは平均20.8枚。</p>
                <p>descriptions for images in isolation (DII)：画像一枚の記述</p>
                <p>descriptions of images in sequence (DIS)：連続画像</p>
                <p>stories for images in sequence (SIS)：ストーリー</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180303VisualStorytelling_1.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法・リンク</h1>
                <p>SINDを使いキャプションをイメージごとに生成(Table5)。ストーリー性を含んだキャプションが生成できている。METEORによるスコアも向上。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1604.03968.pdf">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <p> <img src="slides/figs/180303VisualStorytelling_2.jpg"></p>
              </div>
            </div>
            <div class="slide_index">[#52]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Inferring and Executing Programs for Visual Reasoning</div>
            <div class="info">
              <div class="authors">Justin Johnson, Bharath Hariharan, Laurens van der Maaten, et al.</div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>理由に基づいたVQA。既存の手法では，入力を出力に直接マッピングしているため，視覚的推論の学習というよりも，データの偏りを学習しているといえる。そこで，理由を伴った視覚的推論モデルを提案。モデルは、プログラムジェネレータと実行エンジンの2部構成。CLEVRベンチマークを使用し評価。回答の柔軟性、拡張性の向上。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180303VisualReasoning_1.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法・リンク</h1>
                <p>プログラムジェネレータは、質問の読み取り、単語の羅列として表現される質問から質問に答えるためのプログラムを生成する。基本的にはLSTMのsequence-to-sequenceの考え方。</p>
                <p>実行エンジンは、予測されたプログラムをミラーリングするニューラルモジュールネットワークを構成し、実行することで画像から回答を生成。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1705.03633.pdf">論文</a></li>
                  <li><a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">seq-to-seq</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <p> <img src="slides/figs/180303VisualReasoning_2.jpg"></p>
              </div>
            </div>
            <div class="slide_index">[#53]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Deep mutual learning </div>
            <div class="info">
              <div class="authors">Ying Zhang, Tao Xiang, Timothy M. Hospedales, Huchuan Lu</div>
              <div class="conference">2017</div>
            </div>
            <div class="slide_editor">Tomoyuki Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>教師モデルと生徒モデルを分けていた従来の蒸留に対してモデル同士の相互学習を提案。ハードラベルによる交差エントロピーと対象モデル以外のモデルの出力とのKL距離を最小化するように学習する。様々なモデル同士の相互学習実験や通常の蒸留との比較、相互学習を行った場合の解がより高い汎化性能を保有していることの検証実験も行っている。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/deep_mutual2.png"><img src="slides/figs/deep_mutual3.png"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>画像識別において通常の蒸留を行うよりも精度が良くなった。生徒モデルの中で相対的に小規模なモデルのみならず大規模なモデルも独立で学習を行うより精度が良かった。さらに相互学習を行うことで、wider minimaに収束しているという実験結果も得られた。特に出力される事後確率のエントロピーが大きくなるように学習されることがwider minimaへの収束を促していることがいわれている。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>自由記述欄</h1>
                <p>ハードラベルありき（ないと相互学習が正しい方向に向かわない）の手法であったが、教師なし手法に拡張できたら面白くなりそうだと感じる。</p>
                <ul>
                  <li><a href="https://arxiv.org/abs/1706.00384">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#54]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Learning Features by Watching Objects Move </div>
            <div class="info">
              <div class="authors">Deepak Pathak, Ross Girshick, Piotr Dollar, Trevor Darrell, and Bharath Hariharan</div>
              <div class="conference">CVPR 2017</div>
            </div>
            <div class="slide_editor">Tomoyuki Suzuki</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>動き特徴を利用した前景（物体）領域情報は汎用的な表現学習に役立つという考えから、NLCなどのhand-craftな手法を組み合わせて擬似的な動体領域を作成し、それを教師として領域分割をCNNに解かせることで表現特徴を得る。物体検出、物体・行動認識、意味領域分割の問題設定において評価を行った。表現学習のデータとしてYFCCを用いている。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/watching_object.png" alt="180302AffordanceNet"><img src="slides/figs/watching_object3.png" alt="180302AffordanceNet"><img src="slides/figs/watching_object2.png" alt="180302AffordanceNet"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>Pascal VOCの物体検出において教師なし表現学習でSoTA。特にfine-tuningに利用するデータが少量の場合と多くの層のパラメータを固定してfine-tuinigした場合で大きな効果を発揮した。しかし、物体・行動認識、意味領域分割においては従来手法より劣っている。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>自由記述欄</h1>
                <p>実験は丁寧に行われてる印象。表現学習の設定自体が物体検出を意識しているようにも感じられ（単一物体が写っている画像を優先的に取り出している？など）、物体検出でうまくいくのは当たり前な気もした。</p>しかし、意味領域分割で精度が出ない原因がよくわからなかった（物体部分はできているが背景の分割ができていない？）。
                <ul>
                  <li><a href="https://arxiv.org/abs/1612.06370">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#55]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Focal Loss for Dense Object Detection</div>
            <div class="info">
              <div class="authors">Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar, Facebook AI Research (FAIR)</div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>1-stage物体検出手法の精度向上を図る。 YOLOやSSDなどは，矩形領域における前景と背景の面積が不均衡であるため，2-stage物体検出手法に勝てないと推測。この問題を解決するためにクロスエントロピーを再構築したFocal Lossを提案。学習時のネガティブサンプルの影響を減らすことができる。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180303FocalLoss.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法</h1>
                <p>Focal Loss</p>
                <p>クロスエントロピーに重みを追加</p>
                <ul>
                  <li>正解の場合には重みを低減</li>
                  <li>不正解の場合には従来通り</li>
                </ul>
                <p>→正解になりやすい背景に引っ張られなくなる</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>結果・リンク集</h1>
                <p>既存の最先端2ステージ検出器(2017年現在)の全てにおいて精度を上回り，既存の1ステージ検出器の検出速度と同等</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1708.02002.pdf">論文</a></li>
                  <li><a href="https://github.com/facebookresearch/Detectron">ソースコード</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#56]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Towards Diverse and Natural Image Descriptions via a Conditional GAN</div>
            <div class="info">
              <div class="authors">Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin, et al.</div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>画像キャプショニングの性能向上を図る。従来のロバスト性が低いRNNに代わって，GANのフレームワークを採用することで，自然性と多様性を向上。図より，ジェネレータ(G)が文を生成し，ディスクリミネータ(E)が文や段落がどれだけうまく記述されているかを評価する。GとEを同時に学習させることにより，自然な文章を生成。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180303TowardsDiverse_1.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>結果・リンク集</h1>
                <ul>
                  <li>人間，G-MLE，G-GAN(本手法)の3つを比較して性能評価</li>
                  <li>ユーザー調査、定性的な例、および検索アプリケーションなどの評価により，より自然かつ多様、意味的に関連する記述を実現</li>
                </ul>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1703.06029.pdf">論文</a></li>
                  <li><a href="https://arxiv.org/abs/1411.4555">MLE</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <p><img src="slides/figs/180303TowardsDiverse_2.jpg"></p>
              </div>
            </div>
            <div class="slide_index">[#57]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Learning to Segment Every Thing</div>
            <div class="info">
              <div class="authors">Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick</div>
              <div class="conference">CVPR 2018</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>ラベルが完全に手に入らない際にでも転移学習が可能なセグメンテーション手法（論文中ではPartially Supervised Training Paradigm, weight transfer functionを紹介）を提案する。条件として、bboxが手に入っている物体に対してセグメンテーション領域を学習可能。Mask R-CNNをベースとしているが、Weight Transfer Functionを追加、セグメントの重みを学習・推定して誤差計算と学習繰り返し。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180303SegmentEverything.png" alt="180303SegmentEverything"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>Visual Genome Datasetから3,000の視覚的概念を獲得、MSCOCOから80のマスクアノテーションを獲得した。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>弱教師付き学習が現実的な精度で動作するようになってきた？アノテーションはお金や知識があっても非常に大変なタスクであり、いかに減らすかという方向に研究が進められている。（What's next?ー弱教師/教師なしの先とは？）</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1711.10370.pdf">論文</a></li>
                  <li><a href="http://ronghanghu.com/">著者</a></li>
                  <li><a href="http://kaiminghe.com/">Kaiming He</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#58]</div>
            <div class="timestamp">2018.3.3 10:46:40</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">One-Shot Visual Imitation Learning via Meta-Learning</div>
            <div class="info">
              <div class="authors">Chelsea Finn*, Tianhe Yu*, Tianhao Zhang, Pieter Abbeel, and Sergey Levine</div>
              <div class="conference">NIPS 2017</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>ワンショットで人間の教示を模倣するロボットのための学習「One-shot Imitation Learning」を提案。人間が物体を把持するなど動作を教示するとロボットが特徴や動作を学習してタスクをこなす様子を学習。Model-Agnostic Meta-Learning（MAML; ICML2017）を応用したモデルを提案し、（VR空間、人間のデモによる）教示から動作を学習する。アーキテクチャはCNNをベースとしてRGB入力から特徴を抽出、中間層（全結合層直前）からロボットの動作やバイアス項を入力してロボットの行動（pre-/post-update）を出力する。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180303OneshotImitation.png" alt="180303OneshotImitation"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>ロボットの把持タスクをシミュレーション/実空間にておこなった。シミュレーションでは提案法のMILが1ショットで85.81%、5ショットで88.75%（従来法LSTMでは各78.38%, 83.11%）。実空間では90%を実現（従来法LSTM/contextualでは25%）。詳細にはプロジェクトページやプレゼンのビデオを参照。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1709.04905.pdf">論文</a></li>
                  <li><a href="https://sites.google.com/view/one-shot-imitation">Project</a></li>
                  <li><a href="https://github.com/tianheyu927/mil">GitHub</a></li>
                  <li><a href="https://vimeo.com/252186304">Presen</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#59]</div>
            <div class="timestamp">2018.3.3 10:13:42</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography</div>
            <div class="info">
              <div class="authors">Michael J. Wilber, Chen Fang, Hailin Jin, Aaron Hertzmann, John Collomosse, Serge Belongie</div>
              <div class="conference">ICCV 2017</div>
            </div>
            <div class="slide_editor">Munetaka Minoguchi</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>CVにおけるデータセットでは，写真に対するラベル付けが一般的。写真だけでなく，イラストや風景画などに対して，以下の3属性のラベルを付加。</p>
                <ul>
                  <li>メディア：漫画、油絵、鉛筆スケッチ、水彩画などで作成した画像にラベル付け</li>
                  <li>感情：視聴者に平静、幸せ/陽気、悲しい/悲観的、恐ろしい/恐れを感じさせるような画像にラベル付け</li>
                  <li>オブジェクト：自転車、車、猫、犬、花、人などの画像にラベル付け</li>
                </ul>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180302BehanceArtisticMedia.jpg"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>手法</h1>
                <p>Behance Artistic Media Dataset</p>
                <ul>
                  <li>ラベル作成にhuman-in-the-loopを採用し，人間とコンピュータのハイブリッドを図る。</li>
                  <li>全てのラベルについて学習し，ランク付けを行う。その後，高い順位の画像を人がラベル付け。これを4回繰り返す。</li>
                  <li>基本的にはLSUNの方法に基づいている(リンク参照)</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>結果・リンク集</h1>
                <p>物体認識や物体検出，画像の類似度，属性推定など様々なタスクの機械学習実験を実施。定性的な評価にはなるが，明らかにVOCやImageNetなどの既存のデータセットよりも広い表現の画像で多くのタスクが処理可能となる。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1704.08614.pdf">論文</a></li>
                  <li><a href="http://behance.net">Behence</a></li>
                  <li><a href="https://arxiv.org/abs/1506.03365">LSUN</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#60]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection</div>
            <div class="info">
              <div class="authors">Thanh-Toan Do, Anh Nguyen, Ian Reid, Darwin G. Caldwell, Nikos G. Tsagarakis</div>
              <div class="conference">ICRA 2018</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>物体検出とアフォーダンス（というよりは機能？）のセグメントを同時に回帰するネットワーク、AffordanceNetに関する論文。ロボットへの把持位置/機能教示を行うことができる。基本的なモデルはMask R-CNNを適用していて、物体検出のためのbboxと物体に対する機能セグメントを正解として学習する。多タスクの誤差関数は物体カテゴリ、座標、機能セグメントの3つに関するものである。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180302AffordanceNet.png" alt="180302AffordanceNet"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>従来、物体検知と機能推定は別個に学習・認識されていたが、本研究では多タスク学習の枠組みで、単一モデルにてEnd-to-End学習した。IIT-AFF Datasetにて73.35（SoTAは69.62）、UMD Datasetにて79.9（SoTAは77.0）。モデルも公開されており、誰もがAffordanceNetを実装できるようにしている。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>任意のセグメンテーションラベルさえあれば、物体検知とあらゆる高次なセグメンテーションモデルが実現可能となった。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1709.07326.pdf">論文</a></li>
                  <li><a href="https://github.com/nqanh/affordance-net">GitHub</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#61]</div>
            <div class="timestamp">2018.3.2 20:38:13</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN</div>
            <div class="info">
              <div class="authors">Jiyang Gao, Zijian (James) Guo, Zhen Li, Ram Nevatia</div>
              <div class="conference">arXiv:1711.07607</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>画像識別における知識蒸留（Knowledge Distillation）の内容である。本論文ではある概念（e.g. 動物認識、人工物認識）ごとに教師となる識別器を事前学習しておき、それらの知識を単一の識別器に学習（これをKnowledge Concentrationと呼称）する。いわば複数の先生がある生徒に教えるという流れで学習する。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180302KnowledgeConcentration.png" alt="180302KnowledgeConcentration"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>よくも悪くも、現状のCVはImageNetの1,000カテゴリに頼っているが、これを100倍の100,000カテゴリに増やして学習したらどうか？また、いかにしたら効率よく学習ができるかを検討した。結果はSingle Model（従来のようなCNNによるカテゴリ識別学習）にするよりも複数のスペシャリスト識別器から知識蒸留を行う方が効率よく、精度よく学習ができた。本論文で使用したEFT（Entity-Foto-Tree）データセットはカテゴリ数でImageNetの100倍、JFT-300Mの5倍である。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント・リンク集</h1>
                <p>読んだだけでGoogleの研究所であることがわかってしまう論文。実行ができるかどうかはさておき、大規模データの扱いやCNN学習の効率化という意味でも精読すべき論文。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1711.07607.pdf">論文</a></li>
                  <li><a href="https://jiyanggao.github.io/">著者</a></li>
                  <li><a href="http://codecrafthouse.jp/p/2018/01/knowledge-distillation/">知識蒸留の参考</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#62]</div>
            <div class="timestamp">2018.3.2 20:04:42</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">We Are Humor Beings: Understanding and Prediction Visual Humor</div>
            <div class="info">
              <div class="authors">Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Virginia Tech, TTI-Chicago, Facebook AI Research</div>
              <div class="conference">CVPR 2016</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                <ul>
                  <li>視覚とユーモアの関係をモデル化(不調和説に基づく)</li>
                  <li>アニメ画像の面白さを推定</li>
                  <li>画像のオブジェクトと面白さの関連性を推定</li>
                  <li>データセットの作成</li>
                  <li>抽象的なシーンを使用したユーモアを引き起こすシーンの理解</li>
                </ul>
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180205HumorBeings.jpg"></div>
            <div class="item3">
              <h1>手法1</h1>
              <div class="text">
                <p>面白さ推定</p>
                <ul>
                  <li>特徴抽出し，重み付き誤差平均(面白さによって誤差を変える)</li>
                  <li>Abstract Scenesのアニメ画像(VQAの画像とクラウドソーシングで依頼した面白い画像)に5段階評価したデータセット(AVH)</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>手法2</h1>
              <div class="text">
                <p>面白い画像・面白くない画像の変換</p>
                <ul>
                  <li>オブジェクトを変更することで，面白い⇔面白くない画像に相互変換</li>
                  <li>AVHから面白いものを選び，クラウドソーシングで依頼した面白くない画像を使い/面白い画像ペアを作成</li>
                  <li>どのオブジェクトが面白さに影響しているか調査</li>
                </ul>
                <p>結果：特に人や動物などのオブジェクトが面白さに影響</p>
              </div>
            </div>
            <div class="slide_index">[#63]</div>
            <div class="slide_editor">Munetaka Minoguchi</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Vehicle Three-Dimensional Pose and Shape Estimation from Multiple Monocular Vision</div>
            <div class="info">
              <div class="authors">Wenhao Ding, Shuaijun Li, Guilin Zhang, Xiangyu Lei, Huihuan Qian, Yangsheng Xu</div>
              <div class="conference">arXiv:1802.03515</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>オーバーラップが少ない複数視点カメラから自動車の3次元姿勢や形状を復元する研究。CNNにより自動車のキーポイントや姿勢/3次元形状を出力する。これら情報をヒントに、カメラ視点を推定する。2D画像上でのキーポイント推定にはconv-de-convを4回繰り返すhourglassアーキテクチャを採用、3次元姿勢や形状の推定にはCross Projection Optimization (CPO)を採用し2D-3Dの投影誤差を最小化した。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180302Vehicle3DPose.png" alt="180302Vehicle3DPose"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>物体のキーポイント検出においてState-of-the-art。6DoF推定手法 (Pavlakos, ICRA17)では12キーポイントの平均誤差が37.88であったが、提案手法では10.48まで低減した。また、回転/並進誤差も3DVP (Xiang+, CVPR15)では11.18/N/Aであったが、2.87/4.73まで向上させた。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1802.03515.pdf">PDF</a></li>
                  <li><a href="https://arxiv.org/abs/1703.04670">6-DoF Object Pose from Semantic Keypoints</a></li>
                  <li><a href="http://cvgl.stanford.edu/papers/xiang_cvpr15_3dvp.pdf">Data-Driven 3D Voxel Patterns for Object Category Recognition</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#64]</div>
            <div class="timestamp">2018.3.2 10:15:30</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Joint Event Detection and Description in Continuous Video Streams</div>
            <div class="info">
              <div class="authors">Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko</div>
              <div class="conference">arXiv:1802.10250</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>End-to-Endでイベント検出（行動の時系列セグメント化）とキャプショニングを実行するタスクを提供する。モデルには3D Convolutionや階層的LSTM（two-level hierarchical LSTM）を採用した。基本的には<a href="https://cs.stanford.edu/people/ranjaykrishna/densevid/">Dense Captioning Events in Videos</a>をベースにして研究を行なっているが、{Controller, Captioner} LSTMの二段階により前の候補のセンテンスやビデオコンテキストを考慮しつつ状態を更新（Controller LSTM）し、候補領域の特徴を参照しつつキャプションを生成（Captioner LSTM）する。時系列候補領域とキャプションはmulti-task学習、End-to-Endで学習される。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180302VideoCaption.png" alt="180302VideoCaption"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>候補領域生成やキャプショニングの精度を検証した。また、データセットにはActivityNet CaptionsやTACoS Datasetを用いた。候補領域については従来法のDAPが30, multi-scale DAPが38 @AUC (IoU>0.8)に対して提案法であるJEDDi-Netは58.21を記録した。また、キャプショニングについては従来法が{17.95, 4.82, 17.29} (各BLEU1, METEOR, CIDEr)の問題に対して{19.97, 8.58, 19.88}を記録した。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>コメント/リンク集</h1>
                <p>動画のタスクはカテゴリのみでなく言語やより表現力豊かな認識ができなければいけない時期になって来た？時系列表現にもまだまだ課題が多いので、これからさらに動画認識にチャレンジすべき。また、キャプショニングの問題は感性評価に対する知見や確固たる評価方法が確立されるとさらに面白くなるのではないか。</p>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1802.10250.pdf">PDF</a></li>
                  <li><a href="https://www.bu.edu/cs/profiles/kate-saenko/">Kate Saenko</a></li>
                  <li><a href="https://cs.stanford.edu/people/ranjaykrishna/densevid/">Dense Captioning Events in Videos</a></li>
                  <li><a href="https://github.com/kenshohara/3D-ResNets-PyTorch">3D Convolution (3D-ResNets-PyTorch)</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#65]</div>
            <div class="timestamp">2018.3.2 09:36:26</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Neural Aesthetic Image Reviewer</div>
            <div class="info">
              <div class="authors">W. Wang, et al.</div>
              <div class="conference">arXiv:1802.10240</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>AIが写真の感性評価やコメント生成を行なってくれる。写真とそのコメントが対応づけられた大規模DBであるAVA-Reviews dataset（52,118画像、312,708コメント）を学習することで写真を入力して図の(1)Predictionや(2)Commentsのようなものが得られる。モデルはCNNにより感性評価（Low-/High-Aesthetic category）を、CNN+LSTM（RNN）によりコメント（e.g. Fastastic colors）を出力する。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180301NeuralAesthetic.png" alt="180301NeuralAesthetic"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>(i) 人間のような画像に対する感性評価（image aesthetics）をコンピュータに実装した。(ii)自然言語の出力により人間の高次な認知能力を実現。(iii) 画像-言語の組み合わせによるデータセットAVA-Reviews datasetを新規に構築した。</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/pdf/1802.10240.pdf">PDF</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#66]</div>
            <div class="timestamp">2018.3.2 08:39:04</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Interpreting CNN Knowledge via an Explanatory Graph</div>
            <div class="info">
              <div class="authors">Q. Zhang, et al.</div>
              <div class="conference">AAAI 2018</div>
            </div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  深層学習の解釈性に関する論文であり、畳み込み層の特徴マップの応答を外的に解析して対応する反応を可視化。畳み込みの各フィルタが異なる部位（e.g. 馬の耳や目）に反応するので、グラフにより解析して元画像の対象位置にアクセス。</p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180301interpretability.png" alt="180301interpretability.png"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>
                  Ground-truthなしに各部位に関する解釈性を与えたことが新規性である。図に示すように入力画像に対するパーツごとの解析をフィルタの反応やグラフの解析から可視化することができる。
                  さらに、異なる画像間においても一貫性のある反応を得ることができた。
                </p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>自由記述欄</h1>
                <p>
                  深層学習は教師なしによる解釈性を獲得しているが、まだ反応している部分の可視化や部分ごとの解析が進んでいるにすぎない。さらなる発展のためには、言語的な解釈や人間にわかりやすい加工（イラストとか？）が必要になるのではないだろうか。</p>
              </div>
            </div>
            <div class="slide_index">[#67]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">HemaApp: Noninvasive Blood Screening of Hemoglobin using Smartphone Cameras</div>
            <div class="info">
              <div class="authors">E. J. Wang et al.,</div>
              <div class="conference">Ubicomp 2016</div>
            </div>
            <div class="slide_editor">Kensho Hara</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  スマホカメラを使って非侵襲なヘモグロビン濃度の測定を実現．血中の酸素飽和度の測定などはこれまでにもあったがヘモグロビン濃度まで測定できているものはなかった．
                  照明条件とRGBの変化からヘモグロビン濃度を推定するためのアルゴリズムを提案．
                </p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/180228_hemaapp.png" alt="180228_hemaapp.png"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>特別な装置を使うことなく簡単にスマホカメラでヘモグロビン濃度測定を実現した点．</p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>自由記述欄</h1>
                <p>システムやユーザスタディの完成度や完全に実現できたときの有用性が評価された？</p>
              </div>
            </div>
            <div class="slide_index">[#68]</div>
            <div class="timestamp">2018.3.1 10:44:03</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">End-to-end Driving via Conditional Imitation Learning</div>
            <div class="info">
              <div class="authors">Felipe Codevilla, Matthias Müller, Alexey Dosovitskiy, Antonio López, and Vladlen Koltun,</div>
              <div class="conference">arXiv</div>
              <div class="paper_id">1710.02410</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">自動運転を模倣学習により行う手法を提案。実空間での学習結果をヴァーチャルな空間での自動運転にて再現することができた。RGB画像、計測（e.g. スピード）や命令（e.g. turn right）などからステアリング、アクセル、ブレーキなどのコマンドを出力して自動車を操作する。</div>
            </div>
            <div class="item2"><img src="slides/figs/180205conditionalimitationlearning.png"><img src="slides/figs/180205framework_imitation.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>模倣学習による自動運転を実現した。</li>
                  <li>実空間とシミュレーションベースの転移を行うことにも成功。</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>リンク集</h1>
              <div class="text">
                <ul>
                  <li>自動運転の学習はシミュレーションベースで完結してしまう可能性がある。</li>
                  <li>メタ学習/模倣学習/強化学習などはCVに徐々に取り入れられてくるはずなので、2018年は学習しておいて損はない？</li>
                  <li><a href="https://arxiv.org/pdf/1710.02410.pdf" target="blank">[論文] End-to-end Driving via Conditional Imitation Learning</a></li>
                  <li><a href="https://www.youtube.com/watch?v=cFtnflNe5fM" target="blank">YouTube</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#69]</div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Open3D: A Modern Library for 3D Data Processing</div>
            <div class="info">
              <div class="authors">Qian-Yi Zhou et al.,</div>
              <div class="conference">arXiv</div>
              <div class="paper_id">1801.09847</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">3Dデータを取り扱い、迅速な開発を可能にする<a href="http://www.open3d.org/" target="blank">Open3D</a>を提供する。Open3DはC++/Pythonをサポート、並列化にも対応しており、クラウドで開発することが可能。
                点群読み込み-ダウンサンプリング-法線の計算、シーン再構築、3次元可視化などの処理が含まれている。
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180205open3d.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">3次元画像処理のコミュニティにて有益なオープンソースを提供し、その<a href="https://github.com/IntelVCL/Open3D" target="blank">コード</a>も提供されている。</div>
            </div>
            <div class="item4">
              <h1>コメント・リンク集</h1>
              <div class="text">
                <ul>
                  <li><a href="http://www.open3d.org/" target="blank">Project page</a></li>
                  <li><a href="https://arxiv.org/pdf/1801.09847.pdf" target="blank">[論文] Open3D: A Modern Library for 3D Data Processing</a></li>
                  <li><a href="https://github.com/IntelVCL/Open3D" target="blank">Code</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#70]</div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Hierarchical Variational Autoencoders for Music</div>
            <div class="info">
              <div class="authors">A. Roberts et al.,</div>
              <div class="conference">NIPS WS on Machine Learning for Creativity and Design, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                音楽を生成するためのHierarchical Variational Autoencoders (VAE) を提案．
                エンコーダとデコーダがLSTMで構成されているReccurent VAEがベース．<a href="https://goo.gl/twGuP2">結果サンプル</a>長い音楽（実験では32小節）を単純なLSTMデコーダで生成するのは難しいので，
                この研究では複数のLSTMを階層的に重ねて，段階的に長くしていくHierarchical VAEを提案．
                ループメロディの外挿や，メロディの生成，3ピース構成の音楽生成の実験で性能を検討．
                結果の音楽やコードは公開されている．
              </div>
            </div>
            <div class="item2"><img src="slides/figs/hierarchical_vae.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>階層的なLSTMによるデコーダをVAEによる音楽生成に導入</li>
                  <li><a href="https://goo.gl/twGuP2">結果サンプル</a></li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>これも長期的な構成を考えて生成することはできていない</li>
                  <li>Future Workにはインタフェースを作るとあるし，1曲まるごと作るというよりは適当にサンプルを出して作曲家のアイデアを促進することを目指しているのかな．</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#71]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Generating the Future with Adversarial Transformers</div>
            <div class="info">
              <div class="authors">C. Vondrick et al.,</div>
              <div class="conference">CVPR, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                未来の動画を予測して生成する手法を提案．
                4フレーム x 64画素 x 64画素のクリップを入力として，その後の16フレームの動画を生成．
                完全に新しいフレームを生成するのは難しいので，入力フレームの変換により未来のフレームを生成するのがポイント．
                論文の主張としては，きれいな動画を作るにはLow-Levelな情報が重要だけど，未来予測のためにはHigh-Levelな理解も必要で，
                その両者を一つのネットワークで一気に学習するのは難しいとしている．
                だから，Low-Levelな情報は元のフレームを変換することで引っ張ってきて，ネットワークはHigh-Levelな特徴抽出に集中させるのが良いとのこと．
                このネットワークの学習はGANベース．
                生成動画の主観評価や可視化，Generatorの特徴を利用した物体認識タスクなどで性能を評価．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/generating_future.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>元のフレームからの変換により未来のフレームを生成する手法を提案</li>
                  <li>未来の動画生成において，敵対的学習により大規模な教師なしデータを利用した学習を実現</li>
                  <li>直接動画を生成したり，回帰誤差で学習したりする手法よりも良いことを主観評価実験で確認          </li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>入力が4フレームだけだけど，もっと増やすと性能は変わるのか気になる</li>
                  <li>主観評価で本物と比較すると提案手法が一番嫌われている率が高いのもちょっと気になる</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#72]</div>
            <div class="slide_editor">Kensho Hara</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">DensePose: Dense Human Pose Estimation In The Wild</div>
            <div class="info">
              <div class="authors">Rıza Alp Guler et al.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                身体の表面形状まで考慮したDenseな姿勢推定手法に関する研究。サーフェイスモデルを提供するSMPLタイプとアノテーションベースのMSCOCOタイプを提供。手法はMask RCNN（w/ ResNet-50, ROI-align, Regression）をベースに構築している。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180205densepose.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>DenseReg [Guler,CVPR17]は顔表面の推定に対して、本研究では身体全体の表面やデンスなポイントを回帰。</li>
                  <li>SMPLやDense-COCOのデータセットを構築</li>
                  <li>非拘束（in the wild）の環境にてDensePoseを学習。</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>リアルが完全に崩壊した。（Face2Faceの全身モデル版が実現可能になった？）</li>
                  <li>CG/UIの分野との親和性がより高くなった</li>
                  <li><a href="https://arxiv.org/abs/1802.00434" target="blank">DensePose: Dense Human Pose Estimation In The Wild</a></li>
                  <li><a href="https://arxiv.org/abs/1612.01202" target="blank">DenseReg</a></li>
                  <li><a href="http://smpl.is.tue.mpg.de" target="blank">SMPL</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#73]</div>
            <div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">What will Happen Next? Forecasting Player Moves in Sports Videos</div>
            <div class="info">
              <div class="authors">Panna Felsen et al.</div>
              <div class="conference">ICCV, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                チームスポーツにおいて次に起こることを予測する研究。2チームに分かれたゴール型スポーツを対象とし、ボールを持つ選手の遷移やファールの有無などの推定を行った．
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180303whatwillhappen.PNG"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>水球とバスケットボールのデータセットを構築した</li>
                  <li>画像から選手やボールの位置を上から見た画像に変換する手法を提案した</li>
                  <li>他のスポーツで学習したものを適用した場合(例：学習→水球　テスト→バスケ)ランダムフォレストの方がニューラルネットより精度が高いことが分かった</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>この論文のようにニューラルネットがうまくいかない例を調べるのは面白そう</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#74]</div>
            <div class="slide_editor">Shintaro Yamamoto</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Visual Forecasting by Imitating Dynamics in Natural Sequences</div>
            <div class="info">
              <div class="authors">Zeng et al.</div>
              <div class="conference">ICCV, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                動画シークエンスから未来を予測する研究。フレーム間の遷移モデルを考え，次のフレームや行動を推定する。適用対象はフレームの生成から次のシーンの選択など幅広い。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180303visualforecasting.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>ドメイン知識やhandcrafted特徴無しにinverse reinforcement learningとして学習させる</li>
                  <li>フレーム生成、行動予測、ストーリー予測全てにおいて精度の向上に成功した</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li><a href="https://arxiv.org/abs/1708.05827" target="blank">論文URL</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#75]</div>
            <div class="slide_editor">Shintaro Yamamoto</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">A Domain Based Approach to Social Relation Recognition</div>
            <div class="info">
              <div class="authors">Qianru Sun et al.</div>
              <div class="conference">CVPR, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                画像中に写っている人々の関係を推測する研究。社会心理学に基づいた16の関係(親子、友人など)を識別する。それぞれの人物から抽出された特徴を入力とするネットワークにより判定する。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180306relation.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>社会心理学に基づいた理論をコンピュータビジョンに導入した</li>
                  <li>画像に関係性などのラベルを付けることで、より広い用途で用いることができるデータベースを提案</li>
                  <li>社会心理学に基づき、セマンティックなアトリビュートを収集した</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>社会学系の理論をCVに持ってくるのは面白そう</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#76]</div>
            <div class="slide_editor">Shintaro Yamamoto</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Forecasting Interactive Dynamics of Pedestrians with Fictitious Play</div>
            <div class="info">
              <div class="authors">Ma et al.</div>
              <div class="conference">CVPR, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                画像中に写っている人々の歩行ルートを予測する手法。各歩行者に対して歩行モデルを決定し、他の人とぶつからないようによけるなど他者の行動を考慮した上で歩行ルートを決定していく。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180306pedestrians1.png"><img src="slides/figs/180306pedestrians2.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>ゲーム理論に基づき、他の歩行者の進行方向を予測した上でルートを決定する</li>
                  <li>年齢などの情報を抽出し、各歩行者の歩行速度などを決定する</li>
                  <li>既存手法と比べて長期的な予測の精度が向上</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>ゲーム理論の応用は興味深い</li>
                  <li>どれくらいの人数までできるのだろうか？（人ゴミは無理？）</li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#77]</div>
            <div class="slide_editor">Shintaro Yamamoto</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">DeepNav: Learning to Navigate Large Cities</div>
            <div class="info">
              <div class="authors">Brahmbhatt and Hays</div>
              <div class="conference">CVPR, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                目的地までのルートを推測する研究。ストリートビューの画像から、どの方向に進めば銀行やガソリンスタンドなどの目的地に近付けるかを決定していく。ネットワークとしては、目的地までの距離、最も最短となる方角、2枚の画像のどちらが目的地に近いかの3種類を提案。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180306Deepnav.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>アメリカ10都市を対象にストリートビューのデータセットを構築</li>
                  <li>3種類のCNNネットワークを構築し，hand-crafted特徴及びSVRベースの手法より精度が向上した</li>
                  <li>ラベル付けを効率化するメカニズムを提案した</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li>それぞれの目的地に対してどのような特徴を持った方向が選ばれているのか気になった</li>
                  <li>場合によっては同じ場所を何度も回るだけになってしまう？</li>
                  <li><a href="https://arxiv.org/abs/1701.09135" target="blank">論文URL</a></li>
                  <li><a href="http://mcdonalds.csail.mit.edu/" target="blank">比較論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#78]</div>
            <div class="slide_editor">Shintaro Yamamoto</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Forecasting Human Dynamics from Static Images</div>
            <div class="info">
              <div class="authors">Chao et al.</div>
              <div class="conference">CVPR, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                1枚の画像から、人間のモーションを推定する研究。画像から2次元の姿勢を推定し，その結果を3次元に変換することで出力を得る。学習は3段階に分かれており、2次元姿勢推定部は2次元姿勢データベースを使用して学習をし、3次元姿勢推定部はモーションキャプチャデータを2次元投影することにより学習を行い、最後に全体を通して学習を行う。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180306humandynamics1.png"><img src="slides/figs/180306humandynamics2.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>従来研究とは異なり、RNNを用いることにより静止画からモーションを推定することを可能とした</li>
                  <li>推定した2次元の姿勢から3次元の情報を復元するネットワークを提案した</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li><a href="https://arxiv.org/abs/1704.03432" target="blank">論文URL</a></li>
                  <li><a href="http://www-personal.umich.edu/~ywchao/image-play/" target="blank">プロジェクトページ</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#79]</div>
            <div class="slide_editor">Shintaro Yamamoto</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Toward Geometric Deep SLAM</div>
            <div class="authors">Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich</div>
            <div class="conference">arXiv</div>
            <div class="paper_id">arXiv:1707.07410</div>
            <div class="slide_editor">Yoshihiro Fukuhara</div>
            <div class="item1">
              <div class="text">
                <h1>概要</h1>
                <p>
                  2つのCNNを用いた高速かつ頑強な物体追跡手法を提案. 1つ目のCNN(MagicPoint)で入力画像から特徴点を抽出し,
                  2つ目のCNN(MagicWarp)で抽出された特徴点の位置情報のみから２つの画像間のホモグラフィー行列の推定を行う.
                </p>
              </div>
            </div>
            <div class="item2">
              <div class="text">
                <p><img src="slides/figs/20180306--TowardGeometricDeepSLAM.png" alt="20180306--TowardGeometricDeepSLAM.png"></p>
              </div>
            </div>
            <div class="item3">
              <div class="text">
                <h1>新規性・結果</h1>
                <p>
                  MagicPointは幾何学的に安定した点（物体の角や辺など）のみを抽出するため, ノイズに頑強である. また, MagicWarpを用いることで従来手法のように特徴量の記述子（descriptor） を計算する必要がなくなるため, 高速な動作が可能となった. 
                  作成した単純形状のデータセット（Synthetic Shapes Dataset）では FAST, Haris, Shi よりも高精度.
                </p>
              </div>
            </div>
            <div class="item4">
              <div class="text">
                <h1>リンク集</h1>
                <ul>
                  <li><a href="https://arxiv.org/abs/1707.07410">論文</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#80]</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Pedestrian Travel Time Estimation in Crowded Scenes</div>
            <div class="info">
              <div class="authors">Yi et al.</div>
              <div class="conference">ICCV, 2015.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                画像中に写っている群衆が、目的地にたどり着くまでの時間を推測する手法。目的地まで歩行するにあたり、他の歩行者の流れや立ち止まっている人の存在によって歩行ルートは変化する。このように、目的地まで最短距離で向かうことができず人によってルートが変化する状況における歩行時間の推定を行う。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/180307pedestrian.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>目的地までの所要時間を統計的に推定する手法を提案</li>
                  <li>人の流れの妨げになっている場所や異常行動の検出が可能に</li>
                  <li>個人の移動時間に着目している既存研究と比べ、大衆に着目することで精度が向上</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yi_Pedestrian_Travel_Time_ICCV_2015_paper.pdf" target="blank">論文URL</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#81]</div>
            <div class="slide_editor">Shintaro Yamamoto</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">Emotional Filters: Automatic Image Transformation for Inducing Affect</div>
            <div class="info">
              <div class="authors">Afsheen Rafaqat Ali, Mohsen Ali.</div>
              <div class="conference">BMVC, 2017.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">
                入力された感情ヒストグラム(anger, disgust, fear, joy, sadness, surprise, neutral)を想起するように入力画像をカラートランスファーを行う手法を提案。ユーザは参照画像を用意することなく、入力された感情ヒストグラムと画像を元にデータベースから選択される。オブジェクト検出、シーン識別のそれぞれに対して訓練されたCNNに画像を入力することで、それぞれのトップレイヤーから特徴量を抽出。この特徴量と入力感情ヒストグラムを元にデータベースから、参照画像を10枚選択し、Poulのアルゴリズムを元にカラートランスファーを行う。
                
              </div>
            </div>
            <div class="item2"><img src="slides/figs/bmvc2017_emotional_filters.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>ユーザスタディの結果、多くの画像で入力感情を想起するようにカラートランスファーを行うことができた。</li>
                  <li>失敗例1 joy成分が強いヒストグラムを入力したところ、生成画像のjoy成分が強く感じたユーザは少なかった。</li>
                  <li>失敗例2 画像が高次元なコンテキストを含んでいる場合(泣いている少女など)には良い結果は得られなかった。</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li><a href="https://arxiv.org/pdf/1707.08148.pdf" target="blank">論文URL</a></li>
                  <li><a href="http://im.itu.edu.pk/affective-image-transfer/" target="blank">Project</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#82]</div>
            <div class="slide_editor">Kazuki Inoue</div>
          </div>
        </section>
        <section>
          <div class="paper-abstract">
            <div class="title">A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions Supplementary Material</div>
            <div class="info">
              <div class="authors">Kuan-Chuan Peng, Tsuhan Chen, Amir Sadovnik, Andrew Gallagher.</div>
              <div class="conference">CVPR, 2015.</div>
            </div>
            <div class="item1">
              <h1>概要</h1>
              <div class="text">一人の人間は一枚の画像に対して様々な感情を抱くことから、一枚の画像に対する感情のアノテーションをある一つの感情ラベルにするのではなく、ヒストグラム(各成分はanger, disgust, fear, joy, sadness, surprise, neutral)として扱い、データセットを構築。このデータセットに対して、感情ヒストグラムを推定するCNNRというモデルを提案。また、ターゲットとなる感情ヒストグラムをもつ参照画像を用いて、生成画像がターゲット感情ヒストグラムに近くなるようなカラートランスファーを提案。</div>
            </div>
            <div class="item2"><img src="slides/figs/cvpr2015_a_mixed_bag.png"></div>
            <div class="item3">
              <h1>新規性・結果</h1>
              <div class="text">
                <ul>
                  <li>画像1980枚に対して感情ヒストグラム(各7つの成分は概要参照)と感情を表すVA値がアノテーションされているEmotion6データセットデータベースを構築</li>
                  <li>入力を画像とし、感情ヒストグラムをSVRとCNNよりも精度が高いCNNRを提案</li>
                  <li>入力画像のもつ感情ヒストグラムに対して、ターゲット感情ヒストグラムに近くなるようなカラートランスファーを提案。</li>
                </ul>
              </div>
            </div>
            <div class="item4">
              <h1>自由記述欄</h1>
              <div class="text">
                <ul>
                  <li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Peng_A_Mixed_Bag_2015_CVPR_paper.pdf" target="blank">論文URL</a></li>
                  <li><a href="http://chenlab.ece.cornell.edu/Publication/Kuan-Chuan/CVPR15_emotion_supp.pdf" target="blank">Supplementary</a></li>
                </ul>
              </div>
            </div>
            <div class="slide_index">[#83]</div>
            <div class="slide_editor">Kazuki Inoue</div>
          </div>
        </section>
      </div>
    </div>
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>
    <script>
      Reveal.initialize({
        history: true,
        center: false,
        width: '100%',
        height: '100%',
        transition: 'none',
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
  </body>
</html>