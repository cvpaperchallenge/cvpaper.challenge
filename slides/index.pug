extends layouts/_layout

block slides
  +slide
    .title Neural Scene De-rendering
    .info
      .authors Jiajun Wu, et al.
      .conference CVPR 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p シーンの全体理解。オブジェクトの数とそのカテゴリ、ポーズ、位置などの情報をエンコードし、シーンのコンパクトかつ表現力豊に解釈可能な表現の提案。decoderとencoderにより、XML形式の言語表現を実現。特に、encoderは、Renderingの逆であるDe-renderingを実行することで、入力画像をscene XMLに変換する。
    .item2
      .text
        p
          img(src=`${figpath}180307NSD.jpg`)
    .item3
      .text
        h1 新規性
        p 従来研究では、encorderとdecoderベースの深層学習を使用した画像表現を提案してたが、その出力は解釈不可能であるかシーン単一のオブジェクトのみの説明である。そこで、シーン全体かつ解釈可能な表現を出力するモデルの提案。
        p マインクラフトベースの新しいデータセット。
        p de-rendering：入力画像からセグメントを生成し、オブジェクトのプロパティを解釈。推測結果を統合し、元の画像を再構成する。
    .item4
      .text
        h1 コメント・リンク集
        p 単純な全体シーン解釈は進んでいる。これからは、より複雑なシーンの解釈に移る。
        ul
          li
            a(href="http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf") 論文
          li
            a(href="http://nsd.csail.mit.edu/") NSD
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Visual Diarog
    .info
      .authors Abhishek Das, et al. 
      .conference CVPR 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p Visual Diarog：AIエージェントが人間と、画像に関した対話をするタスクを目標とする。エージェントが、画像に対する質問に、対話履歴から文脈を推測し正確に回答する。チャットデータ収集プロトコルを開発し、Visual Dialogデータセット(VisDial)を作成。COCOの120kの画像に10の質問と回答ペアを含む1つのダイアログが含まれており、合計は1.2Mのダイアログ質問回答ペア。
    .item2
      .text
        p
          img(src=`${figpath}180307VIsualDialog.jpg`)
    .item3
      .text
        h1 新規性・リンク
        p VQAとは異なる、Visual Dialogタスク。
        p 3つのエンコーダ2つのデコーダからなるVisual Diarogモデル。
        p コード、モデル、データセット、ビジュアルチャットボックスを公開中。
        ul
          li
            a(href="https://arxiv.org/pdf/1611.08669v5.pdf") 論文
          li
            a(href="https://visualdialog.org") Visual Dialog
    .item4
      .text
        p
          img(src=`${figpath}180307VIsualDialog_2.jpg`)
          
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Learning to Linearize Under Uncertainty  
    .info
      .authors Ross Goroshin  et al. 
      .conference in NIPS 2015
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 
        |「動画は単一画像を表す特徴空間上におけるManifoldとして表すことができる」という考えをもとにしている。その場合、線形な時間変化に対して各フレームを表す特徴量も線形な変位をするのが妥当であり、制約を加えることでそのような特徴空間への埋め込みを学習させる。
        //|時間の設定をもとに対応する特徴量を計算することができ、実験では仮想的なデータにおけるフレーム生成タスクにおいて、鮮明な出力が得られている
          
    .item3
      .text
        h1 手法
        p 
        |t-1, t の埋め込みベクトルzt, zt-1からt+1の埋め込みベクトルzt+1を予測し、それからt+1の画像復元を行うモデルを考えるが、以下の３つの要素を加える。（1）zの時間的変位のcos類似度が近くなるようにする、（２）max-pooling(出力m)とargmax-pooling (出力p)を行い、t+1のz(=(m, p))を求める際は、pを線形外挿により求める、（3）未来の不確実性の対処として、潜在変数δを定義。
        |argmax-poolingはソフトな近似関数を定義することで逆伝搬可能にし、δは学習時はサンプルごとにSGDにより最適化し、テスト時はランダムサンプリング。
        
    .item2
      .text
        p
          img(src=`${figpath}Linearize1.png`)
          img(src=`${figpath}Linearize2.png`)
          //img(src=`${figpath}Linearize3.png`)
          img(src=`${figpath}Linearize4.png`)
    .item4
      .text
        h1 コメント・リンク
        p
        |動画を画像表現空間上のManifold捉える視点、逆伝搬不可能な関数をソフトな関数で近似する手法、潜在変数の導入による未来の不確実性への対応が面白く、非常に参考になりそう。
        |時間の設定をもとに対応する特徴量を計算することができ、実験では仮想的なデータにおけるフレーム生成タスクにおいて、鮮明な出力が得られているが、
        |線形で動く事を前提とした仮想データや今回の特徴空間の制約が実際の識別タスクなどで有効な特徴量かの実験がないなど疑問な点もあった。

        ul
          li
            a(href="https://arxiv.org/abs/1506.03011") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Deep Image Prior 
    .info
      .authors Dmitry Ulyanov et al. 
      .conference 2017
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 
        |「CNNは理論上任意の関数を近似できるが、その構造自体に汎化性能をあげるようなPriorが含まれている」という考えのもと、ランダム初期化されたCNNを用いて高いレベルの画像復元、ノイズ除去などを行った。
        |また、CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかることも示された。深いネットワークの方が復元性能が高かった。
          
    .item3
      .text
        h1 手法
        p 
        |ノイズ画像ｚをencoder-decoderモデルに入力して、生成された画像を欠損画像にMSEを近づけるように学習。
        |注意点として、完全に学習仕切ってしまうと欠損画像と同じものが出るだけなので、学習をある程度のiterationで止めると、復元された画像が得られる。
        |CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかる。
        
    .item2
      .text
        p
          img(src=`${figpath}prior1.png`)
          img(src=`${figpath}prior2.png`)
          img(src=`${figpath}prior3.png`)
    .item4
      .text
        h1 コメント・リンク
        p
        |畳み込み処理×SGDの異常なまでの汎化性能を実験的に裏付けていると思われ非常に面白い。逆にCNNのPriorの苦手なところとして、Adversarial exampleやGANのチェッカーボード現象も関係してそう。
        |自然画像と畳み込みとの関連なのでFractal画像とも関係してそう。逆に人工データに対しては苦手とかあるのだろうか。Deformable ConvやTemporal ConvなどのPriorの気になる。

        ul
          li
            a(href="https://sites.skoltech.ru/app/data/uploads/sites/25/2017/12/deep_image_prior.pdf") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Catching the Temporal Regions-of-Interest for Video Captioning
    .info
      .authors Ziwei Yang, Yahong han, Zheng Wang
      .conference ACM MM 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 動画キャプションのため、動画中から時系列のRegions-of-Interest（RoI）を獲得する。動画中のアテンションを獲得するDual Memory Recurrent Model（DMRM）を提案して時系列の大域的構造/特徴とRoI特徴を対応づける。これにより、人間のように動画を粗く流し見することに相当するモデルが構築できる。さらに詳細に特徴を評価するため、意味的な教示（semantic supervision）を行う。
    .item2
      .text
        p
          img(src=`${figpath}180307VideoROI.png`,alt="180307VideoROI")
    .item3
      .text
        h1 新規性・結果
        p 評価にはMicrosoft Video Description Corpus (MSVD)やMotreal Video Annotation (M-VAD)を採用。動画キャプショニングにおける評価法、BLEU-4, CIDEr, METEORにてState-of-the-artな精度を得た。
    .item4
      .text
        h1 コメント・リンク集
        p 動画キャプショニングは今やると面白い？動画VQAなんかは進んでいるかも？
        ul
          li
            a(href="https://dl.acm.org/citation.cfm?id=3123327") 論文
          li
            a(href="https://ziweiyang.github.io/") Project
          li
            a(href="https://github.com/ziweiyang/dualMemoryModel") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.7 12:28:51

  +slide
    .title Temporal Relational Reasoning in Videos
    .info
      .authors Bolei Zhou, Alex Andonian, Antonio Torralba
      .conference arXiv:1711.08496
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 時系列の理由付け、（物体や人物行動などの）関連性を学習するTemporal Relation Network (TRN)を提案する。TRNはフレーム数を変えながら特徴表現を行い、前後の時系列を対応づけることで理由付けを行う。このネットワークを学習して時系列の対応付けを行うため、3つの動画データベースーSomething-Something（ビデオ数108,499）, Jester（148,092）, Charades（9,848）ーを用いた。
    .item2
      .text
        p
          img(src=`${figpath}180307TRN.png`,alt="180307TRN")
    .item3
      .text
        h1 新規性・結果
        p TRNは場面によりC3DやTwo-Stream ConvNetsよりも高精度。ビジュアルの結果は動画を参照。
    .item4
      .text
        h1 コメント・リンク集
        p 動画像に対しても理由付け（Reasoning）ができるようになってきた。行動検出の高精度化は待たれるが、トリミングされた動画像に対しては効果を発揮する手法。
        ul
          li
            a(href="https://arxiv.org/pdf/1711.08496.pdf") 論文
          li
            a(href="http://relation.csail.mit.edu/") Project
          li
            a(href="https://github.com/metalbubble/TRN-pytorch") GitHub
          li
            a(href="https://www.youtube.com/watch?v=D42erLb42_k") YouTube
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.7 09:23:28

  +slide
    .title Egocentric Basketball Motion Planning from a Single First-Person Image
    .info
      .authors Gedas Bertasius, Aaron Chan, Jianbo Shi
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 一人称視点の画像からゴールリングに到達するまでのバスケットボール選手の動線を生成する。本論文では3D位置や頭部方向も記録する。同タスクを実行するため、まずは画像空間から12Dのカメラ空間に投影を行うEgoCam CNNを学習。次に予測を行うCNN（Future CNN）を構築、さらに予測位置やゴールまでの位置が正確かどうかを検証するGoal Verifier CNNを用いることでより正確な推定を行うことができる。
    .item2
      .text
        p
          img(src=`${figpath}180307EgoBasketball.png`,alt="180307EgoBasketball")
    .item3
      .text
        h1 新規性・結果
        p 複数のネットワークの出力（ここではEgoCamCNNとFutureCNN）を検証するVerification Networkという考え方は面白い。他のネットワークの出力を、検証用のネットワークにより正すというのはあらゆる場面で用いることができる。RNN/LSTM/GANsなどよりも高度な推定ができることが判明した。
    .item4
      .text
        h1 コメント・リンク集
        p 結果例は動画像を参照。未来予測・３次元投影などコンポーネントがDNNにより高度にできるようになってきたからできた研究。さらに検証用のネットワークを構築することで出力自体を操作している。
        ul
          li
            a(href="https://arxiv.org/pdf/1803.01413v1.pdf") 論文
          li
            a(href="https://www.youtube.com/watch?v=wRRRl4QsUQg") YouTube
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.7 09:04:15

  +slide
    .title Beyond Context: Exploring Semantic Similarity for Tiny Face Detection
    .info
      .authors Yue Xi, Jiangbin Zheng, Xiangjian He, Wenjing Jia, Hanhui Li
      .conference arXiv:1803.01555
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p
          a(href="https://www.cs.cmu.edu/~peiyunh/tiny/") Finding Tiny Faces
          |を元ネタにして、画像中から微小な顔を検出する手法を提案。元ネタではコンテキストから小さな顔を検出していたが、本論文では画像の類似性（顔は大小に関わらず特徴が類似する）を考慮して極小な顔を検出した。手法としては、画像中から意味的に類似する領域を計算するためのMetric Learning（特徴空間の距離学習）を用いる。
    .item2
      .text
        p
          img(src=`${figpath}180307BeyondContext.png`,alt="180307BeyondContext")
    .item3
      .text
        h1 新規性・結果
        p 3つの著名な公開データに対して精度を向上させState-of-the-art（と主張しているが、結果のグラフが18/03/07現在論文に埋め込まれていない）。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1803.01555v1.pdf") 論文
          li
            a(href="https://www.cs.cmu.edu/~peiyunh/tiny/") Finding Tiny Faces
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.7 08:27:34

  +slide
    .title Toward Multimodal Image-to-Image Translation
    .info
      .authors Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman
      .conference NIPS 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ピクセル同士の画像対応を行い、画像変換を実行するBicycle GANを提案。従来のImage-to-Image (pix2pix)ではone-to-oneマッピングだったが、本提案ではマルチモーダル、すなわちある画像からあらゆるピクセルの対応関係を考慮した変換をおこなう（例として、図に示すような夜画像の入力からあらゆる日中の画像に変換するなど）。このアルゴリズムを構築するためにVAEベースやLatent RegressorのGANを組み合わせる。
    .item2
      .text
        p
          img(src=`${figpath}180306BicycleGAN.png`,alt="180306BicycleGAN")
    .item3
      .text
        h1 新規性・結果
        p pix2pixと比較して複数の結果を出力する表現力が向上した。マルチモーダルで出力しても結果画像が崩れることなく画像生成を実現した。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1711.11586.pdf") 論文
          li
            a(href="https://junyanz.github.io/BicycleGAN/") Project
          li
            a(href="https://github.com/junyanz/BicycleGAN") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.6 14:58:57

  +slide
    .title Weakly Supervised Affordance Detection
    .info
      .authors J. Sawatzky et al.,
      .conference CVPR 2017
    .item1
      h1 概要
      .text.
        物体のパーツごとのAffordanceを推定する問題の研究．
        CAD120データセットにpixel-wiseのAffordanceラベルを付けてデータセットを作成．
        CNNにより入力画像からAffordanceを推定するが，Affordanceはマルチラベル（複数のラベルを持つ画素が存在）なので，
        それに対応できるような拡張手法を提案．
        加えて，キーポイントレベルのアノテーション (Weakly label) からの学習を行う手法も提案．
        Fully supervised, Weakly supervisedの両設定においてSOTAを達成．

    .item2
      img(src=figpath+"180306_weaklyaffordance.png",alt="180306_weaklyaffordance.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li Affordance推定の問題においてKeypointアノテーションから学習する手法を提案
          li Pixel-wiseアノテーション付きで実データに近いAffordanceデータセットを提供
    .item4
      h1 自由記述欄
      .text
        ul
          li せっかくWeakly Supervisedなんだからデータをたくさん用意したらどうなるかの結果とかも見たい
          li: a(href="https://github.com/ykztawas/Weakly-Supervised-Affordance-Detection") GitHub
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title Action Recognition with Coarse-to-Fine Deep Feature Integration and Asynchronous Fusion
    .info
      .authors Weiyao Lin, Yang Mi, Jianxin Wu, Ke Lu, Hongkai Xiong
      .conference AAAI 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p AAAIに採択された、行動認識の研究。(1)より精緻な特徴量抽出、(2)異なるチャンネルの入力からの非同時性（asynchrony）を考慮して公開データベースに対する認識精度を向上させた。Coarse-, Middle-, Fine-levelの特徴量を統合して識別を実行する、さらにはそれぞれ異なる時間とチャンネル（e.g. rgb at time t & flow at time t+2）からの特徴組み合わせにより参照する尺度を変更し、特徴量をさらに強化した。
    .item2
      .text
        p
          img(src=`${figpath}180306ActionCoarseFine.png`,alt="180306ActionCoarseFine")
    .item3
      .text
        h1 新規性・結果
        p 多階層の特徴量の組み合わせや非同時性を考慮した特徴抽出により手法を構成、UCF101にて95.2%、HMDB51にて72.6%を達成した。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07430.pdf") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.6 14:31:32

  +slide
    .title MarioQA: Answering Questions by Watching Gameplay Videos
    .info
      .authors Jonghwan Mun, et al. 
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 動画によるVideoQA。マリオのプレイ動画から、発生するイベントの質疑応答を行うMarioQAを提案。イベントログを含むビデオクリップを収集し、抽出されたイベントから自動的にQAペアを生成してデータセットを構築。敵を倒す、死ぬ、ジャンプ、キック、持つなどの11個のアクションパラメータを、動画と対応させたコマンド形式で時系列にまとめたものを学習。
    .item2
      .text
        p
          img(src=`${figpath}180306MarioQA_1.jpg`)
    .item3
      .text
        h1 手法・結果・リンク
        p Gated Recurrent Unit (GRU)で質問の特徴抽出。3DFCNでビデオの特徴抽出。2つの特徴から分類。
        p NT (case 1), NT+ET (case 2) and NT+ET+HT (case 3)の3ケースについて精度を比較し、時間的推論能力を検証。ETやHTを加えた場合の方が精度が向上することを確認。
        ul
          li
            a(href="https://arxiv.org/abs/1612.01669") 論文
          li
            a(href="http://cvlab.postech.ac.kr/research/MarioQA/") MarioQA
    .item4
      .text
        p
          img(src=`${figpath}180306MarioQA_2.jpg`)
          
    .slide_index #{getSlideIndex()}

  +slide
    .title Moments in Time Dataset: one million videos for event understanding
    .info
      .authors Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, Aude Oliva
      .conference 1801.03150
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 3秒以内のラベル付けされた動画像が100万以上含まれるデータセットMoments in Time Datasetを提案。今まで動画DBでありがちであった人物のみに偏ることなく、物体や動物、自然現象なども積極的に含んでいる。
    .item2
      .text
        p
          img(src=`${figpath}180305MomentsInTime.png`,alt="180305MomentsInTime")
    .item3
      .text
        h1 新規性・結果
        p 3秒以内の瞬間的な動画にすることでノイズを含まない動画になりやすく、クラス間/クラス内のDIVERSITYを考慮、人物のみに限定せず動画像を汎用的に収集、動き自体の転移を考慮してカテゴリを定義している。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1801.03150.pdf") 論文
          li
            a(href="http://moments.csail.mit.edu/") Project
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.5 20:25:53

  +slide
    .title Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments
    .info
      .authors Peter Anderson, et al.
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 自然言語のナビゲーションを入力として、実空間の中をロボットが動き目的地に到達できるかどうかを競うベンチマーク（Visually-grounded natural language navigation in real buildings）を提案。データセットは3Dのシミュレータによりキャプチャされ、22Kのナビゲーション、文章の平均単語数は29で構成される。
    .item2
      .text
        p
          img(src=`${figpath}180305R2RNavi.png`,alt="180305R2RNavi")
    .item3
      .text
        h1 新規性・結果
        p (1) Matterport3Dデータセットを強化学習を行えるように拡張。(2) 同タスクが行えるようなベンチマークであるRoom-to-Room (R2R)を提案して言語と視覚情報から実空間にてナビができるようにした。(3) seq-to-seqをベースとしたニューラルネットによりベンチマークを構築。VQAをベースにしていて、ナビゲーション（VQAでいう質問文）と移動アクション（VQAでいう回答）という組み合わせで同問題を解決する。
    .item4
      .text
        h1 コメント・リンク集
        p 自然言語の問題はキャプションや質問回答の枠を超えて実空間、さらにいうとロボットタスクに導入されつつある。この研究はビジョン側からのアプローチだが、ロボット側のアプローチが現在どこまでできているか気になる。すでに屋内環境をある程度自由に移動するロボットが実現しているとこの実現可能性が高くなる。SLAMとの組み合わせももう実行できるレベルにある？
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07280.pdf") 論文
          li
            a(href="https://bringmeaspoon.org/") Project
          li
            a(href="https://github.com/peteanderson80/Matterport3DSimulator") GitHub
          li
            a(href="https://niessner.github.io/Matterport/") Matterport3D dataset
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.5 19:53:46

  +slide
    .title Joint Object Category and 3D Pose Estimation from 2D Images
    .info
      .authors Siddharth Mahendran, Haider Ali, Rene Vidal
      .conference arXiv:1711.07426
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 「2D画像」と「物体位置」の入力から「3D物体姿勢」と「カテゴリラベル」を出力する研究。ResNetベースのアーキテクチャを採用している。物体カテゴリが既知/未知の場合の両方で3次元物体姿勢の推定ができる。物体の回転とカテゴリ推定の同時誤差を計算する関数も定義。
    .item2
      .text
        p
          img(src=`${figpath}180305_2D23D.png`,alt="180305_2D23D")
    .item3
      .text
        h1 新規性・結果
        p 3次元物体姿勢推定とカテゴリ推定の同時回帰問題において、Pascal3D+ datasetでState-of-the-artな精度。物体カテゴリが未知の場合でもカテゴリを推定しながら3次元姿勢推定を実行することができる。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07426.pdf") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.5 19:25:04

  +slide
    .title Adversarial Attacks Beyond the Image Space
    .info
      .authors Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi Keung Tang, Alan L. Yuille
      .conference arXiv:1711.07183
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p Adversarial Examples（ネットワークを騙す摂動ノイズ）に関する研究だが、特に物体識別や質問対応（Visual Question Answering）への問題を扱う。さらに、従来の問題では2D画像を取り扱っていたが、本論文では3Dレンダリングとその2D平面投影画像に拡張する。ひとつの摂動ノイズは誤差逆伝播のエラーを直接出力の2D空間に投影すること、もうひとつは敵対的ノイズを予め2D画像に構築して物理空間からレンダリングすることである。
    .item2
      .text
        p
          img(src=`${figpath}180305VQAAttacks.png`,alt="180305VQAAttacks")
    .item3
      .text
        h1 新規性・結果
        p ここでは（１）3次元的な物理的空間を想定して摂動ノイズを加えることができるかどうかについて言及、（２）ノイズを含んだ攻撃画像が与えられた際に、それら攻撃から守るような適切な物理空間を構成できるかどうかを検討した。3次元的な物理空間の攻撃は、法線方向・光源・材質などを考慮しつつ出力に対して防衛可能であるため、2次元の画像空間よりも攻撃が難しいと主張。
    .item4
      .text
        h1 コメント・リンク集
        p 画像空間を超えてボリュームデータに対する摂動ノイズが議論され始めた。どんな空間でも埋め込める攻撃や、それらから防衛可能な手法を汎用的に考えてみたい。また、セキュリティ分野の知見はCVにもっと導入されるべき？
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07183.pdf") 論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.5 19:08:53


  +slide
    .title Personalized Cinemagraphs using Semantic Understanding and Collaborative Learning
    .info
      .authors T. Oh et al.,
      .conference ICCV 2017
    .item1
      h1 概要
      .text.
        Cinemagraph（画像中の一部だけ動画）を自動生成するための手法を提案．
        セマンティックセグメンテーションも導入してシーンの意味的な情報を利用し，
        高品質なCinemagraphの生成を実現する手法とした．
        さらに，動かす対象がたくさんある中でどれを選ぶとよいかをユーザごとの個人的な嗜好を学習することで，
        personalizeされた生成を実現している．
        Stablizeされている動画を入力として，
        セマンティックセグメンテーションの情報を利用したMRFの最適化によりCinemagraphを生成，
        その後学習したuser preferenceのモデルにより候補の中から選択する．

    .item2
      img(src=figpath+"180305personalizedcinemagraph.png",alt="180305personalizedcinemagraph.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li セマンティックセグメンテーションにより意味的な理解をCinemagraph生成に導入
          li 個人的な嗜好に沿ったCinemagraphの自動生成を実現
    .item4
      h1 自由記述欄
      .text
        ul
          li ユーザの嗜好を学習するためにデータにスコア付けしてもらうなど，CVよりはMultimediaっぽい論文
          li: a(href="http://web.mit.edu/taehyun/www/Research/Cinemagraph/SuppleWeb.htm") 結果サンプル
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara
  +slide
    .title Multi-Agent Cooperation and the Emergence of (Natural) Language
    .info
      .authors Angeliki Lazaridou, et al.
      .conference ICLP 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p マルチエージェント間の対話による言語学習を提案。SenderエージェントとReceiverエージェント間で簡単な画像当てゲームを実施。ゲームの正解のためにより良質なコミュニケーションが必要となり、言語を学習していく。また、ゲーム環境を変化させることで、単語の意味と画像がより良く対応するようになる。
    .item2
      .text
        p
          img(src=`${figpath}180305MultiAgent.jpg`)
    .item3
      .text
        h1 手法
        p Senderエージェントは、2枚の画像のうち1枚がtargetであると伝えられる。そして、これを伝えるためにReceiverエージェントにsymbol(メッセージ)を送信する。Receiverエージェントは、受信したsymbolの情報のみから、どちらの画像がtargetであるかを当てる。
        p SenderとReceiverに見せる画像を変える実験や、人がゲームを実施する実験を行った。
    .item4
      .text
        h1 結果・リンク
        p 人間と生産的にコミュニケーションできるAIの開発に貢献できる。言語の習得には、大量のデータだけでなく、他者との対話が重要。また、Senderが出力したsymbol(Image Netのラベルに対応したもの)を人間に見せると68%の正解率となった。
        ul
          li
            a(href="https://arxiv.org/pdf/1612.07182.pdf") 論文
          
    .slide_index #{getSlideIndex()}

  +slide
    .title Turning an Urban Scene Video into a Cinemagraph
    .info
      .authors H. Yan et al.,
      .conference CVPR 2017
    .item1
      h1 概要
      .text.
        Cinemagraph（画像中の一部だけ動画）を自動生成するための手法を提案．
        Warpingして動画中の視点を固定した後，セグメンテーションをかけてからDynamicな領域を見つけて，
        そこだけ動くようにしてCinemagraphを生成．
        街中のシーンで光やディスプレイだけが動くようなCinemagraphを自動的に生成することを可能にした．

    .item2
      img(src=figpath+"180302_turning_cinemagraph.png",alt="180302_turning_cinemagraph.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 街中で普通に撮影した動画から自動的なCinemagraphの生成を実現
          li ノイジーなWarping動画でも有効な動き解析手法を提案
          li: a(href="https://www.youtube.com/watch?v=r3yyL6qrVX4") サンプル動画
    .item4
      h1 自由記述欄
      .text
        ul
          li 特に定量的な評価はなくて，サンプルを出してうまくいっているでしょ，というやり方
          li 失敗例を出してLimitationまで議論しているけど，こういうのはCVPRだと珍しい気がする
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara
  +slide
    .title A Read-Write Memory Network for Movie Story Understanding
    .info
      .authors Seil Na, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 大規模でマルチモーダルの映画ストーリー理解のためのMovieQA を解く。新しいメモリネットワークモデルのRWMN(ReadWrite Memory Network)を提案。一連のフレームを段階的に抽象化して、より高レベルの順次情報を取得し、それをメモリスロットに格納していく。CNNを多用し、読み取りネットワークと書き込みネットワークを設計。これにより、メモリの読み書き操作に高い容量と柔軟性を持たせることができる。
    .item2
      .text
        p
          img(src=`${figpath}180304ReadWriteMemory.jpg`)
    .item3
      .text
        h1 手法
        p Embedding：ResNetとWord2Vecを用いて映画の埋め込みを行う。
        p Write： CNNを書き込みネットワークとして利用し、メモリテンソルを出力。
        p Read： CNNを使用して、一連のシーン全体をつなぎ合わせて関連付けるために、シーケンシャルメモリスロットにチャンクごとにアクセス。構成されたメモリMrを得る。
        p QA： 5つの候補中から最も信頼度のが高い回答を選ぶ。
    .item4
      .text
        h1 結果・リンク
        p ストーリーのコンテンツだけでなく、キャラクターとその行動についての理由など、より抽象的な情報を理解できる可能性を示唆。
        ul
          li
            a(href="https://arxiv.org/pdf/1709.09345.pdf") 論文
          li
            a(href="https://github.com/seilna/RWMN") ソースコード
          
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Sequence to Sequence – Video to Text
    .info
      .authors Subhashini Venugopalan, et al
      .conference ICCV 2015
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p ビデオのキャプションを生成するためのend-to-endかつ、sequence-to-sequenceモデルの提案。本手法のS2VTによって、一連のフレームを一連の単語に直接マッピングし、学習することができる。入力フレームの可変数の扱い、ビデオの時間構造を学習、自然な文法文の生成、この3点が本研究のコントリビューション。
    .item2
      .text
        p
          img(src=`${figpath}180304VideoToText.jpg`)
    .item3
      .text
        h1 手法
        p 各フレームのCNNの出力と、連続したLSTMに入力する。また、ビデオの時間構造をモデル化するためにオプティカルフローを算出し、フロー画像もCNNを介してLSTMに入力する。全てのフレームを読み込んだ後に、単語単位で文章を生成する。
        p 使用データセット：MSVD, M-VAD, MPII Movie Description
    .item4
      .text
        h1 結果・リンク
        p 評価は機械翻訳に使われるMETEORで行う。フレームの順序をランダムにした場合、スコアがかなり低減したことから、時間的構造を利用したキャプションの生成ができていることを示唆。
        ul
          li
            a(href="https://arxiv.org/pdf/1505.00487.pdf") 論文
          li
            a(href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt") ソースコード
          
    .slide_index #{getSlideIndex()}

  +slide
    .title Deformable Convolutional Networks
    .info
      .authors Jifeng Dai, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p CNNの表現力の向上を図る。CNNによる物体検出などでは、矩形を用いるために検出対象の物体だけでなく、余計な背景も含んでしまい精度低下につながる。可変可能な畳み込みとRoIプーリングを提案。これにより、画像の畳み込みを行う際に、重みに加えてセルの位置も学習する。特に、物体検出やセマンティックセグメンテーションなどのタスクに効果的。
    .item2
      .text
        p
          img(src=`${figpath}180304DeformableConvolution_1.jpg`)
    .item3
      .text
        h1 手法・結果・リンク
        p 変形可能な畳み込み：規則的(矩形)にセルをサンプリングする標準の畳み込みに、オフセットを追加することで、自由形状変形を可能にする。オフセットは追加の畳み込みレイヤを介し、前のfeature mapから学習可能。
        p 可変可能なRoIプーリング：RoIプーリング時の各binの位置にオフセットを追加する。畳み込みと同様に、前のfeature mapから学習可能。
        p 様々な条件での実験を実施。どの条件でも提案手法の精度が高い結果となった。
        ul
          li
            a(href="https://arxiv.org/pdf/1703.06211.pdf") 論文
          li
            a(href="https://github.com/msracver/Deformable-ConvNets") ソースコード
    .item4
      .text
        p
          img(src=`${figpath}180304DeformableConvolution_2.jpg`)
          
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Image Captioning with Sentiment Terms via Weakly-Supervised Sentiment Dataset
    .info
      .authors Andrew Shin, et al.
      .conference BMVC 2016
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 画像キャプショニングの中でも、画像上にはない形容詞で表現された“感情”についてのキャプションに焦点を当てる。センチメントタームを用いた画像キャプションモデルを提案。これにより、センチメントの主観的性質に対応するマルチラベル学習を実現。FlickrとDeviantArtから、2.5Mの画像と28Mのコメントを収集し，感情に対するデータセットを構築。“コメント”はキャプションとは性質が異なるが、感情を表現するために適している(否定や不適切を除く)。
    .item2
      .text
        p
          img(src=`${figpath}180304CaptioningWithSentiment.jpg`)
    .item3
      .text
        h1 手法
        p CNN→LSTMという一般的な画像キャプションの流れに、センチメント分析を行うCNNを追加する。SentiWordNetの正または負のスコアが0.5以上の単語を感情単語とする。
        p SentiWordNet：意見聴衆のための語彙リソース。正、負、客観性の3つの感情スコアを算出。
    .item4
      .text
        h1 結果・リンク
        p キャプションが適切出るかどうかと、キャプションのランク付けの2つの人間による評価。モデルからのキャプションがイメージの感情に関してより適切であるという結果となった。
        ul
          li
            a(href="http://www.bmva.org/bmvc/2016/papers/paper053/paper053.pdf") 論文
          li
            a(href="http://nmis.isti.cnr.it/sebastiani/Publications/LREC10.pdf") SentiWordNet

    .slide_index #{getSlideIndex()}

  +slide
    .title Representation Learning by Learning to Count 
    .info
      .authors Mehdi Noroozi et al. 
      .conference ICCV 2017 (Oral)
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 
        |「画像内のprimitiveを認識できることは高次の特徴を掴んでいる」という考えを基にした、self-supervisedな特徴表現学習手法。
        |画像のオリジナルとそれらを各タイルに分割したものを同じNNに入力し、出力されるタイルのprimitive数の和とオリジナルのprimitive数が一致するように学習する。
        |しかしそれでは出力を単に小さくするように学習することで損失を０にできてしまうので異なる画像も含めたcontrastiveな損失を用いる。
          
    .item3
      .text
        h1 新規性・結果
        p 
        |画像識別、物体検出、意味領域分割などのタスクで評価を行っており、識別ではSoTA。
        |学習したNNからの出力を確認すると、ノルムが大きいものは高次な物体が含まれる画像、小さいものは低次なテクスチャしか含まない画像が得られた。
        |これからNNが高次なprimitiveをcountしていることが考察できる。
        
    .item2
      .text
        p
          img(src=`${figpath}count1.png`)
          img(src=`${figpath}count2.png`)
          img(src=`${figpath}count3.png`)
        
    .item4
      .text
        h1 自由記述欄
        p
        |損失を最小化することで結果的にNNが「何かしらのprimitiveを数えていること」になり、冒頭の考えと合わせることで特徴表現学習が可能となる。 
        |何か明示的に数える対象を与えるように想像したが、実際に何を数えているかは学習ベースで、明示的には与えていない点が非常に面白い。

        ul
          li
            a(href="https://arxiv.org/abs/1708.06734") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Visual Storytelling
    .info
      .authors Ting-Hao (Kenneth) Huang, et al.
      .conference NAACL 2016
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p アルバムのような時系列画像でキャプション生成を行うためのデータセット。ストーリ性のある画像キャプションデータセット：SINDを構築。10,117個のFlickrアルバム、210,819枚の写真。各アルバムは平均20.8枚。
        p descriptions for images in isolation (DII)：画像一枚の記述
        p descriptions of images in sequence (DIS)：連続画像
        p stories for images in sequence (SIS)：ストーリー
    .item2
      .text
        p
          img(src=`${figpath}180303VisualStorytelling_1.jpg`)
    .item3
      .text
        h1 手法・リンク
        p SINDを使いキャプションをイメージごとに生成(Table5)。ストーリー性を含んだキャプションが生成できている。METEORによるスコアも向上。
        ul
          li
            a(href="https://arxiv.org/pdf/1604.03968.pdf") 論文
    .item4
      .text
        p 
          img(src=`${figpath}180303VisualStorytelling_2.jpg`)

    .slide_index #{getSlideIndex()}

  +slide
    .title Inferring and Executing Programs for Visual Reasoning
    .info
      .authors Justin Johnson, Bharath Hariharan, Laurens van der Maaten, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 理由に基づいたVQA。既存の手法では，入力を出力に直接マッピングしているため，視覚的推論の学習というよりも，データの偏りを学習しているといえる。そこで，理由を伴った視覚的推論モデルを提案。モデルは、プログラムジェネレータと実行エンジンの2部構成。CLEVRベンチマークを使用し評価。回答の柔軟性、拡張性の向上。
    .item2
      .text
        p
          img(src=`${figpath}180303VisualReasoning_1.jpg`)
    .item3
      .text
        h1 手法・リンク
        p プログラムジェネレータは、質問の読み取り、単語の羅列として表現される質問から質問に答えるためのプログラムを生成する。基本的にはLSTMのsequence-to-sequenceの考え方。
        p 実行エンジンは、予測されたプログラムをミラーリングするニューラルモジュールネットワークを構成し、実行することで画像から回答を生成。
        ul
          li
            a(href="https://arxiv.org/pdf/1705.03633.pdf") 論文
          li
            a(href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf") seq-to-seq
    .item4
      .text
        p 
          img(src=`${figpath}180303VisualReasoning_2.jpg`)

    .slide_index #{getSlideIndex()}
    
  +slide
    .title Deep mutual learning 
    .info
      .authors Ying Zhang, Tao Xiang, Timothy M. Hospedales, Huchuan Lu
      .conference 2017
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 教師モデルと生徒モデルを分けていた従来の蒸留に対してモデル同士の相互学習を提案。ハードラベルによる交差エントロピーと対象モデル以外のモデルの出力とのKL距離を最小化するように学習する。様々なモデル同士の相互学習実験や通常の蒸留との比較、相互学習を行った場合の解がより高い汎化性能を保有していることの検証実験も行っている。

        
    .item2
      .text
        p
          img(src=`${figpath}deep_mutual2.png`)
          img(src=`${figpath}deep_mutual3.png`)
          
    .item3
      .text
        h1 新規性・結果
        p 画像識別において通常の蒸留を行うよりも精度が良くなった。生徒モデルの中で相対的に小規模なモデルのみならず大規模なモデルも独立で学習を行うより精度が良かった。さらに相互学習を行うことで、wider minimaに収束しているという実験結果萌えたれた。特に出力される事後確率のエントロピーが大きくなるように学習されることがwider minimaへの収束を促していることがいわれている。
        
    .item4
      .text
        h1 自由記述欄
        p ハードラベルありき（ないと相互学習が正しい方向に向かわない）の手法であったが、教師なし手法に拡張できたら面白くなりそうだと感じる。
        ul
          li
            a(href="https://arxiv.org/abs/1706.00384") 論文
    .slide_index #{getSlideIndex()}
  
  +slide
    .title Learning Features by Watching Objects Move 
    .info
      .authors Deepak Pathak, Ross Girshick, Piotr Dollar, Trevor Darrell, and Bharath Hariharan
      .conference CVPR 2017
    .slide_editor Tomoyuki Suzuki

    .item1
      .text
        h1 概要
        p 動き特徴を利用した前景（物体）領域情報は汎用的な表現学習に役立つという考えから、NLCなどのhand-craftな手法を組み合わせて擬似的な動体領域を作成し、それを教師として領域分割をCNNに解かせることで表現特徴を得る。物体検出、物体・行動認識、意味領域分割の問題設定において評価を行った。表現学習のデータとしてYFCCを用いている。
        
    .item2
      .text
        p
          img(src=`${figpath}watching_object.png`, alt="180302AffordanceNet")
          img(src=`${figpath}watching_object3.png`, alt="180302AffordanceNet")
          img(src=`${figpath}watching_object2.png`, alt="180302AffordanceNet")
          
    .item3
      .text
        h1 新規性・結果
        p Pascal VOCの物体検出において教師なし表現学習でSoTA。特にfine-tuningに利用するデータが少量の場合と多くの層のパラメータを固定してfine-tuinigした場合で大きな効果を発揮した。しかし、物体・行動認識、意味領域分割においては従来手法より劣っている。
        
    .item4
      .text
        h1 自由記述欄
        p 実験は丁寧に行われてる印象。表現学習の設定自体が物体検出を意識しているようにも感じられ（単一物体が写っている画像を優先的に取り出している？など）、物体検出でうまくいくのは当たり前な気もした。
        |しかし、意味領域分割で精度が出ない原因がよくわからなかった（物体部分はできているが背景の分割ができていない？）。
        ul
          li
            a(href="https://arxiv.org/abs/1612.06370") 論文
    .slide_index #{getSlideIndex()}
    
  +slide
    .title Focal Loss for Dense Object Detection
    .info
      .authors Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar, Facebook AI Research (FAIR)
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 1-stage物体検出手法の精度向上を図る。 YOLOやSSDなどは，矩形領域における前景と背景の面積が不均衡であるため，2-stage物体検出手法に勝てないと推測。この問題を解決するためにクロスエントロピーを再構築したFocal Lossを提案。学習時のネガティブサンプルの影響を減らすことができる。
    .item2
      .text
        p
          img(src=`${figpath}180303FocalLoss.jpg`)
    .item3
      .text
        h1 手法
        p Focal Loss
        p クロスエントロピーに重みを追加
        ul
          li 正解の場合には重みを低減
          li 不正解の場合には従来通り
        p →正解になりやすい背景に引っ張られなくなる
    .item4
      .text
        h1 結果・リンク集
        p 既存の最先端2ステージ検出器(2017年現在)の全てにおいて精度を上回り，既存の1ステージ検出器の検出速度と同等
        ul
          li
            a(href="https://arxiv.org/pdf/1708.02002.pdf") 論文
          li
            a(href="https://github.com/facebookresearch/Detectron") ソースコード

    .slide_index #{getSlideIndex()}

  +slide
    .title Towards Diverse and Natural Image Descriptions via a Conditional GAN
    .info
      .authors Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin, et al.
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p 画像キャプショニングの性能向上を図る。従来のロバスト性が低いRNNに代わって，GANのフレームワークを採用することで，自然性と多様性を向上。図より，ジェネレータ(G)が文を生成し，ディスクリミネータ(E)が文や段落がどれだけうまく記述されているかを評価する。GとEを同時に学習させることにより，自然な文章を生成。
    .item2
      .text
        p
          img(src=`${figpath}180303TowardsDiverse_1.jpg`)
    .item3
      .text
        h1 結果・リンク集
        ul
          li 人間，G-MLE，G-GAN(本手法)の3つを比較して性能評価
          li ユーザー調査、定性的な例、および検索アプリケーションなどの評価により，より自然かつ多様、意味的に関連する記述を実現
        ul
          li
            a(href="https://arxiv.org/pdf/1703.06029.pdf") 論文
          li
            a(href="https://arxiv.org/abs/1411.4555") MLE
    .item4
      .text
        p
          img(src=`${figpath}180303TowardsDiverse_2.jpg`)

    .slide_index #{getSlideIndex()}

  +slide
    .title Learning to Segment Every Thing
    .info
      .authors Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ラベルが完全に手に入らない際にでも転移学習が可能なセグメンテーション手法（論文中ではPartially Supervised Training Paradigm, weight transfer functionを紹介）を提案する。条件として、bboxが手に入っている物体に対してセグメンテーション領域を学習可能。Mask R-CNNをベースとしているが、Weight Transfer Functionを追加、セグメントの重みを学習・推定して誤差計算と学習繰り返し。
    .item2
      .text
        p
          img(src=`${figpath}180303SegmentEverything.png`,alt="180303SegmentEverything")
    .item3
      .text
        h1 新規性・結果
        p Visual Genome Datasetから3,000の視覚的概念を獲得、MSCOCOから80のマスクアノテーションを獲得した。
    .item4
      .text
        h1 コメント・リンク集
        p 弱教師付き学習が現実的な精度で動作するようになってきた？アノテーションはお金や知識があっても非常に大変なタスクであり、いかに減らすかという方向に研究が進められている。（What's next?ー弱教師/教師なしの先とは？）
        ul
          li
            a(href="https://arxiv.org/pdf/1711.10370.pdf") 論文
          li
            a(href="http://ronghanghu.com/") 著者
          li
            a(href="http://kaiminghe.com/") Kaiming He
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.3 10:46:40

  +slide
    .title One-Shot Visual Imitation Learning via Meta-Learning
    .info
      .authors Chelsea Finn*, Tianhe Yu*, Tianhao Zhang, Pieter Abbeel, and Sergey Levine
      .conference NIPS 2017
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p ワンショットで人間の教示を模倣するロボットのための学習「One-shot Imitation Learning」を提案。人間が物体を把持するなど動作を教示するとロボットが特徴や動作を学習してタスクをこなす様子を学習。Model-Agnostic Meta-Learning（MAML; ICML2017）を応用したモデルを提案し、（VR空間、人間のデモによる）教示から動作を学習する。アーキテクチャはCNNをベースとしてRGB入力から特徴を抽出、中間層（全結合層直前）からロボットの動作やバイアス項を入力してロボットの行動（pre-/post-update）を出力する。
    .item2
      .text
        p
          img(src=`${figpath}180303OneshotImitation.png`,alt="180303OneshotImitation")
    .item3
      .text
        h1 新規性・結果
        p ロボットの把持タスクをシミュレーション/実空間にておこなった。シミュレーションでは提案法のMILが1ショットで85.81%、5ショットで88.75%（従来法LSTMでは各78.38%, 83.11%）。実空間では90%を実現（従来法LSTM/contextualでは25%）。詳細にはプロジェクトページやプレゼンのビデオを参照。
    .item4
      .text
        h1 コメント・リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1709.04905.pdf") 論文
          li
            a(href="https://sites.google.com/view/one-shot-imitation") Project
          li
            a(href="https://github.com/tianheyu927/mil") GitHub
          li
            a(href="https://vimeo.com/252186304") Presen
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.3 10:13:42

  +slide
    .title BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography
    .info
      .authors Michael J. Wilber, Chen Fang, Hailin Jin, Aaron Hertzmann, John Collomosse, Serge Belongie
      .conference ICCV 2017
    .slide_editor Munetaka Minoguchi

    .item1
      .text
        h1 概要
        p CVにおけるデータセットでは，写真に対するラベル付けが一般的。写真だけでなく，イラストや風景画などに対して，以下の3属性のラベルを付加。
        ul
          li メディア：漫画、油絵、鉛筆スケッチ、水彩画などで作成した画像にラベル付け
          li 感情：視聴者に平静、幸せ/陽気、悲しい/悲観的、恐ろしい/恐れを感じさせるような画像にラベル付け
          li オブジェクト：自転車、車、猫、犬、花、人などの画像にラベル付け
    .item2
      .text
        p
          img(src=`${figpath}180302BehanceArtisticMedia.jpg`)
    .item3
      .text
        h1 手法
        p Behance Artistic Media Dataset
        ul
          li ラベル作成にhuman-in-the-loopを採用し，人間とコンピュータのハイブリッドを図る。
          li 全てのラベルについて学習し，ランク付けを行う。その後，高い順位の画像を人がラベル付け。これを4回繰り返す。
          li 基本的にはLSUNの方法に基づいている(リンク参照)
    .item4
      .text
        h1 結果・リンク集
        p 物体認識や物体検出，画像の類似度，属性推定など様々なタスクの機械学習実験を実施。定性的な評価にはなるが，明らかにVOCやImageNetなどの既存のデータセットよりも広い表現の画像で多くのタスクが処理可能となる。
        ul
          li
            a(href="https://arxiv.org/pdf/1704.08614.pdf") 論文
          li
            a(href="http://behance.net") Behence
          li
            a(href="https://arxiv.org/abs/1506.03365") LSUN
    .slide_index #{getSlideIndex()}

  +slide
    .title AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection
    .info
      .authors Thanh-Toan Do, Anh Nguyen, Ian Reid, Darwin G. Caldwell, Nikos G. Tsagarakis
      .conference ICRA 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 物体検出とアフォーダンス（というよりは機能？）のセグメントを同時に回帰するネットワーク、AffordanceNetに関する論文。ロボットへの把持位置/機能教示を行うことができる。基本的なモデルはMask R-CNNを適用していて、物体検出のためのbboxと物体に対する機能セグメントを正解として学習する。多タスクの誤差関数は物体カテゴリ、座標、機能セグメントの3つに関するものである。
    .item2
      .text
        p
          img(src=`${figpath}180302AffordanceNet.png`,alt="180302AffordanceNet")
    .item3
      .text
        h1 新規性・結果
        p 従来、物体検知と機能推定は別個に学習・認識されていたが、本研究では多タスク学習の枠組みで、単一モデルにてEnd-to-End学習した。IIT-AFF Datasetにて73.35（SoTAは69.62）、UMD Datasetにて79.9（SoTAは77.0）。モデルも公開されており、誰もがAffordanceNetを実装できるようにしている。
    .item4
      .text
        h1 コメント・リンク集
        p 任意のセグメンテーションラベルさえあれば、物体検知とあらゆる高次なセグメンテーションモデルが実現可能となった。
        ul
          li
            a(href="https://arxiv.org/pdf/1709.07326.pdf") 論文
          li
            a(href="https://github.com/nqanh/affordance-net") GitHub
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 20:38:13


  +slide
    .title Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN
    .info
      .authors Jiyang Gao, Zijian (James) Guo, Zhen Li, Ram Nevatia
      .conference arXiv:1711.07607
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 画像識別における知識蒸留（Knowledge Distillation）の内容である。本論文ではある概念（e.g. 動物認識、人工物認識）ごとに教師となる識別器を事前学習しておき、それらの知識を単一の識別器に学習（これをKnowledge Concentrationと呼称）する。いわば複数の先生がある生徒に教えるという流れで学習する。
    .item2
      .text
        p
          img(src=`${figpath}180302KnowledgeConcentration.png`,alt="180302KnowledgeConcentration")
    .item3
      .text
        h1 新規性・結果
        p よくも悪くも、現状のCVはImageNetの1,000カテゴリに頼っているが、これを100倍の100,000カテゴリに増やして学習したらどうか？また、いかにしたら効率よく学習ができるかを検討した。結果はSingle Model（従来のようなCNNによるカテゴリ識別学習）にするよりも複数のスペシャリスト識別器から知識蒸留を行う方が効率よく、精度よく学習ができた。本論文で使用したEFT（Entity-Foto-Tree）データセットはカテゴリ数でImageNetの100倍、JFT-300Mの5倍である。
    .item4
      .text
        h1 コメント・リンク集
        p 読んだだけでGoogleの研究所であることがわかってしまう論文。実行ができるかどうかはさておき、大規模データの扱いやCNN学習の効率化という意味でも精読すべき論文。
        ul
          li
            a(href="https://arxiv.org/pdf/1711.07607.pdf") 論文
          li
            a(href="https://jiyanggao.github.io/") 著者
          li
            a(href="http://codecrafthouse.jp/p/2018/01/knowledge-distillation/") 知識蒸留の参考
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 20:04:42


  +slide
    .title We Are Humor Beings: Understanding and Prediction Visual Humor
    .info
      .authors Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Virginia Tech, TTI-Chicago, Facebook AI Research
      .conference CVPR 2016
    .item1
      h1 概要
      .text
        ul
          li 視覚とユーモアの関係をモデル化(不調和説に基づく)
          li アニメ画像の面白さを推定
          li 画像のオブジェクトと面白さの関連性を推定
          li データセットの作成
          li 抽象的なシーンを使用したユーモアを引き起こすシーンの理解
    .item2
      img(src=figpath+"180205HumorBeings.jpg")
    .item3
      h1 手法1
      .text
        p 面白さ推定
        ul
          li 特徴抽出し，重み付き誤差平均(面白さによって誤差を変える)
          li Abstract Scenesのアニメ画像(VQAの画像とクラウドソーシングで依頼した面白い画像)に5段階評価したデータセット(AVH)
    .item4
      h1 手法2
      .text
        p 面白い画像・面白くない画像の変換
        ul
          li オブジェクトを変更することで，面白い⇔面白くない画像に相互変換
          li AVHから面白いものを選び，クラウドソーシングで依頼した面白くない画像を使い/面白い画像ペアを作成
          li どのオブジェクトが面白さに影響しているか調査
        p 結果：特に人や動物などのオブジェクトが面白さに影響
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Munetaka Minoguchi

  +slide
    .title Vehicle Three-Dimensional Pose and Shape Estimation from Multiple Monocular Vision
    .info
      .authors Wenhao Ding, Shuaijun Li, Guilin Zhang, Xiangyu Lei, Huihuan Qian, Yangsheng Xu
      .conference arXiv:1802.03515
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p オーバーラップが少ない複数視点カメラから自動車の3次元姿勢や形状を復元する研究。CNNにより自動車のキーポイントや姿勢/3次元形状を出力する。これら情報をヒントに、カメラ視点を推定する。2D画像上でのキーポイント推定にはconv-de-convを4回繰り返すhourglassアーキテクチャを採用、3次元姿勢や形状の推定にはCross Projection Optimization (CPO)を採用し2D-3Dの投影誤差を最小化した。
    .item2
      .text
        p
          img(src=`${figpath}180302Vehicle3DPose.png`,alt="180302Vehicle3DPose")
    .item3
      .text
        h1 新規性・結果
        p 物体のキーポイント検出においてState-of-the-art。6DoF推定手法 (Pavlakos, ICRA17)では12キーポイントの平均誤差が37.88であったが、提案手法では10.48まで低減した。また、回転/並進誤差も3DVP (Xiang+, CVPR15)では11.18/N/Aであったが、2.87/4.73まで向上させた。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1802.03515.pdf") PDF
          li
            a(href="https://arxiv.org/abs/1703.04670") 6-DoF Object Pose from Semantic Keypoints
          li
            a(href="http://cvgl.stanford.edu/papers/xiang_cvpr15_3dvp.pdf") Data-Driven 3D Voxel Patterns for Object Category Recognition
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 10:15:30


  +slide
    .title Joint Event Detection and Description in Continuous Video Streams
    .info
      .authors Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko
      .conference arXiv:1802.10250
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p End-to-Endでイベント検出（行動の時系列セグメント化）とキャプショニングを実行するタスクを提供する。モデルには3D Convolutionや階層的LSTM（two-level hierarchical LSTM）を採用した。基本的には
          a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
          |をベースにして研究を行なっているが、{Controller, Captioner} LSTMの二段階により前の候補のセンテンスやビデオコンテキストを考慮しつつ状態を更新（Controller LSTM）し、候補領域の特徴を参照しつつキャプションを生成（Captioner LSTM）する。時系列候補領域とキャプションはmulti-task学習、End-to-Endで学習される。
    .item2
      .text
        p
          img(src=`${figpath}180302VideoCaption.png`,alt="180302VideoCaption")
    .item3
      .text
        h1 新規性・結果
        p 候補領域生成やキャプショニングの精度を検証した。また、データセットにはActivityNet CaptionsやTACoS Datasetを用いた。候補領域については従来法のDAPが30, multi-scale DAPが38 @AUC (IoU>0.8)に対して提案法であるJEDDi-Netは58.21を記録した。また、キャプショニングについては従来法が{17.95, 4.82, 17.29} (各BLEU1, METEOR, CIDEr)の問題に対して{19.97, 8.58, 19.88}を記録した。
    .item4
      .text
        h1 コメント/リンク集
        p 動画のタスクはカテゴリのみでなく言語やより表現力豊かな認識ができなければいけない時期になって来た？時系列表現にもまだまだ課題が多いので、これからさらに動画認識にチャレンジすべき。また、キャプショニングの問題は感性評価に対する知見や確固たる評価方法が確立されるとさらに面白くなるのではないか。
        ul
          li
            a(href="https://arxiv.org/pdf/1802.10250.pdf") PDF
          li
            a(href="https://www.bu.edu/cs/profiles/kate-saenko/") Kate Saenko
          li
            a(href="https://cs.stanford.edu/people/ranjaykrishna/densevid/") Dense Captioning Events in Videos
          li
            a(href="https://github.com/kenshohara/3D-ResNets-PyTorch") 3D Convolution (3D-ResNets-PyTorch)
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 09:36:26

  +slide
    .title Neural Aesthetic Image Reviewer
    .info
      .authors W. Wang, et al.
      .conference arXiv:1802.10240
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p AIが写真の感性評価やコメント生成を行なってくれる。写真とそのコメントが対応づけられた大規模DBであるAVA-Reviews dataset（52,118画像、312,708コメント）を学習することで写真を入力して図の(1)Predictionや(2)Commentsのようなものが得られる。モデルはCNNにより感性評価（Low-/High-Aesthetic category）を、CNN+LSTM（RNN）によりコメント（e.g. Fastastic colors）を出力する。
    .item2
      .text
        p
          img(src=`${figpath}180301NeuralAesthetic.png`,alt="180301NeuralAesthetic")
    .item3
      .text
        h1 新規性・結果
        p (i) 人間のような画像に対する感性評価（image aesthetics）をコンピュータに実装した。(ii)自然言語の出力により人間の高次な認知能力を実現。(iii) 画像-言語の組み合わせによるデータセットAVA-Reviews datasetを新規に構築した。
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/pdf/1802.10240.pdf") PDF
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.2 08:39:04


  +slide
    .title Interpreting CNN Knowledge via an Explanatory Graph
    .info
      .authors Q. Zhang, et al.
      .conference AAAI 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 深層学習の解釈性に関する論文であり、畳み込み層の特徴マップの応答を外的に解析して対応する反応を可視化。
          |畳み込みの各フィルタが異なる部位（e.g. 馬の耳や目）に反応するので、グラフにより解析して元画像の対象位置にアクセス。

    .item2
      .text
        p
          img(src=`${figpath}180301interpretability.png`,alt="180301interpretability.png")
    .item3
      .text
        h1 新規性・結果
        p Ground-truthなしに各部位に関する解釈性を与えたことが新規性である。
          |図に示すように入力画像に対するパーツごとの解析をフィルタの反応やグラフの解析から可視化することができる。
          |さらに、異なる画像間においても一貫性のある反応を得ることができた。
    .item4
      .text
        h1 自由記述欄
        p 深層学習は教師なしによる解釈性を獲得しているが、まだ反応している部分の可視化や部分ごとの解析が進んでいるにすぎない。
          |さらなる発展のためには、言語的な解釈や人間にわかりやすい加工（イラストとか？）が必要になるのではないだろうか。
    .slide_index #{getSlideIndex()}

  +slide
    .title HemaApp: Noninvasive Blood Screening of Hemoglobin using Smartphone Cameras
    .info
      .authors E. J. Wang et al.,
      .conference Ubicomp 2016
    .slide_editor Kensho Hara

    .item1
      .text
        h1 概要
        p スマホカメラを使って非侵襲なヘモグロビン濃度の測定を実現．
          |血中の酸素飽和度の測定などはこれまでにもあったがヘモグロビン濃度まで測定できているものはなかった．
          |照明条件とRGBの変化からヘモグロビン濃度を推定するためのアルゴリズムを提案．
    .item2
      .text
        p
          img(src=`${figpath}180228_hemaapp.png`,alt="180228_hemaapp.png")
    .item3
      .text
        h1 新規性・結果
        p 特別な装置を使うことなく簡単にスマホカメラでヘモグロビン濃度測定を実現した点．
    .item4
      .text
        h1 自由記述欄
        p システムやユーザスタディの完成度や完全に実現できたときの有用性が評価された？
    .slide_index #{getSlideIndex()}
    .timestamp 2018.3.1 10:44:03

  +slide
    .title End-to-end Driving via Conditional Imitation Learning
    .info
      .authors Felipe Codevilla, Matthias Müller, Alexey Dosovitskiy, Antonio López, and Vladlen Koltun,
      .conference arXiv
      .paper_id 1710.02410
    .item1
      h1 概要
      .text.
        自動運転を模倣学習により行う手法を提案。実空間での学習結果をヴァーチャルな空間での自動運転にて再現することができた。RGB画像、計測（e.g. スピード）や命令（e.g. turn right）などからステアリング、アクセル、ブレーキなどのコマンドを出力して自動車を操作する。
    .item2
      img(src=figpath+"180205conditionalimitationlearning.png")
      img(src=figpath+"180205framework_imitation.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 模倣学習による自動運転を実現した。
          li 実空間とシミュレーションベースの転移を行うことにも成功。
    .item4
      h1 リンク集
      .text
        ul
          li 自動運転の学習はシミュレーションベースで完結してしまう可能性がある。
          li メタ学習/模倣学習/強化学習などはCVに徐々に取り入れられてくるはずなので、2018年は学習しておいて損はない？
          li: a(href="https://arxiv.org/pdf/1710.02410.pdf" target="blank") [論文] End-to-end Driving via Conditional Imitation Learning
          li: a(href="https://www.youtube.com/watch?v=cFtnflNe5fM" target="blank") YouTube
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

  +slide
    .title Open3D: A Modern Library for 3D Data Processing
    .info
      .authors Qian-Yi Zhou et al.,
      .conference arXiv
      .paper_id 1801.09847
    .item1
      h1 概要
      .text
        |3Dデータを取り扱い、迅速な開発を可能にする
        a(href="http://www.open3d.org/" target="blank") Open3D
        |を提供する。Open3DはC++/Pythonをサポート、並列化にも対応しており、クラウドで開発することが可能。
        |点群読み込み-ダウンサンプリング-法線の計算、シーン再構築、3次元可視化などの処理が含まれている。
    .item2
      img(src=figpath+"180205open3d.png")
    .item3
      h1 新規性・結果
      .text
        |3次元画像処理のコミュニティにて有益なオープンソースを提供し、その
        a(href="https://github.com/IntelVCL/Open3D" target="blank") コード
        |も提供されている。
    .item4
      h1 コメント・リンク集
      .text
        ul
          li: a(href="http://www.open3d.org/" target="blank") Project page
          li: a(href="https://arxiv.org/pdf/1801.09847.pdf" target="blank") [論文] Open3D: A Modern Library for 3D Data Processing
          li: a(href="https://github.com/IntelVCL/Open3D" target="blank") Code
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

  +slide
    .title Hierarchical Variational Autoencoders for Music
    .info
      .authors A. Roberts et al.,
      .conference NIPS WS on Machine Learning for Creativity and Design, 2017.
    .item1
      h1 概要
      .text
        | 音楽を生成するためのHierarchical Variational Autoencoders (VAE) を提案．
        | エンコーダとデコーダがLSTMで構成されているReccurent VAEがベース．
        a(href="https://goo.gl/twGuP2") 結果サンプル
        | 長い音楽（実験では32小節）を単純なLSTMデコーダで生成するのは難しいので，
        | この研究では複数のLSTMを階層的に重ねて，段階的に長くしていくHierarchical VAEを提案．
        | ループメロディの外挿や，メロディの生成，3ピース構成の音楽生成の実験で性能を検討．
        | 結果の音楽やコードは公開されている．
    .item2
      img(src=figpath+"hierarchical_vae.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 階層的なLSTMによるデコーダをVAEによる音楽生成に導入
          li
            a(href="https://goo.gl/twGuP2") 結果サンプル
    .item4
      h1 自由記述欄
      .text
        ul
          li これも長期的な構成を考えて生成することはできていない
          li Future Workにはインタフェースを作るとあるし，1曲まるごと作るというよりは適当にサンプルを出して作曲家のアイデアを促進することを目指しているのかな．
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title Generating the Future with Adversarial Transformers
    .info
      .authors C. Vondrick et al.,
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        未来の動画を予測して生成する手法を提案．
        4フレーム x 64画素 x 64画素のクリップを入力として，その後の16フレームの動画を生成．
        完全に新しいフレームを生成するのは難しいので，入力フレームの変換により未来のフレームを生成するのがポイント．
        論文の主張としては，きれいな動画を作るにはLow-Levelな情報が重要だけど，未来予測のためにはHigh-Levelな理解も必要で，
        その両者を一つのネットワークで一気に学習するのは難しいとしている．
        だから，Low-Levelな情報は元のフレームを変換することで引っ張ってきて，ネットワークはHigh-Levelな特徴抽出に集中させるのが良いとのこと．
        このネットワークの学習はGANベース．
        生成動画の主観評価や可視化，Generatorの特徴を利用した物体認識タスクなどで性能を評価．

    .item2
      img(src=figpath+"generating_future.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 元のフレームからの変換により未来のフレームを生成する手法を提案
          li 未来の動画生成において，敵対的学習により大規模な教師なしデータを利用した学習を実現
          li 直接動画を生成したり，回帰誤差で学習したりする手法よりも良いことを主観評価実験で確認          
    .item4
      h1 自由記述欄
      .text
        ul
          li 入力が4フレームだけだけど，もっと増やすと性能は変わるのか気になる
          li 主観評価で本物と比較すると提案手法が一番嫌われている率が高いのもちょっと気になる
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Kensho Hara

  +slide
    .title DensePose: Dense Human Pose Estimation In The Wild
    .info
      .authors Rıza Alp Guler et al.
    .item1
      h1 概要
      .text.
        身体の表面形状まで考慮したDenseな姿勢推定手法に関する研究。サーフェイスモデルを提供するSMPLタイプとアノテーションベースのMSCOCOタイプを提供。手法はMask RCNN（w/ ResNet-50, ROI-align, Regression）をベースに構築している。

    .item2
      img(src=figpath+"180205densepose.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li DenseReg [Guler,CVPR17]は顔表面の推定に対して、本研究では身体全体の表面やデンスなポイントを回帰。
          li SMPLやDense-COCOのデータセットを構築
          li 非拘束（in the wild）の環境にてDensePoseを学習。
    .item4
      h1 自由記述欄
      .text
        ul
          li リアルが完全に崩壊した。（Face2Faceの全身モデル版が実現可能になった？）
          li CG/UIの分野との親和性がより高くなった
          li: a(href="https://arxiv.org/abs/1802.00434" target="blank") DensePose: Dense Human Pose Estimation In The Wild
          li: a(href="https://arxiv.org/abs/1612.01202" target="blank") DenseReg
          li: a(href="http://smpl.is.tue.mpg.de" target="blank") SMPL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka
    
  +slide
    .title What will Happen Next? Forecasting Player Moves in Sports Videos
    .info
      .authors Panna Felsen et al.
      .conference ICCV, 2017.
    .item1
      h1 概要
      .text.
        チームスポーツにおいて次に起こることを予測する研究。2チームに分かれたゴール型スポーツを対象とし、ボールを持つ選手の遷移やファールの有無などの推定を行った．

    .item2
      img(src=figpath+"180303whatwillhappen.PNG")
    .item3
      h1 新規性・結果
      .text
        ul
          li 水球とバスケットボールのデータセットを構築した
          li 画像から選手やボールの位置を上から見た画像に変換する手法を提案した
          li 他のスポーツで学習したものを適用した場合(例：学習→水球　テスト→バスケ)ランダムフォレストの方がニューラルネットより精度が高いことが分かった
    .item4
      h1 自由記述欄
      .text
        ul
          li この論文のようにニューラルネットがうまくいかない例を調べるのは面白そう
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Visual Forecasting by Imitating Dynamics in Natural Sequences
    .info
      .authors Zeng et al.
      .conference ICCV, 2017.
    .item1
      h1 概要
      .text.
        動画シークエンスから未来を予測する研究。フレーム間の遷移モデルを考え，次のフレームや行動を推定する。適用対象はフレームの生成から次のシーンの選択など幅広い。

    .item2
      img(src=figpath+"180303visualforecasting.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li ドメイン知識やhandcrafted特徴無しにinverse reinforcement learningとして学習させる
          li フレーム生成、行動予測、ストーリー予測全てにおいて精度の向上に成功した
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://arxiv.org/abs/1708.05827" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
    
  +slide
    .title A Domain Based Approach to Social Relation Recognition
    .info
      .authors Qianru Sun et al.
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        画像中に写っている人々の関係を推測する研究。社会心理学に基づいた16の関係(親子、友人など)を識別する。それぞれの人物から抽出された特徴を入力とするネットワークにより判定する。

    .item2
      img(src=figpath+"180306relation.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 社会心理学に基づいた理論をコンピュータビジョンに導入した
          li 画像に関係性などのラベルを付けることで、より広い用途で用いることができるデータベースを提案
          li 社会心理学に基づき、セマンティックなアトリビュートを収集した
    .item4
      h1 自由記述欄
      .text
        ul
          li 社会学系の理論をCVに持ってくるのは面白そう
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto

  +slide
    .title Forecasting Interactive Dynamics of Pedestrians with Fictitious Play
    .info
      .authors Ma et al.
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        画像中に写っている人々の歩行ルートを予測する手法。各歩行者に対して歩行モデルを決定し、他の人とぶつからないようによけるなど他者の行動を考慮した上で歩行ルートを決定していく。

    .item2
      img(src=figpath+"180306pedestrians1.png")
      img(src=figpath+"180306pedestrians2.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li ゲーム理論に基づき、他の歩行者の進行方向を予測した上でルートを決定する
          li 年齢などの情報を抽出し、各歩行者の歩行速度などを決定する
          li 既存手法と比べて長期的な予測の精度が向上
    .item4
      h1 自由記述欄
      .text
        ul
          li ゲーム理論の応用は興味深い
          li どれくらいの人数までできるのだろうか？（人ゴミは無理？）
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title DeepNav: Learning to Navigate Large Cities
    .info
      .authors Brahmbhatt and Hays
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        目的地までのルートを推測する研究。ストリートビューの画像から、どの方向に進めば銀行やガソリンスタンドなどの目的地に近付けるかを決定していく。ネットワークとしては、目的地までの距離、最も最短となる方角、2枚の画像のどちらが目的地に近いかの3種類を提案。

    .item2
      img(src=figpath+"180306Deepnav.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li アメリカ10都市を対象にストリートビューのデータセットを構築
          li 3種類のCNNネットワークを構築し，hand-crafted特徴及びSVRベースの手法より精度が向上した
          li ラベル付けを効率化するメカニズムを提案した
    .item4
      h1 自由記述欄
      .text
        ul
          li それぞれの目的地に対してどのような特徴を持った方向が選ばれているのか気になった
          li 場合によっては同じ場所を何度も回るだけになってしまう？
          li: a(href="https://arxiv.org/abs/1701.09135" target="blank") 論文URL
          li: a(href="http://mcdonalds.csail.mit.edu/" target="blank") 比較論文
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto
	
  +slide
    .title Forecasting Human Dynamics from Static Images
    .info
      .authors Chao et al.
      .conference CVPR, 2017.
    .item1
      h1 概要
      .text.
        1枚の画像から、人間のモーションを推定する研究。画像から2次元の姿勢を推定し，その結果を3次元に変換することで出力を得る。学習は3段階に分かれており、2次元姿勢推定部は2次元姿勢データベースを使用して学習をし、3次元姿勢推定部はモーションキャプチャデータを2次元投影することにより学習を行い、最後に全体を通して学習を行う。

    .item2
      img(src=figpath+"180306humandynamics1.png")
      img(src=figpath+"180306humandynamics2.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 従来研究とは異なり、RNNを用いることにより静止画からモーションを推定することを可能とした
          li 推定した2次元の姿勢から3次元の情報を復元するネットワークを提案した
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://arxiv.org/abs/1704.03432" target="blank") 論文URL
          li: a(href="http://www-personal.umich.edu/~ywchao/image-play/" target="blank") プロジェクトページ
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto

  +slide
    .title Toward Geometric Deep SLAM
    .authors Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich
    .conference arXiv
    .paper_id arXiv:1707.07410
    .slide_editor Yoshihiro Fukuhara
    .item1
      .text
        h1 概要
        p 2つのCNNを用いた高速かつ頑強な物体追跡手法を提案. 
          |1つ目のCNN(MagicPoint)で入力画像から特徴点を抽出し,
          |2つ目のCNN(MagicWarp)で抽出された特徴点の位置情報のみから２つの画像間のホモグラフィー行列の推定を行う.
    .item2
      .text
        p
          img(src=`${figpath}20180306--TowardGeometricDeepSLAM.png`,alt="20180306--TowardGeometricDeepSLAM.png")
    .item3
      .text
        h1 新規性・結果
        p MagicPointは幾何学的に安定した点（物体の角や辺など）のみを抽出するため, ノイズに頑強である. 
          |また, MagicWarpを用いることで従来手法のように特徴量の記述子（descriptor） を計算する必要がなくなるため, 高速な動作が可能となった. 
          |作成した単純形状のデータセット（Synthetic Shapes Dataset）では FAST, Haris, Shi よりも高精度.
    .item4
      .text
        h1 リンク集
        ul
          li
            a(href="https://arxiv.org/abs/1707.07410") 論文
    .slide_index #{getSlideIndex()}

  +slide
    .title Pedestrian Travel Time Estimation in Crowded Scenes
    .info
      .authors Yi et al.
      .conference ICCV, 2015.
    .item1
      h1 概要
      .text.
        画像中に写っている群衆が、目的地にたどり着くまでの時間を推測する手法。目的地まで歩行するにあたり、他の歩行者の流れや立ち止まっている人の存在によって歩行ルートは変化する。このように、目的地まで最短距離で向かうことができず人によってルートが変化する状況における歩行時間の推定を行う。

    .item2
      img(src=figpath+"180307pedestrian.png")
    .item3
      h1 新規性・結果
      .text
        ul
          li 目的地までの所要時間を統計的に推定する手法を提案
          li 人の流れの妨げになっている場所や異常行動の検出が可能に
          li 個人の移動時間に着目している既存研究と比べ、大衆に着目することで精度が向上
    .item4
      h1 自由記述欄
      .text
        ul
          li: a(href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yi_Pedestrian_Travel_Time_ICCV_2015_paper.pdf" target="blank") 論文URL
    .slide_index
      | #{getSlideIndex()}
    .slide_editor Shintaro Yamamoto